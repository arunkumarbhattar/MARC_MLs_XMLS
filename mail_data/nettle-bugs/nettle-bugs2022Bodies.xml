<?xml version="1.0" encoding="utf-8"?>
<emails><email><emailId>20220601173607</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-06-01 17:36:07-0400</timestampReceived><subject>[PATCH v2 0/2] export sha256 and sha512 compress functions</subject><body>

Hello

I am working on implementing crypto offloader devices I use and maintain
in Linux in qemu.
The hardware does not do full hashes offload but only their compress
part. (The driver need to do padding etc...)

From all crypto library, only nettle provides helper for compress but
only for md5 and sha1.

The first one device I implement (sun4i-ss) only do sha1 and md5, so its
fine. But the second (sun8i-ce) need also sha224/sha256/sha384/sha512.

So it is why I propose to export sha256/sha512 compress functions.

Regard

Changes since v1:
- removed the nettle prefix
- use xxx_compress on place of COMPRESS macro

Corentin Labbe (2):
  Export sha256/sha512_compress functions
  Use new xxx_compress in place of COMPRESS macro

 sha2.h   |  9 ++++++++-
 sha256.c | 14 +++++++++-----
 sha512.c | 14 +++++++++-----
 3 files changed, 26 insertions(+), 11 deletions(-)

-- 
2.35.1

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220601173608</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-06-01 17:36:08-0400</timestampReceived><subject>[PATCH v2 1/2] Export sha256/sha512_compress functions</subject><body>

nettle export only md5_compress and sha1_compress.
Let's export also the compress functions for sha256 and sha512.

Signed-off-by: Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt;
---
 sha2.h   | 9 ++++++++-
 sha256.c | 6 ++++++
 sha512.c | 6 ++++++
 3 files changed, 20 insertions(+), 1 deletion(-)

diff --git a/sha2.h b/sha2.h
index ca8222a7..264ab6ba 100644
--- a/sha2.h
+++ b/sha2.h
@@ -46,11 +46,13 @@ extern "C" {
 #define sha256_init nettle_sha256_init
 #define sha256_update nettle_sha256_update
 #define sha256_digest nettle_sha256_digest
+#define sha256_compress nettle_sha256_compress
 #define sha384_init nettle_sha384_init
 #define sha384_digest nettle_sha384_digest
 #define sha512_init nettle_sha512_init
 #define sha512_update nettle_sha512_update
 #define sha512_digest nettle_sha512_digest
+#define sha512_compress nettle_sha512_compress
 #define sha512_224_init   nettle_sha512_224_init
 #define sha512_224_digest nettle_sha512_224_digest
 #define sha512_256_init   nettle_sha512_256_init
@@ -138,6 +140,8 @@ sha512_digest(struct sha512_ctx *ctx,
 	      size_t length,
 	      uint8_t *digest);
 
+void
+sha512_compress(uint64_t *state, const uint8_t *input);
 
 /* SHA384, a truncated SHA512 with different initial state. */
 
@@ -186,7 +190,10 @@ void
 sha512_256_digest(struct sha512_256_ctx *ctx,
                   size_t length,
                   uint8_t *digest);
-  
+
+void
+sha256_compress(uint32_t *state, const uint8_t *input);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/sha256.c b/sha256.c
index 253c1319..ed11d801 100644
--- a/sha256.c
+++ b/sha256.c
@@ -161,3 +161,9 @@ sha224_digest(struct sha256_ctx *ctx,
   sha256_write_digest(ctx, length, digest);
   sha224_init(ctx);
 }
+
+void
+sha256_compress(uint32_t *state, const uint8_t *input)
+{
+  _nettle_sha256_compress(state, input, K);
+}
diff --git a/sha512.c b/sha512.c
index 6936cb50..6832d83a 100644
--- a/sha512.c
+++ b/sha512.c
@@ -312,3 +312,9 @@ sha512_256_digest(struct sha512_256_ctx *ctx,
   sha512_write_digest(ctx, length, digest);
   sha512_256_init(ctx);
 }
+
+void
+sha512_compress(uint64_t *state, const uint8_t *input)
+{
+  _nettle_sha512_compress(state, input, K);
+}
-- 
2.35.1

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220601173609</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-06-01 17:36:09-0400</timestampReceived><subject>[PATCH v2 2/2] Use new xxx_compress in place of COMPRESS macro</subject><body>

COMPRESS macro and sha256_compress/sha512_compress do the same thing,
so replace COMPRESS by the later.

Signed-off-by: Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt;
---
 sha256.c | 8 +++-----
 sha512.c | 8 +++-----
 2 files changed, 6 insertions(+), 10 deletions(-)

diff --git a/sha256.c b/sha256.c
index ed11d801..91a4db7c 100644
--- a/sha256.c
+++ b/sha256.c
@@ -70,8 +70,6 @@ K[64] =
   0x90befffaUL, 0xa4506cebUL, 0xbef9a3f7UL, 0xc67178f2UL, 
 };
 
-#define COMPRESS(ctx, data) (_nettle_sha256_compress((ctx)-&gt;state, (data), K))
-
 /* Initialize the SHA values */
 
 void
@@ -97,7 +95,7 @@ void
 sha256_update(struct sha256_ctx *ctx,
 	      size_t length, const uint8_t *data)
 {
-  MD_UPDATE (ctx, length, data, COMPRESS, ctx-&gt;count++);
+  MD_UPDATE (ctx, length, data, sha256_compress, ctx-&gt;count++);
 }
 
 static void
@@ -109,7 +107,7 @@ sha256_write_digest(struct sha256_ctx *ctx,
 
   assert(length &lt;= SHA256_DIGEST_SIZE);
 
-  MD_PAD(ctx, 8, COMPRESS);
+  MD_PAD(ctx, 8, sha256_compress);
 
   /* There are 512 = 2^9 bits in one block */  
   bit_count = (ctx-&gt;count &lt;&lt; 9) | (ctx-&gt;index &lt;&lt; 3);
@@ -118,7 +116,7 @@ sha256_write_digest(struct sha256_ctx *ctx,
      big-endian format, and will be converted back by the compression
      function. It's probably not worth the effort to fix this. */
   WRITE_UINT64(ctx-&gt;block + (SHA256_BLOCK_SIZE - 8), bit_count);
-  COMPRESS(ctx, ctx-&gt;block);
+  sha256_compress(ctx, ctx-&gt;block);
 
   _nettle_write_be32(length, digest, ctx-&gt;state);
 }
diff --git a/sha512.c b/sha512.c
index 6832d83a..be21e2d7 100644
--- a/sha512.c
+++ b/sha512.c
@@ -113,8 +113,6 @@ K[80] =
   0x5FCB6FAB3AD6FAECULL,0x6C44198C4A475817ULL,
 };
 
-#define COMPRESS(ctx, data) (_nettle_sha512_compress((ctx)-&gt;state, (data), K))
-
 void
 sha512_init(struct sha512_ctx *ctx)
 {
@@ -148,7 +146,7 @@ void
 sha512_update(struct sha512_ctx *ctx,
 	      size_t length, const uint8_t *data)
 {
-  MD_UPDATE (ctx, length, data, COMPRESS, MD_INCR(ctx));
+  MD_UPDATE (ctx, length, data, sha512_compress, MD_INCR(ctx));
 }
 
 static void
@@ -164,7 +162,7 @@ sha512_write_digest(struct sha512_ctx *ctx,
 
   assert(length &lt;= SHA512_DIGEST_SIZE);
 
-  MD_PAD(ctx, 16, COMPRESS);
+  MD_PAD(ctx, 16, sha512_compress);
 
   /* There are 1024 = 2^10 bits in one block */  
   high = (ctx-&gt;count_high &lt;&lt; 10) | (ctx-&gt;count_low &gt;&gt; 54);
@@ -175,7 +173,7 @@ sha512_write_digest(struct sha512_ctx *ctx,
      function. It's probably not worth the effort to fix this. */
   WRITE_UINT64(ctx-&gt;block + (SHA512_BLOCK_SIZE - 16), high);
   WRITE_UINT64(ctx-&gt;block + (SHA512_BLOCK_SIZE - 8), low);
-  COMPRESS(ctx, ctx-&gt;block);
+  sha512_compress(ctx, ctx-&gt;block);
 
   words = length / 8;
   leftover = length % 8;
-- 
2.35.1

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220602192205</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-06-02 19:22:05-0400</timestampReceived><subject>ANNOUNCE: Nettle-3.8</subject><body>

[Attachment #2 (multipart/signed)]


I'm happy to announce a new release of GNU Nettle, a low-level
cryptographics library. This includes a few new features, and several
optimizations, see NEWS entries below.

The Nettle home page can be found at
https://www.lysator.liu.se/~nisse/nettle/, and the manual at
https://www.lysator.liu.se/~nisse/nettle/nettle.html.

The release can be downloaded from

  https://ftp.gnu.org/gnu/nettle/nettle-3.8.tar.gz
  ftp://ftp.gnu.org/gnu/nettle/nettle-3.8.tar.gz
  https://www.lysator.liu.se/~nisse/archive/nettle-3.8.tar.gz

Happy hacking,
/Niels MÃ¶ller

NEWS for the Nettle 3.8 release

	This release includes a couple of new features, and many
	performance improvements. It adds assembly code for two more
	architectures: ARM64 and S390x.

	The new version is intended to be fully source and binary
	compatible with Nettle-3.6. The shared library names are
	libnettle.so.8.5 and libhogweed.so.6.5, with sonames
	libnettle.so.8 and libhogweed.so.6.

	New features:

	* AES keywrap (RFC 3394), contributed by Nicolas Mora.

	* SM3 hash function, contributed by Tianjia Zhang.

	* New functions cbc_aes128_encrypt, cbc_aes192_encrypt,
	  cbc_aes256_encrypt.

	  On processors where AES is fast enough, e.g., x86_64 with
	  aesni instructions, the overhead of using Nettle's general
	  cbc_encrypt can be significant. The new functions can be
	  implemented in assembly, to do multiple blocks with reduced
	  per-block overhead.

	  Note that there's no corresponding new decrypt functions,
	  since the general cbc_decrypt doesn't suffer from the same
	  performance problem.

	Bug fixes:

	* Fix fat builds for x86_64 windows, these appear to never
          have worked.

	Optimizations:

	* New ARM64 implementation of AES, GCM, Chacha, SHA1 and
	  SHA256, for processors supporting crypto extensions. Great
	  speedups, and fat builds are supported. Contributed by
	  Mamone Tarsha.

	* New s390x implementation of AES, GCM, Chacha, memxor, SHA1,
	  SHA256, SHA512 and SHA3. Great speedups, and fat builds are
	  supported. Contributed by Mamone Tarsha.

	* New PPC64 assembly for ecc modulo/redc operations,
	  contributed by Amitay Isaacs, Martin Schwenke and Alastair
	  D ´Silva.

	* The x86_64 AES implementation using aesni instructions has
	  been reorganized with one separate function per key size,
	  each interleaving the processing of two blocks at a time
	  (when the caller processes multiple blocks with each call).
	  This gives a modest performance improvement on some
	  processors.

	* Rewritten and faster x86_64 poly1305 assembly.

	Known issues:

	* Nettle's testsuite doesn't work out-of-the-box on recent
	  MacOS, due to /bin/sh discarding the DYLD_LIBRARY_PATH
	  environment variable. Nettle's test scripts handle this in
	  some cases, but currently fails the test cases that are
	  themselves written as /bin/sh scripts. As a workaround, use

	  make check EMULATOR='env DYLD_LIBRARY_PATH=$(TEST_SHLIB_DIR)'

	Miscellaneous:

	* Updated manual to current makeinfo conventions, with no
	  explicit node pointers. Generate pdf version with texi2pdf,
	  to get working hyper links.

	* Added square root functions for NIST ecc curves, as a
	  preparation for supporting compact point representation.

	* Reworked internal GCM/ghash interfaces, simplifying assembly
	  implementations. Deleted unused GCM C implementation
	  variants with less than 8-bit lookup table.

-- 
Niels MÃ¶ller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.


["signature.asc" (application/pgp-signature)]

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se


</body></email><email><emailId>20220321202412</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-03-21 20:24:12-0400</timestampReceived><subject>Status update</subject><body>

Sorry for not being that active last weeks, I've been a bit busy at
work, and I've also been spending probably too many hours following the
war news. 

Next things I want to do are: Review pending work, including SM4,
poly1305, HPKE. I've also noticed that the copyright/authorship section
of the manual is very out-of-date, so I'm trying to put together a more
comprehensive AUTHORS file to replace that. I'll post it to the list for
review when it's getting more complete.

At some point, we need to stop to make a new release, despite a lot of
great ongoing work. Opinions on release priorities and any "must haves"
before a new release are welcome.

Regards,
/Niels

PS. Speaking of work, I'm considering looking for new
employment/contracting, any advice or suggestions welcome off-list. My
wish list:

* Primarily foss work. 
* Problems in the intersection of computer science and math.
* Part time, e.g., 4 days a week.
* Either located in Stockholm area, or remote.
See also https://www.lysator.liu.se/~nisse/cv.html

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220104195437</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-04 19:54:37-0400</timestampReceived><subject>Re: powerpc ecc 256 redc (was Re: x86_64 ecc_256_redc)</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt;&gt; I think it should be possible to reduce number of needed registers, and
&gt;&gt; completely avoid using callee-save registers (load the values now in
&gt;&gt; U4-U7 one at a time a bit closer to the place where they are needed in),
&gt;&gt; and replace F3 with $1 in the FOLD and FOLDC macros.
&gt;
&gt; Attaching a variant to do this. Passes tests with qemu, but I haven't
&gt; benchmarked it on any real hardware.

Would you like to test and benchmark this on relevant real hardware,
before I merged this version?

Code still below, and committed to the branch ppc-secp256-tweaks.

Regards,
/Niels

C powerpc64/ecc-secp256r1-redc.asm

ifelse(`
   Copyright (C) 2021 Amitay Isaacs &amp; Martin Schwenke, IBM Corporation

   Based on x86_64/ecc-secp256r1-redc.asm

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

C Register usage:

define(`RP', `r4')
define(`XP', `r5')

define(`F0', `r3')
define(`F1', `r6')
define(`F2', `r7')
define(`T', `r8')

define(`U0', `r9')
define(`U1', `r10')
define(`U2', `r11')
define(`U3', `r12')

	.file "ecc-secp256r1-redc.asm"

C FOLD(x), sets (x,F2,F1,F0)  &lt;-- [(x &lt;&lt; 192) - (x &lt;&lt; 160) + (x &lt;&lt; 128) + (x &lt;&lt;32)]
define(`FOLD', `
	sldi	F0, $1, 32
	srdi	F1, $1, 32
	subfc	F2, F0, $1
	subfe	$1, F1, $1
')

C FOLDC(x), sets (x,F2,F1,F0)  &lt;-- [((x+c) &lt;&lt; 192) - (x &lt;&lt; 160) + (x &lt;&lt; 128) + (x &lt;&lt;32)]
define(`FOLDC', `
	sldi	F0, $1, 32
	srdi	F1, $1, 32
	addze	T, $1
	subfc	F2, F0, $1
	subfe	$1, F1, T
')

	C void ecc_secp256r1_redc (const struct ecc_modulo *p, mp_limb_t *rp, mp_limb_t *xp)
	.text
define(`FUNC_ALIGN', `5')
PROLOGUE(_nettle_ecc_secp256r1_redc)

	ld	U0, 0(XP)
	ld	U1, 8(XP)
	ld	U2, 16(XP)
	ld	U3, 24(XP)

	FOLD(U0)
	ld	T, 32(XP)
	addc	U1, F0, U1
	adde	U2, F1, U2
	adde	U3, F2, U3
	adde	U0, U0, T

	FOLDC(U1)
	ld	T, 40(XP)
	addc	U2, F0, U2
	adde	U3, F1, U3
	adde	U0, F2, U0
	adde	U1, U1, T

	FOLDC(U2)
	ld	T, 48(XP)
	addc	U3, F0, U3
	adde	U0, F1, U0
	adde	U1, F2, U1
	adde	U2, U2, T

	FOLDC(U3)
	ld	T, 56(XP)
	addc	U0, F0, U0
	adde	U1, F1, U1
	adde	U2, F2, U2
	adde	U3, U3, T

	C If carry, we need to add in
	C 2^256 - p = &lt;0xfffffffe, 0xff..ff, 0xffffffff00000000, 1&gt;
	li	F0, 0
	addze	F0, F0
	neg	F2, F0
	sldi	F1, F2, 32
	srdi	T, F2, 32
	li	XP, -2
	and	T, T, XP

	addc	U0, F0, U0
	adde	U1, F1, U1
	adde	U2, F2, U2
	adde	U3, T, U3

	std	U0, 0(RP)
	std	U1, 8(RP)
	std	U2, 16(RP)
	std	U3, 24(RP)

	blr
EPILOGUE(_nettle_ecc_secp256r1_redc)


-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220110054604</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-10 05:46:04-0400</timestampReceived><subject>Re: powerpc ecc 256 redc (was Re: x86_64 ecc_256_redc)</subject><body>

Hi Niels,

On Tue, 2022-01-04 at 20:54 +0100, Niels Möller wrote:
&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt; 
&gt; &gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt; &gt; 
&gt; &gt; &gt; I think it should be possible to reduce number of needed
&gt; &gt; &gt; registers, and
&gt; &gt; &gt; completely avoid using callee-save registers (load the values now
&gt; &gt; &gt; in
&gt; &gt; &gt; U4-U7 one at a time a bit closer to the place where they are
&gt; &gt; &gt; needed in),
&gt; &gt; &gt; and replace F3 with $1 in the FOLD and FOLDC macros.
&gt; &gt; 
&gt; &gt; Attaching a variant to do this. Passes tests with qemu, but I
&gt; &gt; haven't
&gt; &gt; benchmarked it on any real hardware.
&gt; 
&gt; Would you like to test and benchmark this on relevant real hardware,
&gt; before I merged this version?
&gt; 
&gt; Code still below, and committed to the branch ppc-secp256-tweaks.

Compared to the current version in master branch, this version
definitely improves the performance of the reduction code.

On POWER9, the reduction code shows 7% speed up when tested separately.

The improvement in P256 sign/verify is marginal.  Here are the numbers
from hogweed-benchmark on POWER9.

 
            name size   sign/ms verify/ms
           ecdsa  256   11.1013    3.5713  (master)
           ecdsa  256   11.1527    3.6011  (this patch)


Amitay.
-- 

People on the net are always telling other people to "get a life." It 
would be so much simlper if there were on available under GPL. "If you
use this life, you must tell other people where to get a life of their
own."  - Christopher Davis
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220110192333</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-10 19:23:33-0400</timestampReceived><subject>Re: powerpc ecc 256 redc (was Re: x86_64 ecc_256_redc)</subject><body>

Amitay Isaacs &lt;amitay@ozlabs.org&gt; writes:

&gt; Compared to the current version in master branch, this version
&gt; definitely improves the performance of the reduction code.
&gt;
&gt; On POWER9, the reduction code shows 7% speed up when tested separately.
&gt;
&gt; The improvement in P256 sign/verify is marginal.  Here are the numbers
&gt; from hogweed-benchmark on POWER9.
&gt;
&gt;  
&gt;             name size   sign/ms verify/ms
&gt;            ecdsa  256   11.1013    3.5713  (master)
&gt;            ecdsa  256   11.1527    3.6011  (this patch)

Thanks for testing. Committed to the master branch now.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121035109</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 03:51:09-0400</timestampReceived><subject>Re: [PATCH 1/7] ecc: Add powerpc64 assembly for ecc_192_modp</subject><body>

Here's the updated code for P192 curve after simplifying C2 folding.

Amitay.
-- 

Retirement: When you quit working just before your heart does.
Retirement: When you quit working just before your heart does.

["ecc-secp192r1-modp.asm" (ecc-secp192r1-modp.asm)]

C powerpc64/ecc-secp192r1-modp.asm

ifelse(`
   Copyright (C) 2021 Amitay Isaacs, IBM Corporation

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

	.file "ecc-secp192r1-modp.asm"

define(`RP', `r4')
define(`XP', `r5')

define(`T0', `r6')
define(`T1', `r7')
define(`T2', `r8')
define(`T3', `r9')
define(`C1', `r10')
define(`C2', `r11')

	C void ecc_secp192r1_modp (const struct ecc_modulo *m, mp_limb_t *rp)
	.text
define(`FUNC_ALIGN', `5')
PROLOGUE(_nettle_ecc_secp192r1_modp)
	ld	T0, 0(XP)
	ld	T1, 8(XP)
	ld	T2, 16(XP)

	li	C1, 0
	li	C2, 0

	ld	T3, 24(XP)
	addc	T0, T3, T0
	adde	T1, T3, T1
	addze	T2, T2
	addze	C1, C1

	ld	T3, 32(XP)
	addc	T1, T3, T1
	adde	T2, T3, T2
	addze	C1, C1

	ld	T3, 40(XP)
	addc	T0, T3, T0
	adde	T1, T3, T1
	adde	T2, T3, T2
	addze	C1, C1

	addc	T0, C1, T0
	adde	T1, C1, T1
	addze	T2, T2
	addze	C2, C2

	addc	T0, C2, T0
	adde	T1, C2, T1
	addze	T2, T2

	std	T0, 0(RP)
	std	T1, 8(RP)
	std	T2, 16(RP)

	blr
EPILOGUE(_nettle_ecc_secp192r1_modp)

[Attachment #4 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220104194627</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-04 19:46:27-0400</timestampReceived><subject>Re: Build problem on ppc64be + musl</subject><body>

Going through some old mail... From a discussion in September:

nisse@lysator.liu.se (Niels Möller) writes:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt;&gt; I've tried a different approach on branch
&gt;&gt; https://git.lysator.liu.se/nettle/nettle/-/tree/ppc64-efv2-check. Patch
&gt;&gt; below. (It makes sense to me to have the new check together with the ABI
&gt;&gt; check, but on second thought, probably a mistake to overload the ABI
&gt;&gt; variable. It would be better to have a separate configure variable, more
&gt;&gt; similar to the W64_ABI).
&gt;
&gt; Another iteration, on that branch (sorry for the typo in the branch
&gt; name), or see patch below.
&gt;
&gt; Stijn, can you try it out and see if it works for you?

I haven't seen any response to this, but I've nevertheless just added
these changes on the master-updates branch. It would be nice if you can
confirm that it solves the problem with musl.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220208162208</emailId><senderName>Justus Winter</senderName><senderEmail>justus@sequoia-pgp.org</senderEmail><timestampReceived>2022-02-08 16:22:08-0400</timestampReceived><subject>Re: Feature request: OCB mode</subject><body>

[Attachment #2 (multipart/signed)]


Hello Niels :)

sorry for not following up earlier.  Thanks for working on it!

nisse@lysator.liu.se (Niels Möller) writes:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt;&gt; If someone wants to work on it, please post to the list. I might look
&gt;&gt; into it myself, but as you have noticed, I have rather limited hacking
&gt;&gt; time.
&gt;
&gt; I've given it a try, see branch ocb-mode. Based on RFC 7253. Passes
&gt; tests, but not particularly optimized.

I have wrapped it in our Rust bindings, glued Sequoia to it, and did
some interop testing.  Looks all good.

&gt; Some comments and questions:
&gt;
&gt; 1. Most of the operations use only the enrypt function of the underlying
&gt;    block cipher. Except ocb decrypt, which needs *both* the decrypt
&gt;    function and the encrypt function. For ciphers that use different key
&gt;    setup for encrypt and decrypt, e.g., AES, that means that to decrypt
&gt;    OCB one needs to initialize two separate aes128_ctx. To call the
&gt;    somewhat unwieldy
&gt;
&gt;       void
&gt;       ocb_decrypt (struct ocb_ctx *ctx, const struct ocb_key *key,
&gt;                    const void *encrypt_ctx, nettle_cipher_func *encrypt,
&gt;                    const void *decrypt_ctx, nettle_cipher_func *decrypt,
&gt;                    size_t length, uint8_t *dst, const uint8_t *src);

I don't mind it being unwieldy.

&gt; 2. It's not obvious how to best manage the different L_i values. Can be
&gt;    computed upfront, on demand, or cached in some way. Current code
&gt;    computes only L_*, L_$ and L_0 up front (part of ocb_set_key), and
&gt;    the others recomputed each time they're needed.

I cannot comment on that.

&gt; 3. The processing of the authenticated data doesn't depend on the nonce
&gt;    in any way. That means that if one processes several messages with
&gt;    the same key and associated data, the associated data can be
&gt;    processed once, with the same sum reused for all messages.
&gt;
&gt;    Is that something that is useful in practice, and which nettle
&gt;    interfaces should support?

That is an interesting question.  Currently, the OpenPGP drafts that
include AEAD do include the chunk index in the authenticated data and
would therefore not benefit from this optimization.  However, I've
raised this point in our issue tracker:

https://gitlab.com/openpgp-wg/rfc4880bis/-/issues/86

&gt; 4. The way the nonce is used seems designed to allow cheap incrementing
&gt;    of the nonce. The nonce is used to determine
&gt;
&gt;      Offset_0 = Stretch[1+bottom..128+bottom]
&gt;
&gt;    where "bottom" is the least significant 6 bits of the nonce, acting as
&gt;    a shift, and "Stretch" is independent of those nonce bits, so
&gt;    unchanged on all but one out of 64 nonce increments.
&gt;
&gt;    Should nettle support some kind of auto-incrementing nonce that takes
&gt;    advantage of this? Nettle does something similar for UMAC (not sure
&gt;    if there are others).

That is also interesting.  I have raised the point in our issue tracker,
and Daniel Huigens observed that at least their Go implementation simply
compares the top-most bits with the ones provided for the previous
chunk.  Botan does the same.

https://gitlab.com/openpgp-wg/rfc4880bis/-/issues/84
https://github.com/ProtonMail/go-crypto/blob/70ae35bab23f26f6188bab82cb34d7f7adf2b200/ocb/ocb.go#L157
https://botan.randombit.net/doxygen/ocb_8cpp_source.html#l00264

This has the benefit of working for how OpenPGP currently constructs the
nonce, which does not result in monotonically incrementing nonces
(currently, we take an IV and xor in the chunk index).  But, we may
change the scheme.


Thanks,
Justus

["signature.asc" (application/pgp-signature)]
[Attachment #6 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220215191256</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-15 19:12:56-0400</timestampReceived><subject>Re: Feature request: OCB mode</subject><body>

Justus Winter &lt;justus@sequoia-pgp.org&gt; writes:

&gt;&gt; I've given it a try, see branch ocb-mode. Based on RFC 7253. Passes
&gt;&gt; tests, but not particularly optimized.
&gt;
&gt; I have wrapped it in our Rust bindings, glued Sequoia to it, and did
&gt; some interop testing.  Looks all good.

Based on the functions declared in ocb.h, or the struct
nettle_ocb_aesxxx?

I'm thinking that it might make sense to add the latter as part of the
public api for next release, but leave all other functions as internal
to let dust settle a bit?

&gt;&gt; 2. It's not obvious how to best manage the different L_i values. Can be
&gt;&gt;    computed upfront, on demand, or cached in some way. Current code
&gt;&gt;    computes only L_*, L_$ and L_0 up front (part of ocb_set_key), and
&gt;&gt;    the others recomputed each time they're needed.
&gt;
&gt; I cannot comment on that.

Changing it later we be a bit difficult (ABI break, if more space is
needed in the context struct), so we need to decide on something
reasonable.

&gt;&gt; 3. The processing of the authenticated data doesn't depend on the nonce
&gt;&gt;    in any way. That means that if one processes several messages with
&gt;&gt;    the same key and associated data, the associated data can be
&gt;&gt;    processed once, with the same sum reused for all messages.
&gt;&gt;
&gt;&gt;    Is that something that is useful in practice, and which nettle
&gt;&gt;    interfaces should support?
&gt;
&gt; That is an interesting question.  Currently, the OpenPGP drafts that
&gt; include AEAD do include the chunk index in the authenticated data and
&gt; would therefore not benefit from this optimization.  However, I've
&gt; raised this point in our issue tracker:

This choice can affect both API and ABI.

&gt;&gt; 4. The way the nonce is used seems designed to allow cheap incrementing
&gt;&gt;    of the nonce. The nonce is used to determine
&gt;&gt;
&gt;&gt;      Offset_0 = Stretch[1+bottom..128+bottom]
&gt;&gt;
&gt;&gt;    where "bottom" is the least significant 6 bits of the nonce, acting as
&gt;&gt;    a shift, and "Stretch" is independent of those nonce bits, so
&gt;&gt;    unchanged on all but one out of 64 nonce increments.
&gt;&gt;
&gt;&gt;    Should nettle support some kind of auto-incrementing nonce that takes
&gt;&gt;    advantage of this? Nettle does something similar for UMAC (not sure
&gt;&gt;    if there are others).
&gt;
&gt; That is also interesting.  I have raised the point in our issue tracker,
&gt; and Daniel Huigens observed that at least their Go implementation simply
&gt; compares the top-most bits with the ones provided for the previous
&gt; chunk.  Botan does the same.

For any kind of optimization of this, one needs to store previous nonce
and relatede values in the context.

To generalize it to more than auto-increment, one get the problem that
for the first set_nonce, there is no previous nonce to compare to. So
one would need an extra flag just for that, which I don't think is so
nice. Alternatively, use a zero nonce by default at initialization. Then
there's another slight complication: To set the nonce, one needs to know
the "tag_length". In the current version, that is passed as an argument
to set_nonce. It could perhaps be passed with set_key instead, it's now
some time since I read the RFC, but I don't think it is proper use to
use OCB with the same key, but change tag_length from message to message.

&gt; This has the benefit of working for how OpenPGP currently constructs the
&gt; nonce, which does not result in monotonically incrementing nonces
&gt; (currently, we take an IV and xor in the chunk index).  But, we may
&gt; change the scheme.

I think it would be nice to stick to a simply incrementing nonce value.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220216095427</emailId><senderName>Justus Winter</senderName><senderEmail>justus@sequoia-pgp.org</senderEmail><timestampReceived>2022-02-16 09:54:27-0400</timestampReceived><subject>Re: Feature request: OCB mode</subject><body>

[Attachment #2 (multipart/signed)]


nisse@lysator.liu.se (Niels Möller) writes:

&gt; Justus Winter &lt;justus@sequoia-pgp.org&gt; writes:
&gt;
&gt;&gt;&gt; I've given it a try, see branch ocb-mode. Based on RFC 7253. Passes
&gt;&gt;&gt; tests, but not particularly optimized.
&gt;&gt;
&gt;&gt; I have wrapped it in our Rust bindings, glued Sequoia to it, and did
&gt;&gt; some interop testing.  Looks all good.
&gt;
&gt; Based on the functions declared in ocb.h, or the struct
&gt; nettle_ocb_aesxxx?
&gt;
&gt; I'm thinking that it might make sense to add the latter as part of the
&gt; public api for next release, but leave all other functions as internal
&gt; to let dust settle a bit?

Based on the ocb.h interface.  The latter is not a good fit for our
bindings, which let you combine AEAD modes with block ciphers, something
OpenPGP also allows (for better or worse).  Also, the latter is not
useful for OpenPGP as specified in both the RFC4880bis draft and the
crypto-refresh draft, as both use 120 bit nonces, whereas the interface
uses 96 bit nonces.

I have opened an issue about the nonce size, and we decided to seek
clarification from the authors of RFC7253:

https://gitlab.com/openpgp-wg/rfc4880bis/-/issues/83

&gt;&gt;&gt; 3. The processing of the authenticated data doesn't depend on the nonce
&gt;&gt;&gt;    in any way. That means that if one processes several messages with
&gt;&gt;&gt;    the same key and associated data, the associated data can be
&gt;&gt;&gt;    processed once, with the same sum reused for all messages.
&gt;&gt;&gt;
&gt;&gt;&gt;    Is that something that is useful in practice, and which nettle
&gt;&gt;&gt;    interfaces should support?
&gt;&gt;
&gt;&gt; That is an interesting question.  Currently, the OpenPGP drafts that
&gt;&gt; include AEAD do include the chunk index in the authenticated data and
&gt;&gt; would therefore not benefit from this optimization.  However, I've
&gt;&gt; raised this point in our issue tracker:
&gt;
&gt; This choice can affect both API and ABI.

We optimistically changed our scheme to not change the associated data
between chunks (except for the last chunk):

] For each chunk, the AEAD construction is given the Packet Tag in new
] format encoding (bits 7 and 6 set, bits 5-0 carry the packet tag),
] version number, cipher algorithm octet, AEAD algorithm octet, and chunk
] size octet as additional data. For example, the additional data of the
] first chunk using EAX and AES-128 with a chunk size of 2**16 octets
] consists of the octets 0xD2, 0x02, 0x07, 0x01, and 0x10.
]
] After the final chunk, the AEAD algorithm is used to produce a final
] authentication tag encrypting the empty string. This AEAD instance is
] given the additional data specified above, plus an eight-octet,
] big-endian value specifying the total number of plaintext octets
] encrypted. This allows detection of a truncated ciphertext.

&gt;&gt;&gt; 4. The way the nonce is used seems designed to allow cheap incrementing
&gt;&gt;&gt;    of the nonce. The nonce is used to determine
&gt;&gt;&gt;
&gt;&gt;&gt;      Offset_0 = Stretch[1+bottom..128+bottom]
&gt;&gt;&gt;
&gt;&gt;&gt;    where "bottom" is the least significant 6 bits of the nonce, acting as
&gt;&gt;&gt;    a shift, and "Stretch" is independent of those nonce bits, so
&gt;&gt;&gt;    unchanged on all but one out of 64 nonce increments.
&gt;&gt;&gt;
&gt;&gt;&gt;    Should nettle support some kind of auto-incrementing nonce that takes
&gt;&gt;&gt;    advantage of this? Nettle does something similar for UMAC (not sure
&gt;&gt;&gt;    if there are others).
&gt;&gt;
&gt;&gt; That is also interesting.  I have raised the point in our issue tracker,
&gt;&gt; and Daniel Huigens observed that at least their Go implementation simply
&gt;&gt; compares the top-most bits with the ones provided for the previous
&gt;&gt; chunk.  Botan does the same.
&gt;
&gt; For any kind of optimization of this, one needs to store previous nonce
&gt; and relatede values in the context.
&gt;
&gt; To generalize it to more than auto-increment, one get the problem that
&gt; for the first set_nonce, there is no previous nonce to compare to. So
&gt; one would need an extra flag just for that, which I don't think is so
&gt; nice. Alternatively, use a zero nonce by default at initialization. Then
&gt; there's another slight complication: To set the nonce, one needs to know
&gt; the "tag_length". In the current version, that is passed as an argument
&gt; to set_nonce. It could perhaps be passed with set_key instead, it's now
&gt; some time since I read the RFC, but I don't think it is proper use to
&gt; use OCB with the same key, but change tag_length from message to message.
&gt;
&gt;&gt; This has the benefit of working for how OpenPGP currently constructs the
&gt;&gt; nonce, which does not result in monotonically incrementing nonces
&gt;&gt; (currently, we take an IV and xor in the chunk index).  But, we may
&gt;&gt; change the scheme.
&gt;
&gt; I think it would be nice to stick to a simply incrementing nonce value.

We optimistically changed the scheme so that it is a counter, albeit one
not starting from zero:

] The nonce for AEAD mode consists of two parts. Let N be the size of the
] nonce. The left-most N - 64 bits are the initialization vector derived
] using HKDF. The right-most 64 bits are the chunk index as big-endian
] value. The index of the first chunk is zero.

Cheers,
Justus

["signature.asc" (application/pgp-signature)]
[Attachment #6 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220121035436</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 03:54:36-0400</timestampReceived><subject>Re: [PATCH 2/7] ecc: Add powerpc64 assembly for ecc_224_modp</subject><body>

Updated version using actual register names for storing and restoring
from stack.

Amitay.
-- 

The manager administers, the leader innovates. The manager maintains,
the
leader develops. The manager relies on systems, the leader relies on
people.
The manager counts on controls, the leader counts on trust. The manager
does
things right, the leader does the right thing. - Fortune Magazine

["ecc-secp224r1-modp.asm" (ecc-secp224r1-modp.asm)]

C powerpc64/ecc-secp224r1-modp.asm

ifelse(`
   Copyright (C) 2021 Amitay Isaacs, IBM Corporation

   Based on x86_64/ecc-secp224r1-modp.asm

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

	.file "ecc-secp224r1-modp.asm"

define(`SP', `r1')

define(`RP', `r4')
define(`XP', `r5')

define(`T0', `r6')
define(`T1', `r7')
define(`H0', `r8')
define(`H1', `r9')
define(`H2', `r10')
define(`F0', `r11')
define(`F1', `r12')
define(`F2', `r14')
define(`T2', `r3')

	C void ecc_secp224r1_modp (const struct ecc_modulo *m, mp_limb_t *rp)
	.text
define(`FUNC_ALIGN', `5')
PROLOGUE(_nettle_ecc_secp224r1_modp)
	std	r14, -8(SP)

	ld	H0, 48(XP)
	ld	H1, 56(XP)
	C set (F2, F1, F0) &lt;-- (H1, H0) &lt;&lt; 32
	sldi	F0, H0, 32
	srdi	F1, H0, 32
	sldi	T0, H1, 32
	srdi	F2, H1, 32
	or	F1, T0, F1

	li	H2, 0
	ld	T0, 16(XP)
	ld	T1, 24(XP)
	subfc	T0, F0, T0
	subfe	T1, F1, T1
	subfe	H0, F2, H0
	addme	H1, H1

	ld	T2, 32(XP)
	addc	H0, T2, H0
	ld	T2, 40(XP)
	adde	H1, T2, H1
	addze	H2, H2

	C Set (F2, F1, F0) &lt;-- (H2, H1, H0) &lt;&lt; 32
	sldi	F0, H0, 32
	srdi	F1, H0, 32
	addc	H0, T0, H0
	sldi	T0, H1, 32
	srdi	F2, H1, 32
	adde	H1, T1, H1
	sldi	T1, H2, 32
	addze	H2, H2
	or	F1, T0, F1
	or	F2, T1, F2

	ld	T0, 0(XP)
	ld	T1, 8(XP)
	subfc	T0, F0, T0
	subfe	T1, F1, T1
	subfe	H0, F2, H0
	addme	H1, H1
	addme	H2, H2

	srdi	F0, H1, 32
	sldi	F1, H2, 32
	or	F0, F1, F0
	clrrdi	F1, H1, 32
	mr	F2, H2
	clrldi	H1, H1, 32

	subfc	T0, F0, T0
	addme	F1, F1
	addme	F2, F2
	addc	T1, F1, T1
	adde	H0, F2, H0
	addze	H1, H1

	std	T0, 0(RP)
	std	T1, 8(RP)
	std	H0, 16(RP)
	std	H1, 24(RP)

	ld	r14, -8(SP)

	blr
EPILOGUE(_nettle_ecc_secp224r1_modp)

[Attachment #4 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220121040233</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:33-0400</timestampReceived><subject>[PATCH v2 1/6] ecc: Add powerpc64 assembly for ecc_192_modp</subject><body>

Signed-off-by: Amitay Isaacs &lt;amitay@ozlabs.org&gt;
---
 powerpc64/ecc-secp192r1-modp.asm | 87 ++++++++++++++++++++++++++++++++
 1 file changed, 87 insertions(+)
 create mode 100644 powerpc64/ecc-secp192r1-modp.asm

diff --git a/powerpc64/ecc-secp192r1-modp.asm b/powerpc64/ecc-secp192r1-modp.asm
new file mode 100644
index 00000000..ee38ec60
--- /dev/null
+++ b/powerpc64/ecc-secp192r1-modp.asm
@@ -0,0 +1,87 @@
+C powerpc64/ecc-secp192r1-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Amitay Isaacs, IBM Corporation
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-secp192r1-modp.asm"
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`T0', `r6')
+define(`T1', `r7')
+define(`T2', `r8')
+define(`T3', `r9')
+define(`C1', `r10')
+define(`C2', `r11')
+
+	C void ecc_secp192r1_modp (const struct ecc_modulo *m, mp_limb_t *rp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_secp192r1_modp)
+	ld	T0, 0(XP)
+	ld	T1, 8(XP)
+	ld	T2, 16(XP)
+
+	li	C1, 0
+	li	C2, 0
+
+	ld	T3, 24(XP)
+	addc	T0, T3, T0
+	adde	T1, T3, T1
+	addze	T2, T2
+	addze	C1, C1
+
+	ld	T3, 32(XP)
+	addc	T1, T3, T1
+	adde	T2, T3, T2
+	addze	C1, C1
+
+	ld	T3, 40(XP)
+	addc	T0, T3, T0
+	adde	T1, T3, T1
+	adde	T2, T3, T2
+	addze	C1, C1
+
+	addc	T0, C1, T0
+	adde	T1, C1, T1
+	addze	T2, T2
+	addze	C2, C2
+
+	addc	T0, C2, T0
+	adde	T1, C2, T1
+	addze	T2, T2
+
+	std	T0, 0(RP)
+	std	T1, 8(RP)
+	std	T2, 16(RP)
+
+	blr
+EPILOGUE(_nettle_ecc_secp192r1_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121040234</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:34-0400</timestampReceived><subject>[PATCH v2 2/6] ecc: Add powerpc64 assembly for ecc_224_modp</subject><body>

Signed-off-by: Amitay Isaacs &lt;amitay@ozlabs.org&gt;
---
 powerpc64/ecc-secp224r1-modp.asm | 123 +++++++++++++++++++++++++++++++
 1 file changed, 123 insertions(+)
 create mode 100644 powerpc64/ecc-secp224r1-modp.asm

diff --git a/powerpc64/ecc-secp224r1-modp.asm b/powerpc64/ecc-secp224r1-modp.asm
new file mode 100644
index 00000000..e4bbf366
--- /dev/null
+++ b/powerpc64/ecc-secp224r1-modp.asm
@@ -0,0 +1,123 @@
+C powerpc64/ecc-secp224r1-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Amitay Isaacs, IBM Corporation
+
+   Based on x86_64/ecc-secp224r1-modp.asm
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-secp224r1-modp.asm"
+
+define(`SP', `r1')
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`T0', `r6')
+define(`T1', `r7')
+define(`H0', `r8')
+define(`H1', `r9')
+define(`H2', `r10')
+define(`F0', `r11')
+define(`F1', `r12')
+define(`F2', `r14')
+define(`T2', `r3')
+
+	C void ecc_secp224r1_modp (const struct ecc_modulo *m, mp_limb_t *rp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_secp224r1_modp)
+	std	r14, -8(SP)
+
+	ld	H0, 48(XP)
+	ld	H1, 56(XP)
+	C set (F2, F1, F0) &lt;-- (H1, H0) &lt;&lt; 32
+	sldi	F0, H0, 32
+	srdi	F1, H0, 32
+	sldi	T0, H1, 32
+	srdi	F2, H1, 32
+	or	F1, T0, F1
+
+	li	H2, 0
+	ld	T0, 16(XP)
+	ld	T1, 24(XP)
+	subfc	T0, F0, T0
+	subfe	T1, F1, T1
+	subfe	H0, F2, H0
+	addme	H1, H1
+
+	ld	T2, 32(XP)
+	addc	H0, T2, H0
+	ld	T2, 40(XP)
+	adde	H1, T2, H1
+	addze	H2, H2
+
+	C Set (F2, F1, F0) &lt;-- (H2, H1, H0) &lt;&lt; 32
+	sldi	F0, H0, 32
+	srdi	F1, H0, 32
+	addc	H0, T0, H0
+	sldi	T0, H1, 32
+	srdi	F2, H1, 32
+	adde	H1, T1, H1
+	sldi	T1, H2, 32
+	addze	H2, H2
+	or	F1, T0, F1
+	or	F2, T1, F2
+
+	ld	T0, 0(XP)
+	ld	T1, 8(XP)
+	subfc	T0, F0, T0
+	subfe	T1, F1, T1
+	subfe	H0, F2, H0
+	addme	H1, H1
+	addme	H2, H2
+
+	srdi	F0, H1, 32
+	sldi	F1, H2, 32
+	or	F0, F1, F0
+	clrrdi	F1, H1, 32
+	mr	F2, H2
+	clrldi	H1, H1, 32
+
+	subfc	T0, F0, T0
+	addme	F1, F1
+	addme	F2, F2
+	addc	T1, F1, T1
+	adde	H0, F2, H0
+	addze	H1, H1
+
+	std	T0, 0(RP)
+	std	T1, 8(RP)
+	std	H0, 16(RP)
+	std	H1, 24(RP)
+
+	ld	r14, -8(SP)
+
+	blr
+EPILOGUE(_nettle_ecc_secp224r1_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121040235</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:35-0400</timestampReceived><subject>[PATCH v2 3/6] ecc: Add powerpc64 assembly for ecc_384_modp</subject><body>

From: Martin Schwenke &lt;martin@meltin.net&gt;

Signed-off-by: Martin Schwenke &lt;martin@meltin.net&gt;
Signed-off-by: Amitay Isaacs &lt;amitay@ozlabs.org&gt;
Signed-off-by: Alastair D'Silva &lt;alastair@d-silva.org&gt;
---
 powerpc64/ecc-secp384r1-modp.asm | 227 +++++++++++++++++++++++++++++++
 1 file changed, 227 insertions(+)
 create mode 100644 powerpc64/ecc-secp384r1-modp.asm

diff --git a/powerpc64/ecc-secp384r1-modp.asm b/powerpc64/ecc-secp384r1-modp.asm
new file mode 100644
index 00000000..d673bf1e
--- /dev/null
+++ b/powerpc64/ecc-secp384r1-modp.asm
@@ -0,0 +1,227 @@
+C powerpc64/ecc-secp384r1-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Martin Schwenke, Amitay Isaacs &amp; Alastair D ´Silva, IBM Corporation
+
+   Based on x86_64/ecc-secp256r1-redc.asm
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-secp384r1-modp.asm"
+
+C Register usage:
+
+define(`SP', `r1')
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`D5', `r6')
+define(`T0', `r7')
+define(`T1', `r8')
+define(`T2', `r9')
+define(`T3', `r10')
+define(`T4', `r11')
+define(`T5', `r12')
+define(`H0', `r14')
+define(`H1', `r15')
+define(`H2', `r16')
+define(`H3', `r17')
+define(`H4', `r18')
+define(`H5', `r19')
+define(`C2', `r3')
+define(`C0', H5)	C Overlap
+define(`TMP', XP)	C Overlap
+
+
+	C void ecc_secp384r1_modp (const struct ecc_modulo *m, mp_limb_t *rp, mp_limb_t *xp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_secp384r1_modp)
+
+	std	r14, -48(SP)
+	std	r15, -40(SP)
+	std	r16, -32(SP)
+	std	r17, -24(SP)
+	std	r18, -16(SP)
+	std	r19, -8(SP)
+
+	C First get top 2 limbs, which need folding twice.
+	C B^10 = B^6 + B^4 + 2^32 (B-1)B^4.
+	C We handle the terms as follow:
+	C
+	C B^6: Folded immediatly.
+	C
+	C B^4: Delayed, added in in the next folding.
+	C
+	C 2^32(B-1) B^4: Low half limb delayed until the next
+	C folding. Top 1.5 limbs subtracted and shifter now, resulting
+	C in 2.5 limbs. The low limb saved in D5, high 1.5 limbs added
+	C in.
+
+	ld	H4, 80(XP)
+	ld	H5, 88(XP)
+	C Shift right 32 bits, into H1, H0
+	srdi	H1, H5, 32
+	sldi	D5, H5, 32
+	srdi	H0, H4, 32
+	or	H0, H0, D5
+
+	C	H1 H0
+	C       -  H1 H0
+	C       --------
+	C       H1 H0 D5
+	subfic	D5, H0, 0
+	subfe	H0, H1, H0
+	addme	H1, H1
+
+	li	C2, 0
+	addc	H0, H4, H0
+	adde	H1, H5, H1
+	addze	C2, C2
+
+	C Add in to high part
+	ld	T1, 48(XP)
+	ld	T2, 56(XP)
+	addc	H0, T1, H0
+	adde	H1, T2, H1
+	addze	C2, C2		C Do C2 later
+
+	C +1 term
+	ld	T0, 0(XP)
+	ld	T1, 8(XP)
+	ld	T2, 16(XP)
+	ld	T3, 24(XP)
+	ld	T4, 32(XP)
+	ld	T5, 40(XP)
+	ld	H2, 64(XP)
+	ld	H3, 72(XP)
+	addc	T0, H0, T0
+	adde	T1, H1, T1
+	adde	T2, H2, T2
+	adde	T3, H3, T3
+	adde	T4, H4, T4
+	adde	T5, H5, T5
+	li	C0, 0
+	addze	C0, C0
+
+	C +B^2 term
+	addc	T2, H0, T2
+	adde	T3, H1, T3
+	adde	T4, H2, T4
+	adde	T5, H3, T5
+	addze	C0, C0
+
+	C Shift left, including low half of H4
+	sldi	H4, H4, 32
+	srdi	TMP, H3, 32
+	or	H4, TMP, H4
+
+	sldi	H3, H3, 32
+	srdi	TMP, H2, 32
+	or	H3, TMP, H3
+
+	sldi	H2, H2, 32
+	srdi	TMP, H1, 32
+	or	H2, TMP, H2
+
+	sldi	H1, H1, 32
+	srdi	TMP, H0, 32
+	or	H1, TMP, H1
+
+	sldi	H0, H0, 32
+
+	C   H4 H3 H2 H1 H0  0
+	C  -   H4 H3 H2 H1 H0
+	C  ---------------
+	C   H4 H3 H2 H1 H0 TMP
+
+	subfic	TMP, H0, 0
+	subfe	H0, H1, H0
+	subfe	H1, H2, H1
+	subfe	H2, H3, H2
+	subfe	H3, H4, H3
+	addme	H4, H4
+
+	addc	T0, TMP, T0
+	adde	T1, H0, T1
+	adde	T2, H1, T2
+	adde	T3, H2, T3
+	adde	T4, H3, T4
+	adde	T5, H4, T5
+	addze	C0, C0
+
+	C Remains to add in C2 and C0
+	C Set H1, H0 = (2^96 - 2^32 + 1) C0
+	sldi	H1, C0, 32
+	subfc	H0, H1, C0
+	addme	H1, H1
+
+	C Set H3, H2 = (2^96 - 2^32 + 1) C2
+	sldi	H3, C2, 32
+	subfc	H2, H3, C2
+	addme	H3, H3
+	addc	H2, C0, H2
+
+	li	C0, 0
+	addc	T0, H0, T0
+	adde	T1, H1, T1
+	adde	T2, H2, T2
+	adde	T3, H3, T3
+	adde	T4, C2, T4
+	adde	T5, D5, T5		C Value delayed from initial folding
+	addze	C0, C0
+
+	C Final unlikely carry
+	sldi	H1, C0, 32
+	subfc	H0, H1, C0
+	addme	H1, H1
+
+	addc	T0, H0, T0
+	adde	T1, H1, T1
+	adde	T2, C0, T2
+	addze	T3, T3
+	addze	T4, T4
+	addze	T5, T5
+
+	std	T0, 0(RP)
+	std	T1, 8(RP)
+	std	T2, 16(RP)
+	std	T3, 24(RP)
+	std	T4, 32(RP)
+	std	T5, 40(RP)
+
+	ld	r14, -48(SP)
+	ld	r15, -40(SP)
+	ld	r16, -32(SP)
+	ld	r17, -24(SP)
+	ld	r18, -16(SP)
+	ld	r19, -8(SP)
+
+	blr
+EPILOGUE(_nettle_ecc_secp384r1_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121040236</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:36-0400</timestampReceived><subject>[PATCH v2 4/6] ecc: Add powerpc64 assembly for ecc_521_modp</subject><body>

From: Martin Schwenke &lt;martin@meltin.net&gt;

Signed-off-by: Martin Schwenke &lt;martin@meltin.net&gt;
Signed-off-by: Alastair D'Silva &lt;alastair@d-silva.org&gt;
---
 powerpc64/ecc-secp521r1-modp.asm | 166 +++++++++++++++++++++++++++++++
 1 file changed, 166 insertions(+)
 create mode 100644 powerpc64/ecc-secp521r1-modp.asm

diff --git a/powerpc64/ecc-secp521r1-modp.asm b/powerpc64/ecc-secp521r1-modp.asm
new file mode 100644
index 00000000..e989f9cf
--- /dev/null
+++ b/powerpc64/ecc-secp521r1-modp.asm
@@ -0,0 +1,166 @@
+C powerpc64/ecc-secp521r1-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Martin Schwenke &amp; Alastair D ´Silva, IBM Corporation
+
+   Based on x86_64/ecc-secp521r1-modp.asm
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-secp521r1-modp.asm"
+
+define(`SP', `r1')
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`U0', `r6')
+define(`U1', `r7')
+define(`U2', `r8')
+define(`U3', `r9')
+define(`U4', `r10')
+define(`U5', `r11')
+define(`U6', `r12')
+define(`U7', `r14')
+define(`U8', `r15')
+define(`U9', `r16')
+
+define(`T0', `r3')
+define(`T1', `r17')
+
+
+	C void ecc_secp521r1_modp (const struct ecc_modulo *p, mp_limb_t *rp, mp_limb_t *xp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_secp521r1_modp)
+
+	std	r14, -32(SP)
+	std	r15, -24(SP)
+	std	r16, -16(SP)
+	std	r17, -8(SP)
+
+	C Read top 17 limbs, shift left 55 bits
+	ld	U1, 72(XP)
+	sldi	U0, U1, 55
+	srdi	U1, U1, 9
+
+	ld	T0, 80(XP)
+	srdi	U2, T0, 9
+	sldi	T0, T0, 55
+	or	U1, T0, U1
+
+	ld	T0, 88(XP)
+	srdi	U3, T0, 9
+	sldi	T0, T0, 55
+	or	U2, T0, U2
+
+	ld	T0, 96(XP)
+	srdi	U4, T0, 9
+	sldi	T0, T0, 55
+	or	U3, T0, U3
+
+	ld	T0, 104(XP)
+	srdi	U5, T0, 9
+	sldi	T0, T0, 55
+	or	U4, T0, U4
+
+	ld	T0, 112(XP)
+	srdi	U6, T0, 9
+	sldi	T0, T0, 55
+	or	U5, T0, U5
+
+	ld	T0, 120(XP)
+	srdi	U7, T0, 9
+	sldi	T0, T0, 55
+	or	U6, T0, U6
+
+	ld	T0, 128(XP)
+	srdi	U8, T0, 9
+	sldi	T0, T0, 55
+	or	U7, T0, U7
+
+	ld	T0, 136(XP)
+	srdi	U9, T0, 9
+	sldi	T0, T0, 55
+	or	U8, T0, U8
+
+	ld	T0, 0(XP)
+	ld	T1, 8(XP)
+	addc	U0, T0, U0
+	adde	U1, T1, U1
+	ld	T0, 16(XP)
+	ld	T1, 24(XP)
+	adde	U2, T0, U2
+	adde	U3, T1, U3
+	ld	T0, 32(XP)
+	ld	T1, 40(XP)
+	adde	U4, T0, U4
+	adde	U5, T1, U5
+	ld	T0, 48(XP)
+	ld	T1, 56(XP)
+	adde	U6, T0, U6
+	adde	U7, T1, U7
+	ld	T0, 64(XP)
+	adde	U8, T0, U8
+	addze	U9, U9
+
+	C Top limbs are &lt;U9, U8&gt;. Keep low 9 bits of 8, and fold the
+	C top bits (at most 65 bits).
+	srdi	T0, U8, 9
+	andi.	U8, U8, 0x1ff
+	srdi	T1, U9, 9
+	sldi	U9, U9, 55
+	or	T0, U9, T0
+
+	addc	U0, T0, U0
+	adde	U1, T1, U1
+	addze	U2, U2
+	addze	U3, U3
+	addze	U4, U4
+	addze	U5, U5
+	addze	U6, U6
+	addze	U7, U7
+	addze	U8, U8
+
+	std	U0, 0(RP)
+	std	U1, 8(RP)
+	std	U2, 16(RP)
+	std	U3, 24(RP)
+	std	U4, 32(RP)
+	std	U5, 40(RP)
+	std	U6, 48(RP)
+	std	U7, 56(RP)
+	std	U8, 64(RP)
+
+	ld	r14, -32(SP)
+	ld	r15, -24(SP)
+	ld	r16, -16(SP)
+	ld	r17, -8(SP)
+
+	blr
+EPILOGUE(_nettle_ecc_secp521r1_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121040238</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:38-0400</timestampReceived><subject>[PATCH v2 6/6] ecc: Add powerpc64 assembly for ecc_448_modp</subject><body>

From: Martin Schwenke &lt;martin@meltin.net&gt;

Signed-off-by: Martin Schwenke &lt;martin@meltin.net&gt;
Signed-off-by: Amitay Isaacs &lt;amitay@gmail.com&gt;
---
 powerpc64/ecc-curve448-modp.asm | 174 ++++++++++++++++++++++++++++++++
 1 file changed, 174 insertions(+)
 create mode 100644 powerpc64/ecc-curve448-modp.asm

diff --git a/powerpc64/ecc-curve448-modp.asm b/powerpc64/ecc-curve448-modp.asm
new file mode 100644
index 00000000..42ed1eb1
--- /dev/null
+++ b/powerpc64/ecc-curve448-modp.asm
@@ -0,0 +1,174 @@
+C powerpc/ecc-curve448-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Martin Schwenke &amp; Amitay Isaacs, IBM Corporation
+
+   Based on x86_64/ecc-curve448-modp.asm
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-curve448-modp.asm"
+
+define(`SP', `r1')
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`X0', `r3')
+define(`X1', `r9')
+define(`X2', `r10')
+define(`X3', `r11')
+define(`X4', `r12')
+define(`X5', `r14')
+define(`X6', `r15')
+define(`X7', `r16')
+define(`T0', `r6')
+define(`T1', `r7')
+define(`T2', `r8')
+define(`TT', `r17')
+
+define(`LO', `TT')	C Overlap
+
+	C void ecc_curve448_modp (const struct ecc_modulo *p, mp_limb_t *rp, mp_limb_t *xp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_curve448_modp)
+
+	std	r14, -32(SP)
+	std	r15, -24(SP)
+	std	r16, -16(SP)
+	std	r17, -8(SP)
+
+	C First load the values to be shifted by 32.
+	ld	T0, 88(XP)	C use for X0, X1
+	ld	T1, 96(XP)	C use for X2
+	ld	T2, 104(XP)	C use for X3
+	ld	X4, 56(XP)
+	ld	X5, 64(XP)
+	ld	X6, 72(XP)
+	ld	X7, 80(XP)
+
+	C Multiply by 2^32
+	sldi	X0, T0, 32
+	srdi	LO, T0, 32
+	sldi	X1, T1, 32
+	or	X1, X1, LO
+	srdi	LO, T1, 32
+	sldi	X2, T2, 32
+	or	X2, X2, LO
+	srdi	LO, T2, 32
+	sldi	X3, X4, 32
+	or	X3, X3, LO
+	srdi	LO, X4, 32
+	sldi	X4, X5, 32
+	or	X4, X4, LO
+	srdi	LO, X5, 32
+	sldi	X5, X6, 32
+	or	X5, X5, LO
+	srdi	LO, X6, 32
+	sldi	X6, X7, 32
+	or	X6, X6, LO
+
+	srdi	X7, X7, 32
+
+	C Multiply by 2
+	addc	T0, T0, T0
+	adde	T1, T1, T1
+	adde	T2, T2, T2
+	addze	X7, X7
+
+	C Main additions
+	ld	TT, 56(XP)
+	addc	X0, TT, X0
+	ld	TT, 64(XP)
+	adde	X1, TT, X1
+	ld	TT, 72(XP)
+	adde	X2, TT, X2
+	ld	TT, 80(XP)
+	adde	X3, TT, X3
+	adde	X4, T0, X4
+	adde	X5, T1, X5
+	adde	X6, T2, X6
+	addze	X7, X7
+
+	ld	T0, 0(XP)
+	addc	X0, T0, X0
+	ld	T1, 8(XP)
+	adde	X1, T1, X1
+	ld	T2, 16(XP)
+	adde	X2, T2, X2
+	ld	TT, 24(XP)
+	adde	X3, TT, X3
+	ld	T0, 32(XP)
+	adde	X4, T0, X4
+	ld	T1, 40(XP)
+	adde	X5, T1, X5
+	ld	T2, 48(XP)
+	adde	X6, T2, X6
+	addze	X7, X7
+
+	C X7 wraparound
+	sldi	T0, X7, 32
+	srdi	T1, X7, 32
+	li	T2, 0
+	addc	X0, X7, X0
+	addze	X1, X1
+	addze	X2, X2
+	adde	X3, T0, X3
+	adde	X4, T1, X4
+	addze	X5, X5
+	addze	X6, X6
+	addze	T2, T2
+
+	C Final carry wraparound. Carry T2 &gt; 0 only if
+	C X6 is zero, so carry is absorbed.
+	sldi	T0, T2, 32
+
+	addc	X0, T2, X0
+	addze	X1, X1
+	addze	X2, X2
+	adde	X3, T0, X3
+	addze	X4, X4
+	addze	X5, X5
+	addze	X6, X6
+
+	std	X0, 0(RP)
+	std	X1, 8(RP)
+	std	X2, 16(RP)
+	std	X3, 24(RP)
+	std	X4, 32(RP)
+	std	X5, 40(RP)
+	std	X6, 48(RP)
+
+	ld	r14, -32(SP)
+	ld	r15, -24(SP)
+	ld	r16, -16(SP)
+	ld	r17, -8(SP)
+
+	blr
+EPILOGUE(_nettle_ecc_curve448_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220124183318</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-24 18:33:18-0400</timestampReceived><subject>Re: [PATCH v2 5/6] ecc: Add powerpc64 assembly for ecc_25519_modp</subject><body>

Amitay Isaacs &lt;amitay@ozlabs.org&gt; writes:

&gt; --- /dev/null
&gt; +++ b/powerpc64/ecc-curve25519-modp.asm
&gt; @@ -0,0 +1,101 @@
&gt; +C powerpc64/ecc-25519-modp.asm
&gt; +define(`RP', `r4')
&gt; +define(`XP', `r5')
&gt; +
&gt; +define(`U0', `r6')	C Overlaps unused modulo input
&gt; +define(`U1', `r7')
&gt; +define(`U2', `r8')
&gt; +define(`U3', `r9')
&gt; +define(`T0', `r10')
&gt; +define(`T1', `r11')
&gt; +define(`M', `r12')
&gt; +
&gt; +define(`UN', r3)

Comment seems misplaced, it's UN / r3 that overlaps the unused input,
right?

&gt; +	C void ecc_curve25519_modp (const struct ecc_modulo *p, mp_limb_t *rp, mp_limb_t *xp)
&gt; +	.text
&gt; +define(`FUNC_ALIGN', `5')
&gt; +PROLOGUE(_nettle_ecc_curve25519_modp)
&gt; +
&gt; +	C First fold the limbs affecting bit 255
&gt; +	ld	UN, 56(XP)
&gt; +	li	M, 38
&gt; +	mulhdu	T1, M, UN
&gt; +	mulld	UN, M, UN
&gt; +	ld	U3, 24(XP)
&gt; +	li	T0, 0
&gt; +	addc	U3, UN, U3
&gt; +	adde	T0, T1, T0
&gt; +
&gt; +	ld	UN, 40(XP)
&gt; +	mulhdu	U2, M, UN
&gt; +	mulld	UN, M, UN
&gt; +
&gt; +	addc	U3, U3, U3
&gt; +	adde	T0, T0, T0
&gt; +	srdi	U3, U3, 1	C Undo shift, clear high bit
&gt; +
&gt; +	C Fold the high limb again, together with RP[5]
&gt; +	li	T1, 19
&gt; +	mulld	T0, T1, T0
&gt; +	ld	U0, 0(XP)
&gt; +	ld	U1, 8(XP)
&gt; +	ld	T1, 16(XP)
&gt; +	addc	U0, T0, U0
&gt; +	adde	U1, UN, U1
&gt; +	ld	T0, 32(XP)
&gt; +	adde	U2, U2, T1
&gt; +	addze	U3, U3
&gt; +
&gt; +	mulhdu	T1, M, T0
&gt; +	mulld	T0, M, T0
&gt; +	addc	U0, T0, U0
&gt; +	adde	U1, T1, U1
&gt; +	std	U0, 0(RP)
&gt; +	std	U1, 8(RP)
&gt; +
&gt; +	ld	T0, 48(XP)
&gt; +	mulhdu	T1, M, T0
&gt; +	mulld	UN, M, T0
&gt; +	adde	U2, UN, U2
&gt; +	adde	U3, T1, U3
&gt; +	std	U2, 16(RP)
&gt; +	std	U3, 24(RP)
&gt; +
&gt; +	blr
&gt; +EPILOGUE(_nettle_ecc_curve25519_modp)

Looks good. I must admit that the x86_64 version this is based on is not
so easy to follow.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220124185422</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-24 18:54:22-0400</timestampReceived><subject>Re: [PATCH v2 0/6] Add powerpc64 assembly for elliptic curves</subject><body>

Amitay Isaacs &lt;amitay@ozlabs.org&gt; writes:

&gt; I posted the modified codes in the earlier email thread, but I think
&gt; posting them as a seperate series will make them easier to cherry pick.

Thanks!

&gt; V2 changes:
&gt;   - Use actual register names when storing/restoring from stack
&gt;   - Drop m4 definitions which are not in use
&gt;   - Simplify C2 folding for P192 curve
&gt;
&gt; Amitay Isaacs (2):
&gt;   ecc: Add powerpc64 assembly for ecc_192_modp
&gt;   ecc: Add powerpc64 assembly for ecc_224_modp
&gt;
&gt; Martin Schwenke (4):
&gt;   ecc: Add powerpc64 assembly for ecc_384_modp
&gt;   ecc: Add powerpc64 assembly for ecc_521_modp
&gt;   ecc: Add powerpc64 assembly for ecc_25519_modp
&gt;   ecc: Add powerpc64 assembly for ecc_448_modp

I merged secp192, secp384, secp521 a few days ago. The other three,
secp224, curve25519, curve448 look good too (with one very minor comment
fix which I can take care of). I'll do some local testing, then merge to
master-updates for a run of the ci system, including tests on ppc
big-endian.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220128065927</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-28 06:59:27-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; Great! I believe this is the best we can get for processing one block.

One may be able to squeeze out one or two cycles more using the mulx
extension, which should make it possible to eliminate some of the move
instructions (I don't think moves cost any execution unit resources, but
they do consume decoding resources).

&gt; I'm trying to implement two-way interleaving using AVX extension and
&gt; the main instruction of interest here is 'vpmuludq' that does double
&gt; multiply operation

My manual seems a bit confused if it's called pmuludq or vpmuludq. But
you're thinking of the instruction that does two 32x32 --&gt; 64
multiplies? It will be interesting to see how that works out! It does
half the work compared to a 64 x 64 --&gt; 128 multiply instruction, but
accumulation/folding may get more efficient by using vector registers.
(There seems to also be an avx variant doing four 32x32 --&gt; 64
multiplies, using 256-bit registers).

&gt; the main concern here is there's a shortage of XMM registers as
&gt; there are 16 of them, I'm working on addressing this issue by using memory
&gt; operands of key values for 'vpmuludq' and hope the processor cache do his
&gt; thing here. 

Reading cached values from memory is usally cheap. So probably fine as
long as values modified are kept in registers.

&gt; I'm expecting to complete the assembly implementation tomorrow.

If my analysis of the single-block code is right, I'd expect it to be
rather important to trim number of instructions per block.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220129200937</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-29 20:09:37-0400</timestampReceived><subject>Re: Latency in polynomial evaluation</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt;   Y_2 B^2 + Y_1 B + Y_0 = (X_2 B^2 + X_1 B + X_0) (K_1 B + K_0)  (mod P)
&gt;
&gt; This can be arranged with 6 independent multiply instructions + cheap
&gt; accumulation. (I haven't worked out the details for the ghash case, but
&gt; I do expect that it's rather practival there too).

I've found a rather straight forward way to express that.

Recall that for ghash, due to the bit-reversal, the
multiply operation of interest is 

  M H x^{-128} mod P

where structure of P means that x^{-64} = x^{64] + P_1, and P_1 is a
single word. Split M and H into halves,

  M = M_1 x^{64} + M_0 
  H = H_1 x^{64} + H_0 

The previous notes defines the precomputation of 

  D_1 x^{64} + D_0 = H_0 x^{64} + H_1 + H_0 P_1

Alternatively, D can be defined as D = x^{-64} H. And the accumulation
part can then be written as

  (M_1 x^64 + M_0) H x^{-128} = (M_1 H + M_0 D) x^{-64}

As before, accumulate this in two 128-bit registers R and F, as

  (M_1 x^64 + M_0) H x^{-128} = R + F x^{-64}

with 

  R = M_1 H_1 + M_0 D_1
  F = M_1 H_0 + M_0 D_0

If we add one more unreduced word to M,

  M = M_1 x^{64} + M_0 + M_{-1} x^{-64}

all we need is to precompute one more constant E = H x^{-128} = D
x^{-64}, in the same way

  E_1 x^{64} + E_0 = D x^{-64} = D_0 x^{64} + D_1 + D0 P_1

and we get one more term each for R and F,

  R = M_1 H_1 + M_0 D_1 + M_{-1} E_1
  F = M_1 H_0 + M_0 D_0 + M_{-1} E_2

At the end of the iteration, just add the high half of F into R, but
keep F_0 as an input (the place of the M_{-1}) for the next iteration.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220315122002</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-03-15 12:20:02-0400</timestampReceived><subject>Re: [PATCH v2 0/7] Introduce SM4 symmetric cipher algorithm</subject><body>

Hi Niels,

Thanks for your review, any comments about this series of patches?

Best regards,
Tianjia

On 2/21/22 4:37 PM, Tianjia Zhang wrote:
&gt; SM4 is a block cipher standard published by the government of the People's
&gt; Republic of China, and it was issued by the State Cryptography Administration
&gt; on March 21, 2012. The standard is GM/T 0002-2012 "SM4 block cipher algorithm".
&gt; 
&gt; SM4 algorithm is a symmetric cipher algorithm in ShangMi cryptosystems. The
&gt; block length and key length are both 128 bits. Both the encryption algorithm
&gt; and the key derivation algorithm use 32 rounds of non-linear iterative
&gt; structure, and the S box is a fixed 8 bits. The RFC 8998 specification
&gt; defines the usage of ShangMi algorithm suite in TLS 1.3, etc. According to
&gt; the State Cryptography Administration of China, its security and efficiency
&gt; are equivalent to AES-128.
&gt; 
&gt; Reference specification:
&gt; 1. http://www.gmbz.org.cn/upload/2018-04-04/1522788048733065051.pdf
&gt; 2. http://gmbz.org.cn/main/viewfile/20180108015408199368.html
&gt; 3. https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html
&gt; 4. https://datatracker.ietf.org/doc/html/rfc8998
&gt; 
&gt; ---
&gt; v2 changes:
&gt;    - use separate set_key functions to avoid two copies of the subkeys.
&gt;    - unify encryption and decryption operations with one function.
&gt;    - use unsigned type instead of uint32_t for loop counter i.
&gt;    - use four variables instead of four5-element array.
&gt; 
&gt; Tianjia Zhang (7):
&gt;    doc: Add Copyright of SM3 hash algorithm
&gt;    Introduce SM4 symmetric cipher algorithm
&gt;    testsuite: add test for SM4 symmetric algorithm
&gt;    nettle-benchmark: bench SM4 symmetric algorithm
&gt;    doc: documentation for SM4 cipher algorithm
&gt;    gcm: Add SM4 as the GCM underlying cipher
&gt;    doc: documentation for GCM using SM4 cipher
&gt; 
&gt;   Makefile.in                  |   2 +
&gt;   examples/nettle-benchmark.c  |   2 +
&gt;   gcm-sm4-meta.c               |  60 ++++++++++
&gt;   gcm-sm4.c                    |  81 +++++++++++++
&gt;   gcm.h                        |  25 +++-
&gt;   nettle-meta-aeads.c          |   1 +
&gt;   nettle-meta-ciphers.c        |   1 +
&gt;   nettle-meta.h                |   3 +
&gt;   nettle.texinfo               |  81 +++++++++++++
&gt;   sm4-meta.c                   |  49 ++++++++
&gt;   sm4.c                        | 223 +++++++++++++++++++++++++++++++++++
&gt;   sm4.h                        |  69 +++++++++++
&gt;   testsuite/.gitignore         |   1 +
&gt;   testsuite/Makefile.in        |   2 +-
&gt;   testsuite/gcm-test.c         |  18 +++
&gt;   testsuite/meta-aead-test.c   |   1 +
&gt;   testsuite/meta-cipher-test.c |   3 +-
&gt;   testsuite/sm4-test.c         |  19 +++
&gt;   18 files changed, 638 insertions(+), 3 deletions(-)
&gt;   create mode 100644 gcm-sm4-meta.c
&gt;   create mode 100644 gcm-sm4.c
&gt;   create mode 100644 sm4-meta.c
&gt;   create mode 100644 sm4.c
&gt;   create mode 100644 sm4.h
&gt;   create mode 100644 testsuite/sm4-test.c
&gt; 
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220505184444</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-05 18:44:44-0400</timestampReceived><subject>Re: CBC-AES</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; I'm not that fond of the struct cbc_aes128_ctx though, which includes
&gt; both (constant) subkeys and iv. So I'm considering changing that to
&gt;
&gt;   void
&gt;   cbc_aes128_encrypt(const struct aes128_ctx *ctx, uint8_t *iv,
&gt;                      size_t length, uint8_t *dst, const uint8_t *src);
&gt;
&gt; I.e., similar to cbc_encrypt, but without the arguments
&gt; nettle_cipher_func *f, size_t block_size.

I had almost forgotten about this, but I did push these changes last
autumn (including x86_64 aesni implementation), and I just added
documentation.

If I rememeber correctly, separate assembly functions should bring great
speedup on s390x (and maybe significant speedup also on other archs
where AES is fast, similar to the x86_64 aesni case). Still no
parallelism, so I expect speedup mainly from less overhead, e.g., not
having to reload subkeys, and keeping iv in registers between blocks (or
in the s390x case, special instruction for aes + cbc).

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220516131916</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-05-16 13:19:16-0400</timestampReceived><subject>[PATCH 1/1] Export sha256/sha512_compress functions</subject><body>

nettle export only md5_compress and sha1_compress.
Let's export also the compress functions for sha256 and sha512.

Signed-off-by: Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt;
---
 sha2.h     | 5 ++++-
 sha256.c   | 6 ++++++
 sha3-384.c | 1 +
 sha3.h     | 3 +++
 sha512.c   | 6 ++++++
 5 files changed, 20 insertions(+), 1 deletion(-)

diff --git a/sha2.h b/sha2.h
index ca8222a7..9eefca75 100644
--- a/sha2.h
+++ b/sha2.h
@@ -186,7 +186,10 @@ void
 sha512_256_digest(struct sha512_256_ctx *ctx,
                   size_t length,
                   uint8_t *digest);
-  
+
+void
+nettle_sha256_compress(uint32_t *state, const uint8_t *input);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/sha256.c b/sha256.c
index 253c1319..0ecb6538 100644
--- a/sha256.c
+++ b/sha256.c
@@ -161,3 +161,9 @@ sha224_digest(struct sha256_ctx *ctx,
   sha256_write_digest(ctx, length, digest);
   sha224_init(ctx);
 }
+
+void
+nettle_sha256_compress(uint32_t *state, const uint8_t *input)
+{
+  _nettle_sha256_compress(state, input, K);
+}
diff --git a/sha3-384.c b/sha3-384.c
index 62860248..43181c8f 100644
--- a/sha3-384.c
+++ b/sha3-384.c
@@ -68,3 +68,4 @@ sha3_384_digest(struct sha3_384_ctx *ctx,
   _nettle_write_le64 (length, digest, ctx-&gt;state.a);
   sha3_384_init (ctx);
 }
+
diff --git a/sha3.h b/sha3.h
index 9220829d..913db6e0 100644
--- a/sha3.h
+++ b/sha3.h
@@ -183,6 +183,9 @@ sha3_512_digest(struct sha3_512_ctx *ctx,
 		size_t length,
 		uint8_t *digest);
 
+void
+nettle_sha512_compress(uint64_t *state, const uint8_t *input);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/sha512.c b/sha512.c
index 6936cb50..8b0142c2 100644
--- a/sha512.c
+++ b/sha512.c
@@ -312,3 +312,9 @@ sha512_256_digest(struct sha512_256_ctx *ctx,
   sha512_write_digest(ctx, length, digest);
   sha512_256_init(ctx);
 }
+
+void
+nettle_sha512_compress(uint64_t *state, const uint8_t *input)
+{
+  _nettle_sha512_compress(state, input, K);
+}
-- 
2.35.1

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220517202158</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-17 20:21:58-0400</timestampReceived><subject>Re: [PATCH 0/1] Export-sha256-sha512_compress-functions</subject><body>

Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt; writes:

&gt; Speed is not really a topic for me, I just need them for emulation.
&gt; So I am totally fine with
&gt; ALGONAME_compress(uint32_t *state, const uint8_t *data)

Sounds good.

&gt; But I think the compress_n version could be provided in parallel and
&gt; still usefull for my case.

If there's no compelling use case, I think it's best to not include
compress_n functions in the public api.

&gt; So basicly, do you agree if I resent my patch by simply removing all
&gt; nettle_ prefix ?

Yes, they should use similar name manging as other public functions,
referred to as sha256_compress in the sources, and a 

  #define sha256_compress nettle_sha256_compress 

in the header file. Please also delete the COMPRESS macros in sha256.c
and sha512.c, and use the new function instead (I'd expect use within
the same source file will typically be inlined).

Basic docs (to go in nettle.texinfo) is also appreciated. Additional
tests probably not needed, if the code is exercised by the existing
tests.

&gt; Then a second patch adding sha_compress/md5_compress (But I dont know
&gt; if you want and how to deprecate old nettle_md5_compress and such)

They should follow the same naming and name mangling as the new
functions, except that the aliases with _nettle prefix should be kept
for backwards compatibility.

&gt; And a third patch adding all compress_n functions.

I'm not convinced public compress_n functions will be useful enough to
motiviate the larger api (and I imagine they will cause some additional
complexity when we get to have optional assembly implementation of those
functions). Let's leave out for now.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220523192413</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-23 19:24:13-0400</timestampReceived><subject>Testing for nettle-3.8 release</subject><body>

Hi,

I'd really like to get a release out before summer vacations. I've
updated the NEWS file (see
https://git.lysator.liu.se/nettle/nettle/-/blob/master/NEWS), and
prepared release candidates, latest one being

https://www.lysator.liu.se/~nisse/archive/nettle-3.8rc4.tar.gz
https://www.lysator.liu.se/~nisse/archive/nettle-3.8rc4.tar.gz.sig

I'm relying mainly on the ci testing, but I've made some additional
tests for freebsd, windows (testing with mingw/wine), and a friend gave it
a test run on Mac as well.

Both additional testing, and review of the NEWS file, highly
appreciated. If you want to test on Mac, please be aware that make check
may not work out-of-the-box, see the "Known issue" section in the NEWS
file for details and a workaround.

If all goes well, I hope to make the release one week from now.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220529021700</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-29 02:17:00-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Sat, May 14, 2022 at 8:07 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt;  I created merge requests that have improvements of Poly1305 for arm64,
&gt; &gt; powerpc64, and s390x architectures by following using two-way
&gt; interleaving.
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/38
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/39
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/41
&gt; &gt; The patches have 41.88% speedup for arm64, 142.95% speedup for powerpc64,
&gt; &gt; and 382.65% speedup for s390x.
&gt;
&gt; I've had a closer look at the ppc merge request #39.
&gt;
&gt; I think it would be good to do the single block radix 2^44 version first
&gt; (I'm assuming that's in itself is an improvement over the C code, and
&gt; over using radix 2^64?).


I agree we should go with single block first. It seems radix 2^44 has a
drawback in terms of single block performance in comparison to C code which
is not the case for radix 2^64 that is superior in this matter. I got "657
Mbyte/s" update speed of 2^64 whereas C code (radix 2^26) produces "470
Mbyte/s" of update speed on POWER9. With that said, I've pushed a patch of
2^64 implementation for single block update with fat build support
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/47


&gt; Is 44 bit pieces ideal (130 = 44+44+42), or
&gt; would anything get simpler with, e.g., 130 = 48 + 48 + 34, or 130 = 56 +
&gt; 56 + 18)?
&gt;

For multi-block processing, it seems to me 44 bit pieces are ideal. In case
B = 2^56 I'm trying to figure how to calculate B^3 R_1 5 = 2^38 R_1 5 which
doesn't fit in 64-bit since R_1 of degree 56. For 2^48, I don't see any
difference in equations when comparing with 2^44 for multiplication and
reduction phases.


&gt; For the 4-way code, the name and organization seems inspired by
&gt; chacha_4core, which is a bit different since it also has a four-block
&gt; output, and then the caller has to be aware. I think it would be better
&gt; to look at the recent ghash. Maybe one can have an internal
&gt; _poly1306_update, following similar conventions as _ghash_update? Then
&gt; the C code doesn't need to know how many blocks are done at a time,
&gt; which should make things a bit simpler (although the assembly code would
&gt; need logic to do left-over blocks, just like for ghash).
&gt;

I agree, I've pushed a new MR
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/48 of
poly1306_update implementation for PowerPC based on radix 2^44 for
multi-block processing, and radix 2^64 to handle single-block update. The
threshold can fit within the assembly file which is in this case set to "12
blocks" since radix 2^64 implementation has relatively superior speed. I
like the structure so far, please take a look and let me know if it fits
well so I can implement it for other architectures.


&gt; &gt; OpenSSL is still ahead in terms of performance speed since it uses 4-way
&gt; &gt; interleaving or maybe more!!
&gt; &gt; Increasing the interleaving ways more than two has nothing to do with
&gt; &gt; parallelism since the execution units are already saturated by using
&gt; 2-ways
&gt; &gt; for the three architectures. The reason behind the performance
&gt; improvement
&gt; &gt; is the number of execution times of reduction procedure is cutted by half
&gt; &gt; for 4-way interleaving since the products of multiplying state parts by
&gt; key
&gt; &gt; can be combined before the reduction phase. Let me know if you are
&gt; &gt; interested in doing that on nettle!
&gt;
&gt; Good to know that 2-way is sufficient to saturate execution units. Going
&gt; to 4-way does have a startup cost for each call, since we don't have
&gt; space for extra pre-computed powers. But for large messages, we'll get
&gt; the best speed if we can make reduction as cheap as possible.
&gt;

I understand 4-way has nothing to offer regarding 'vmsumudm' parallelism
for multiplication phase but as you've mentioned reduce the product per
4-blocks implies performance improvement besides increasing parallelism
level for side procedures like R64_TO_R44_4B macro which yield significant
enhancement (approximately double the performance) over 2-way
implementation on powerpc.

regards,
Mamone


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220423153214</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-04-23 15:32:14-0400</timestampReceived><subject>Nettle AUTHORS list (was: Re: Status update)</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; I've also noticed that the copyright/authorship section of the manual
&gt; is very out-of-date, so I'm trying to put together a more
&gt; comprehensive AUTHORS file to replace that.

See below. Based on the author info in the manual, ChangeLog, and git
history. Let me know if you spot any errors.

Regards,
/Niels

---------8&lt;------------
Authors of GNU Nettle

The oldest code in Nettle was copied into precursors of the Nettle
library from other public domain or LGPLv2 sources.

Steve Reid		Original CAST128 implementation
Dana L. How		Original DES implementation,
Colin Plumb		Original MD5 implementation
Peter Gutmann		Original SHA1 implementation
Andrew Kuchling		Original MD2 implementation

Free Software Foundation
			Holds copyrights on code ported from
			libgcrypt, written by Werner Koch and others.

J.H.M. Dassen		Original port of blowfish to GNU lsh, later
			split off into Nettle. (This blowfish
			implementation was replaced later).
			Twofish tests.

Ruud de Rooij		Twofish implementation for GNU lsh, later split off into Nettle.

Rafael Sevilla		AES C and x86 implementation, original port of
			Serpent, for GNU lsh, later split off into Nettle.

Authors of Nettle (in roughly chronological order of initial contribution)

Niels Möller		Main author.

Dan Egnor		Base64 conversion.

Andreas Sigfridsson	Port of MD2, from Andrew Kuchling's python
			cryptography toolkit.

Marcus Comstedt		Implemented MD4.

D. J. Bernstein		Salsa20 public domain reference implementation.

Simon Josefsson		Port of Arctwo, from GnuTLS and libgcrypt. New
			ports of LGPL Serpent and Blowfish code, from
			libgrypt. Port of Salsa20, based on djb's
			reference. Implementation of PBKDF2 (RFC
			2898).

Henrik Grubbström	AES assembly for Sparc64.

Magnus Holmgren		Conversion of DSA keys from DER format to SEXP
			format.

Daniel Kahn Gillmor	Added the lists for nettle_get_ciphers,
			nettle_get_hashes and nettle_get_armors. Test
			vectors for hashes and hmac.

Nippon Telegraph and Telephone Corporation
			LGPL:ed reference implementation of the
			Camellia cipher.

Andrew M. (floodyberry)
			The poly1305 code nettle's C implementation is
			derived from.

Aleksey Kravchenko	The gosthash94 implementation in rhash library.

Stefan Metzmacher, Jeremy Allison, Michael Adam
			Contributed to the CMAC code, which was added
			to Nettle by Nikos Mavroyanopoulos.

Nikos Mavroyanopoulos	GCM implementation, RSA blinding code,
			interface for general rsa-pkcs1 signatures.
			Port of gosthash94 implementation from rhash. Port of
			poly1305. Salsa20r12 variant. HKDF (RFC 5869)
			implementation. CMAC and SIV-CMAV
			implementation. CTR and GCM optimizations.
			Implementation of versioned symbols. Setup of
			.gitlab-ci.yml. Large number of smaller fixes.

Andres Mejia		Ported Ripemd160 from libgrypt.

Martin Storsjö		Implemented m4 macrology to make x86_64
			assembly files work with the windows ABI.
			Several other portability improvements.

Jeronimo Pellegrini	Documentation for base16 and base64 functions.

Tim Ruehsen		Several smaller cleanups and bugfixes.

Fredrik Thulin		Test vectors for pbkdf2-hmac-sha512.

Joachim Strömbergson	Chacha implementation. Support for sha512_224
			and sha512_256.

Owen Kirby		Implementation of CCM mode.

Amos Jeffries		Implementation of base64url encoding.

Daiki Ueno		Implementation of RSA-PSS signatures,
			curve488, shake256, ed448-shake256 signatures,
			chacha functions for 32-bit nonce, struct
			nettle_mac interface.

Dmitry Baryshkov	CFB and CFB8 modes, CMAC64. gosthash94cp and
			Streebog hash functions, GOST DSA signatures
			and curves GC256B and GC512A. Various bug
			fixes and cleanups.

Simo Sorce		Side-channel silent RSA functions. XTS
			implementation.

H.J. Lu			Assembly annotations for Intel "Control-flow
			Enforcement Technology".

Stephen R. van den Berg
			Port of bcrypt.

Mamone Tarsha Kurdi	Powerpc64 assembly and fat build setup,
			including AES and GCM. Arm64 assembly and fat
			build setup, including AES, Chacha, GCM, SHA1,
			SHA256. S390x assembly and fat build setup,
			including AES, Chacha, memxor, memxor3, SHA1,
			SHA256, SHA512, SHA3.

Nicolas Mora		RFC 3394 keywrap.

Tianjia Zhang		SM3 hash function.

Amitay Isaacs		Powerpc64 assembly for secp192r1, secp224r1
			and secp256r1.

Martin Schwenke		Powerpc64 assembly for secp384r1, secp521r1,
			curve25519 and curve448.

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220428191739</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-04-28 19:17:39-0400</timestampReceived><subject>Re: Nettle AUTHORS list</subject><body>

Daiki Ueno &lt;ueno@gnu.org&gt; writes:

&gt;&gt; See below. Based on the author info in the manual, ChangeLog, and git
&gt;&gt; history. Let me know if you spot any errors.
&gt;
&gt; I see a couple of typos: libgrypt → libgcrypt (two places), curve488 →
&gt; curve448.

Thanks.

I've just committed a new AUTHORS file, and a deletion of the
out-of-date author list in the manual.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220430124147</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-04-30 12:41:47-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

I've added Poly1305 optimization based on radix 26 using AVX2 extension for
x86_64 architecture with fat build support, the patch yields significant
speedup compared to upstream.
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/46
I've also fixed the conflicts for PPC, S390x, and Arm64 patches of Poly1305.
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/38
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/39
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/41

regards,
Mamone


On Fri, Jan 28, 2022 at 8:59 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; Great! I believe this is the best we can get for processing one block.
&gt;
&gt; One may be able to squeeze out one or two cycles more using the mulx
&gt; extension, which should make it possible to eliminate some of the move
&gt; instructions (I don't think moves cost any execution unit resources, but
&gt; they do consume decoding resources).
&gt;
&gt; &gt; I'm trying to implement two-way interleaving using AVX extension and
&gt; &gt; the main instruction of interest here is 'vpmuludq' that does double
&gt; &gt; multiply operation
&gt;
&gt; My manual seems a bit confused if it's called pmuludq or vpmuludq. But
&gt; you're thinking of the instruction that does two 32x32 --&gt; 64
&gt; multiplies? It will be interesting to see how that works out! It does
&gt; half the work compared to a 64 x 64 --&gt; 128 multiply instruction, but
&gt; accumulation/folding may get more efficient by using vector registers.
&gt; (There seems to also be an avx variant doing four 32x32 --&gt; 64
&gt; multiplies, using 256-bit registers).
&gt;
&gt; &gt; the main concern here is there's a shortage of XMM registers as
&gt; &gt; there are 16 of them, I'm working on addressing this issue by using
&gt; memory
&gt; &gt; operands of key values for 'vpmuludq' and hope the processor cache do his
&gt; &gt; thing here.
&gt;
&gt; Reading cached values from memory is usally cheap. So probably fine as
&gt; long as values modified are kept in registers.
&gt;
&gt; &gt; I'm expecting to complete the assembly implementation tomorrow.
&gt;
&gt; If my analysis of the single-block code is right, I'd expect it to be
&gt; rather important to trim number of instructions per block.
&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220321200609</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-03-21 20:06:09-0400</timestampReceived><subject>Re: gcm/ghash organization (was Re: x86_64 gcm)</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt;&gt; I've done the needed changes for the C, the x86_64, arm64 and powerpc64
&gt;&gt; implementations. s390x code also needs update, I hope to get to that in
&gt;&gt; a few days (unless someone else wants to do that).
&gt;
&gt; I handled the s390x part and pushed a MR for changes.

I've now merged all these changes to the master branch. It seems the
gnutls build in the ci keeps failing, but that looks unrelated to this
change.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220322102908</emailId><senderName>Justus Winter</senderName><senderEmail>justus@sequoia-pgp.org</senderEmail><timestampReceived>2022-03-22 10:29:08-0400</timestampReceived><subject>Please make OCB a priority (was: Status update)</subject><body>

[Attachment #2 (multipart/signed)]


Hi Niels :)

nisse@lysator.liu.se (Niels M=C3=B6ller) writes:

&gt; At some point, we need to stop to make a new release, despite a lot of
&gt; great ongoing work. Opinions on release priorities and any "must haves"
&gt; before a new release are welcome.

OCB is the only mandatory-to-implement AEAD mode in the upcoming
revision of the OpenPGP standard.  Therefore, we'd like to see Nettle
support it - even if the implementation isn't optimized for speed -
rather sooner than later because of the latency of getting the new
Nettle release into distributions.

https://www.ietf.org/archive/id/draft-ietf-openpgp-crypto-refresh-05.html#n=
ame-aead-algorithms

Thanks,
Justus

PS: https://lists.lysator.liu.se/mailman/listinfo/nettle-bugs 404s

["signature.asc" (application/pgp-signature)]

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se


</body></email><email><emailId>20220118180749</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-18 18:07:49-0400</timestampReceived><subject>[Arm64, S390x] Optimize Chacha20</subject><body>

I created merge requests that have improvements of Chacha20 for arm64 and
s390x architectures by following the approach used in powerpc
implementation.
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/37
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/40
The patches have 80.85% speedup for arm64 arch and 284.79% speedup for
s390x arch.

It would be nice if the arm64 patch will be tested on big-endian mode since
I don't have access to any big-endian variant for testing.

regards,
Mamone
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220104202836</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-04 20:28:36-0400</timestampReceived><subject>Re: [PATCH 4/7] ecc: Add powerpc64 assembly for ecc_384_modp</subject><body>

Amitay Isaacs &lt;amitay@ozlabs.org&gt; writes:

&gt; diff --git a/powerpc64/ecc-secp384r1-modp.asm b/powerpc64/ecc-secp384r1-modp.asm
&gt; new file mode 100644
&gt; index 00000000..67791f09
&gt; --- /dev/null
&gt; +++ b/powerpc64/ecc-secp384r1-modp.asm
&gt; @@ -0,0 +1,227 @@
&gt; +C powerpc64/ecc-secp384r1-modp.asm

This looks nice (and it seems folding scheme is the same as for
the x86_64 version). Just one minor thing,

&gt; +define(`FUNC_ALIGN', `5')
&gt; +PROLOGUE(_nettle_ecc_secp384r1_modp)
&gt; +
&gt; +	std	H0, -48(SP)
&gt; +	std	H1, -40(SP)
&gt; +	std	H2, -32(SP)
&gt; +	std	H3, -24(SP)
&gt; +	std	H4, -16(SP)
&gt; +	std	H5, -8(SP)

I find it clearer to use register names rather than the m4 defines for
save and restore of callee-save registers.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121040237</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:37-0400</timestampReceived><subject>[PATCH v2 5/6] ecc: Add powerpc64 assembly for ecc_25519_modp</subject><body>

From: Martin Schwenke &lt;martin@meltin.net&gt;

Signed-off-by: Martin Schwenke &lt;martin@meltin.net&gt;
Signed-off-by: Alastair D'Silva &lt;alastair@d-silva.org&gt;
---
 powerpc64/ecc-curve25519-modp.asm | 101 ++++++++++++++++++++++++++++++
 1 file changed, 101 insertions(+)
 create mode 100644 powerpc64/ecc-curve25519-modp.asm

diff --git a/powerpc64/ecc-curve25519-modp.asm b/powerpc64/ecc-curve25519-modp.asm
new file mode 100644
index 00000000..8d87eeaf
--- /dev/null
+++ b/powerpc64/ecc-curve25519-modp.asm
@@ -0,0 +1,101 @@
+C powerpc64/ecc-25519-modp.asm
+
+ifelse(`
+   Copyright (C) 2021 Martin Schwenke &amp; Alastair D ´Silva, IBM Corporation
+
+   Based on x86_64/ecc-25519-modp.asm
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+')
+
+	.file "ecc-25519-modp.asm"
+
+define(`RP', `r4')
+define(`XP', `r5')
+
+define(`U0', `r6')	C Overlaps unused modulo input
+define(`U1', `r7')
+define(`U2', `r8')
+define(`U3', `r9')
+define(`T0', `r10')
+define(`T1', `r11')
+define(`M', `r12')
+
+define(`UN', r3)
+
+	C void ecc_curve25519_modp (const struct ecc_modulo *p, mp_limb_t *rp, mp_limb_t *xp)
+	.text
+define(`FUNC_ALIGN', `5')
+PROLOGUE(_nettle_ecc_curve25519_modp)
+
+	C First fold the limbs affecting bit 255
+	ld	UN, 56(XP)
+	li	M, 38
+	mulhdu	T1, M, UN
+	mulld	UN, M, UN
+	ld	U3, 24(XP)
+	li	T0, 0
+	addc	U3, UN, U3
+	adde	T0, T1, T0
+
+	ld	UN, 40(XP)
+	mulhdu	U2, M, UN
+	mulld	UN, M, UN
+
+	addc	U3, U3, U3
+	adde	T0, T0, T0
+	srdi	U3, U3, 1	C Undo shift, clear high bit
+
+	C Fold the high limb again, together with RP[5]
+	li	T1, 19
+	mulld	T0, T1, T0
+	ld	U0, 0(XP)
+	ld	U1, 8(XP)
+	ld	T1, 16(XP)
+	addc	U0, T0, U0
+	adde	U1, UN, U1
+	ld	T0, 32(XP)
+	adde	U2, U2, T1
+	addze	U3, U3
+
+	mulhdu	T1, M, T0
+	mulld	T0, M, T0
+	addc	U0, T0, U0
+	adde	U1, T1, U1
+	std	U0, 0(RP)
+	std	U1, 8(RP)
+
+	ld	T0, 48(XP)
+	mulhdu	T1, M, T0
+	mulld	UN, M, T0
+	adde	U2, UN, U2
+	adde	U3, T1, U3
+	std	U2, 16(RP)
+	std	U3, 24(RP)
+
+	blr
+EPILOGUE(_nettle_ecc_curve25519_modp)
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220212155851</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-12 15:58:51-0400</timestampReceived><subject>Re: [PATCH 2/7] Introduce SM4 symmetric cipher algorithm</subject><body>

Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt; writes:

&gt; Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
&gt; ---
&gt;  Makefile.in                  |   1 +
&gt;  nettle-meta-ciphers.c        |   1 +
&gt;  nettle-meta.h                |   2 +
&gt;  sm4-meta.c                   |  49 ++++++++
&gt;  sm4.c                        | 225 +++++++++++++++++++++++++++++++++++
&gt;  sm4.h                        |  71 +++++++++++
&gt;  testsuite/meta-cipher-test.c |   3 +-
&gt;  7 files changed, 351 insertions(+), 1 deletion(-)
&gt;  create mode 100644 sm4-meta.c
&gt;  create mode 100644 sm4.c
&gt;  create mode 100644 sm4.h

Overall looks pretty good. But I wonder if one could avoid having two
copies of the subkeys.

&gt; +void
&gt; +sm4_set_key(struct sm4_ctx *ctx, const uint8_t *key)
&gt; +{
&gt; +  uint32_t rk[4];
&gt; +  int i;
&gt; +
&gt; +  rk[0] = READ_UINT32(key +  0) ^ fk[0];
&gt; +  rk[1] = READ_UINT32(key +  4) ^ fk[1];
&gt; +  rk[2] = READ_UINT32(key +  8) ^ fk[2];
&gt; +  rk[3] = READ_UINT32(key + 12) ^ fk[3];
&gt; +
&gt; +  for (i = 0; i &lt; 32; i += 4)
&gt; +    {
&gt; +      rk[0] ^= sm4_key_sub(rk[1] ^ rk[2] ^ rk[3] ^ ck[i + 0]);
&gt; +      rk[1] ^= sm4_key_sub(rk[2] ^ rk[3] ^ rk[0] ^ ck[i + 1]);
&gt; +      rk[2] ^= sm4_key_sub(rk[3] ^ rk[0] ^ rk[1] ^ ck[i + 2]);
&gt; +      rk[3] ^= sm4_key_sub(rk[0] ^ rk[1] ^ rk[2] ^ ck[i + 3]);
&gt; +
&gt; +      ctx-&gt;rkey_enc[i + 0] = rk[0];
&gt; +      ctx-&gt;rkey_enc[i + 1] = rk[1];
&gt; +      ctx-&gt;rkey_enc[i + 2] = rk[2];
&gt; +      ctx-&gt;rkey_enc[i + 3] = rk[3];
&gt; +      ctx-&gt;rkey_dec[31 - 0 - i] = rk[0];
&gt; +      ctx-&gt;rkey_dec[31 - 1 - i] = rk[1];
&gt; +      ctx-&gt;rkey_dec[31 - 2 - i] = rk[2];
&gt; +      ctx-&gt;rkey_dec[31 - 3 - i] = rk[3];
&gt; +    }
&gt; +}

So subkeys are identical for encrypt and decrypt, just used in opposite
order? It seems unnecessary to use two copies.

&gt; +static void
&gt; +sm4_crypt_block(const uint32_t *rk, uint8_t *dst, const uint8_t *src)
&gt; +{
&gt; +  uint32_t x[4], i;

The loop counter i should have type plain int (or unsigned; in Nettle I
tend to use unsigned for non-negative values).

&gt; +  x[0] = READ_UINT32(src + 0 * 4);
&gt; +  x[1] = READ_UINT32(src + 1 * 4);
&gt; +  x[2] = READ_UINT32(src + 2 * 4);
&gt; +  x[3] = READ_UINT32(src + 3 * 4);
&gt; +
&gt; +  for (i = 0; i &lt; 32; i += 4)
&gt; +    {
&gt; +      x[0] = sm4_round(x[0], x[1], x[2], x[3], rk[i + 0]);
&gt; +      x[1] = sm4_round(x[1], x[2], x[3], x[0], rk[i + 1]);
&gt; +      x[2] = sm4_round(x[2], x[3], x[0], x[1], rk[i + 2]);
&gt; +      x[3] = sm4_round(x[3], x[0], x[1], x[2], rk[i + 3]);
&gt; +    }

Since the x[] array is indexed only by constants, you could consider
using scalar variables

  uint32_t x0, x1, x2 x3;

Probably makes no difference with modern compilers, but we'd like the
values to be allocated in registers, not as an array on the stack. And
similarly for the rk array above.

&gt; +  WRITE_UINT32(dst + 0 * 4, x[3 - 0]);
&gt; +  WRITE_UINT32(dst + 1 * 4, x[3 - 1]);
&gt; +  WRITE_UINT32(dst + 2 * 4, x[3 - 2]);
&gt; +  WRITE_UINT32(dst + 3 * 4, x[3 - 3]);
&gt; +}

If this is the same for encrypt and decrypt, you could move the block
loop into this function too, to avoid code duplication.

&gt; +void
&gt; +sm4_encrypt(const struct sm4_ctx *context,
&gt; +	    size_t length,
&gt; +	    uint8_t *dst,
&gt; +	    const uint8_t *src)
&gt; +{
&gt; +  const uint32_t *keys = context-&gt;rkey_enc;
[...]
&gt; +void
&gt; +sm4_decrypt(const struct sm4_ctx *context,
&gt; +	    size_t length,
&gt; +	    uint8_t *dst,
&gt; +	    const uint8_t *src)
&gt; +{
&gt; +  const uint32_t *keys = context-&gt;rkey_dec;

I see two alternatives to use only one copy of the subkeys:

1. Keep using a single sm4_set_key function, but store only the first
   copy (currently rkey_enc). Change the sm4_decrypt function to access
   subkeys in the opposite order. To keep a shared sm4_crypt_block
   function, that function would need another +1/-1 argument used to
   determine subkey access order.

2. Implement two separate functions sm4_set_encrypt_key and
   sm4_set_decrypt_key, where the latter stores the subkeys in opposite
   order. sm4_set_decrypt_key could be implemented as
   sm4_set_encrypt_key + sm4_invert_key, where sm4_invert_key is a function
   that just reverses the order (a bit similar to _nettle_aes_invert,
   but simpler). Then the same sm4_crypt function can be used for both
   encrypt and decrypt.

Not sure what's best, but I'd lean towards (2), since simplicity of the
more performance critical encrypt/decrypt operation seems more important
than a simplicity at key setup.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220427090906</emailId><senderName>Daiki Ueno</senderName><senderEmail>ueno@gnu.org</senderEmail><timestampReceived>2022-04-27 09:09:06-0400</timestampReceived><subject>Re: Nettle AUTHORS list</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt;&gt; I've also noticed that the copyright/authorship section of the manual
&gt;&gt; is very out-of-date, so I'm trying to put together a more
&gt;&gt; comprehensive AUTHORS file to replace that.
&gt;
&gt; See below. Based on the author info in the manual, ChangeLog, and git
&gt; history. Let me know if you spot any errors.

I see a couple of typos: libgrypt → libgcrypt (two places), curve488 →
curve448.

Regards,
-- 
Daiki Ueno
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220119184840</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-19 18:48:40-0400</timestampReceived><subject>Re: [Arm64, S390x] Optimize Chacha20</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; I created merge requests that have improvements of Chacha20 for arm64 and
&gt; s390x architectures by following the approach used in powerpc
&gt; implementation.
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/37
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/40
&gt; The patches have 80.85% speedup for arm64 arch and 284.79% speedup for
&gt; s390x arch.

Nice, I've had a quick first look.

&gt; It would be nice if the arm64 patch will be tested on big-endian mode since
&gt; I don't have access to any big-endian variant for testing.

I've merged the arm64 code to a branch, for CI testing.

For the ARM code, which instructions are provided by the asimd
extension? Basic simd is always available, if I've understood correctly.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220119195136</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-19 19:51:36-0400</timestampReceived><subject>Re: [Arm64, S390x] Optimize Chacha20</subject><body>

On Wed, Jan 19, 2022 at 8:48 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; I created merge requests that have improvements of Chacha20 for arm64 and
&gt; &gt; s390x architectures by following the approach used in powerpc
&gt; &gt; implementation.
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/37
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/40
&gt; &gt; The patches have 80.85% speedup for arm64 arch and 284.79% speedup for
&gt; &gt; s390x arch.
&gt;
&gt; Nice, I've had a quick first look.
&gt;
&gt; &gt; It would be nice if the arm64 patch will be tested on big-endian mode
&gt; since
&gt; &gt; I don't have access to any big-endian variant for testing.
&gt;
&gt; I've merged the arm64 code to a branch, for CI testing.
&gt;
&gt; For the ARM code, which instructions are provided by the asimd
&gt; extension? Basic simd is always available, if I've understood correctly.
&gt;

As far as I understand, SIMD is called Advanced SIMD on AArch64 and it's
standard for this architecture. simd is enabled by default in GCC but it
can be disabled with nosimd option as I can see in here
https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html which is why I made
a specific config option for it.

regards,
Mamone


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220120203248</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-20 20:32:48-0400</timestampReceived><subject>Re: [Arm64, S390x] Optimize Chacha20</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; As far as I understand, SIMD is called Advanced SIMD on AArch64 and it's
&gt; standard for this architecture. simd is enabled by default in GCC but it
&gt; can be disabled with nosimd option as I can see in here
&gt; https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html which is why I made
&gt; a specific config option for it.

If it's present on all known aarch64 systems (and HWCAP_ASIMD flag
always set), I think we can keep things simpler and use the code
unconditionally, with no extra subdir, no fat build function pointers or
configure flag.

I've pushed the merge button for the s390x merge request.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220120210842</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-20 21:08:42-0400</timestampReceived><subject>Re: [Arm64, S390x] Optimize Chacha20</subject><body>

On Thu, Jan 20, 2022 at 10:32 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; As far as I understand, SIMD is called Advanced SIMD on AArch64 and it's
&gt; &gt; standard for this architecture. simd is enabled by default in GCC but it
&gt; &gt; can be disabled with nosimd option as I can see in here
&gt; &gt; https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html which is why I
&gt; made
&gt; &gt; a specific config option for it.
&gt;
&gt; If it's present on all known aarch64 systems (and HWCAP_ASIMD flag
&gt; always set), I think we can keep things simpler and use the code
&gt; unconditionally, with no extra subdir, no fat build function pointers or
&gt; configure flag.
&gt;

Ok, I'll commit the changes with vanilla assembly files.


&gt; I've pushed the merge button for the s390x merge request.
&gt;

Nice! I've made various tests on each core function so merging the changes
is gonna be ok.

In another topic, I'm making experiments on your poly1305 optimizing tips
and I'll get back to you once I'm up to something.

regards,
Mamone

Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220207193156</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-07 19:31:56-0400</timestampReceived><subject>Re: Latency in polynomial evaluation</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; On Sat, Jan 29, 2022 at 4:29 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:
&gt;
&gt;&gt; ** Interleaving **
&gt;&gt;
&gt;&gt; The other approach, used in the recent powerpc gcm code, is to
&gt;&gt; interleave multiple blocks. For simplicity, only consider 2-way
&gt;&gt; interleaving here. The key thing is that if we expand te recurrency
&gt;&gt; once, we get
&gt;&gt;
&gt;&gt;   R_j = K (M_j + K (R_{j-2} + M_{j-1}))
&gt;&gt;       = K M_j + K^2 (R_{j-2} + M_{j-1})
&gt;&gt;
&gt;&gt; We get two field multiplications, but one of them, K M_j, is completely
&gt;&gt; independent of previous blocks (R_{j-2}), and can be computed in
&gt;&gt; parallel. It may add a cycle or so to accumulation latency, but we can
&gt;&gt; do essentially twice as much work without making the critical path
&gt;&gt; longer. We get 8 independent multiply instructions, and one dependent
&gt;&gt; for the final folding.
&gt;
&gt; But in radix 2^64, doesn't K^2 need to be represented in 3 words as
&gt; described in a previous note so considering that we get 10 independent
&gt; multiply instructions rather than 8?

For poly1305, yes. For gcm, I don't think so.

&gt;&gt; ** Doing both **
&gt;&gt;
&gt;&gt; It's possible to combine those two tricks. Processing of two blocks
&gt;&gt; would then be an operation of the form
&gt;&gt;
&gt;&gt;   Z_2 B^2 + Z_1 B + Z_0
&gt;&gt;     = (X_2 B^2 + X_1 B + X_0) K^2 + (Y_1 B + Y_0) K
&gt;&gt;
&gt;&gt; Here, the Xs (three words) represent R_{j-2} + M_{j-1}, the Ys repreent
&gt;&gt; M_{j-2}, and the Zs represents R_j, as three words (without final folding).
&gt;&gt; We would need 10 independent multiples, one more than with plain
&gt;&gt; interleaving, but critical path includes only one multiply latency.
&gt;
&gt; Same matter here, if we consider 3 words for K^3 this sums up the number of
&gt; independent multiples to 13.

For poly1305, yes. This seems to be an unavoidable cost of doing
interleaving with poly1305: Structure of K is choosen to make
multiplication particularly convenient, but that doesn't carry over to
K^2. And we'll also have an X_3, of just a few bits. Unclear if it helps
that most significant words are small; clearly multplying top 3 bits of
X by top 3 bits of K^2 can use a short multiply (imul, on x86_64), but
it seems to me that all other products need the more expensive
64x64-&gt;128 multiply instruction.

Also, if we have to use an extra word for poly1305, we could perhaps
also explore going to radix 2^44, if hardware multipliers can support
that.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220212210807</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-12 21:08:07-0400</timestampReceived><subject>x86_64 gcm</subject><body>

Hi,

I've written a first version of a gcm_hash for x86_64, using the
pclmulqdq (carryless mul) instructions. With only a single block at a time,
no interleaving, this gives to 4.3 GByte/s, 0.5 cycles per byte on
my laptop, one pclmulqdq every second cycle. If we could sustain one mul
instruction per cycle, by interleaving, we could perhaps increase
performance by another factor of two.

See below. Configure options and fat setup still missing.

Regards,
/Niels

C x86_64/gcm-hash.asm

ifelse(`
   Copyright (C) 2022 Niels Möller

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

C Common registers

define(`KEY', `%rdi')
define(`P', `%xmm0')
define(`BSWAP', `%xmm1')
define(`H', `%xmm2')
define(`D', `%xmm3')
define(`T', `%xmm4')


    C void gcm_init_key (union gcm_block *table)

PROLOGUE(_nettle_gcm_init_key)
define(`MASK', `%xmm5')
	movdqa	.Lpolynomial(%rip), P
	movdqa	.Lbswap(%rip), BSWAP
	movups	2048(KEY), H	C Middle element
	pshufb	BSWAP, H
	C Multiply by x mod P, which is a left shift.
	movdqa	H, T
	psllq	$1, T
	psrlq	$63, H		C 127 --&gt; 64, 63 --&gt; 0
	pshufd	$0xaa, H, MASK	C 64 --&gt; (96, 64, 32, 0)
	pslldq	$8, H		C 0 --&gt; 64
	por	T, H
	pxor	T, T
	psubd	MASK, T		C All-ones if bit 127 was set
	pand	P, T
	pxor	T, H
	movups	H, (KEY)

	C Set D = x^{-64} H = {H0, H1} + P1 H0
	pshufd	$0x4e, H, D	C Swap H0, H1
	pclmullqhqdq P, H
	pxor	H, D
	movups	D, 16(KEY)
	ret
undefine(`MASK')
EPILOGUE(_nettle_gcm_init_key)

C Use pclmulqdq, doing one 64x64 --&gt; 127 bit carry-less multiplication,
C with source operands being selected from the halves of two 128-bit registers.
C Variants:
C  pclmullqlqdq low half of both src and destination
C  pclmulhqlqdq low half of src register, high half of dst register
C  pclmullqhqdq high half of src register, low half of dst register
C  pclmulhqhqdq high half of both src and destination

C To do a single block, M0, M1, we need to compute
C
C R = M0 D1 + M1 H1
C F = M0 D0 + M1 H0
C
C Corresponding to x^{-127} M H = R + x^{-64} F
C
C Split F as F = F1 + x^64 F0, then the final reduction is
C
C R + x^{-64} F = R + P1 F0 + x^{64} F0 + F1
C
C In all, 5 pclmulqdq. If we we have enough registers to interleave two blocks,
C final reduction is needed only once, so 9 pclmulqdq for two blocks, etc.
C
C We need one register each for D and H, one for P1, one each for accumulating F
C and R. That uses 5 out of the 16 available xmm registers. If we interleave
C blocks, we need additionan D ang H registers (for powers of the key) and the
C additional message word, but we could perhaps interlave as many as 4, with two
C registers left for temporaries.

define(`X', `%rsi')
define(`LENGTH', `%rdx')
define(`DATA', `%rcx')

define(`R', `%xmm5')
define(`M', `%xmm6')
define(`F', `%xmm7')

    C void gcm_hash (const struct gcm_key *key, union gcm_block *x,
    C                size_t length, const uint8_t *data)

PROLOGUE(_nettle_gcm_hash)
	movdqa		.Lpolynomial(%rip), P
	movdqa		.Lbswap(%rip), BSWAP
	movups		(KEY), H
	movups		16(KEY), D
	movups		(X), R
	pshufb		BSWAP, R

	sub		$16, LENGTH
	jc		.Lfinal

.Loop:
	movups		(DATA), M
	pshufb		BSWAP, M
.Lblock:
	pxor		M, R
	movdqa		R, M
	movdqa		R, F
	movdqa		R, T
	pclmullqlqdq	D, F 	C D0 * M0
	pclmullqhqdq	D, R	C D1 * M0
	pclmulhqlqdq	H, T	C H0 * M1
	pclmulhqhqdq	H, M	C H1 * M1
	pxor		T, F
	pxor		M, R

	pshufd		$0x4e, F, T		C Swap halves of F
	pxor		T, R
	pclmullqhqdq	P, F
	pxor		F, R

	add		$16, DATA
	sub		$16, LENGTH
	jnc		.Loop
.Lfinal:
	add		$16, LENGTH
	jnz		.Lpartial

	pshufb		BSWAP, R
	movups		R, (X)
	ret

.Lpartial:
	C Copy zero padded to stack
	mov		%rsp, %r8
	sub		$16, %rsp
	pxor		M, M
	movups		M, (%rsp)
.Lread_loop:
	movb		(DATA), %al
	sub		$1, %r8
	movb		%al, (%r8)
	add		$1, DATA
	sub		$1, LENGTH
	jnz		.Lread_loop

	C Move into M register, jump into loop with LENGTH = 0
	movups		(%rsp), M
	add		$16, %rsp
	jmp		.Lblock

EPILOGUE(_nettle_gcm_hash)

	RODATA
	C The GCM polynomial is x^{128} + x^7 + x^2 + x + 1,
	C but in bit-reversed representation, that is
	C P = x^{128}+ x^{127} + x^{126} + x^{121} + 1
	C We will mainly use the middle part,
	C P1 = (P + a + x^{128}) / x^64 = x^{563} + x^{62} + x^{57}
	ALIGN(16)
.Lpolynomial:
	.byte 1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0xC2
.Lbswap:
	.byte 15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0



-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221060133</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 06:01:33-0400</timestampReceived><subject>Re: [PATCH 2/7] Introduce SM4 symmetric cipher algorithm</subject><body>

Hi

On 2/12/22 11:58 PM, Niels Möller wrote:
&gt; Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt; writes:
&gt; 
&gt;&gt; Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
&gt;&gt; ---
&gt;&gt;   Makefile.in                  |   1 +
&gt;&gt;   nettle-meta-ciphers.c        |   1 +
&gt;&gt;   nettle-meta.h                |   2 +
&gt;&gt;   sm4-meta.c                   |  49 ++++++++
&gt;&gt;   sm4.c                        | 225 +++++++++++++++++++++++++++++++++++
&gt;&gt;   sm4.h                        |  71 +++++++++++
&gt;&gt;   testsuite/meta-cipher-test.c |   3 +-
&gt;&gt;   7 files changed, 351 insertions(+), 1 deletion(-)
&gt;&gt;   create mode 100644 sm4-meta.c
&gt;&gt;   create mode 100644 sm4.c
&gt;&gt;   create mode 100644 sm4.h
&gt; 
&gt; Overall looks pretty good. But I wonder if one could avoid having two
&gt; copies of the subkeys.
&gt; 
&gt;&gt; +void
&gt;&gt; +sm4_set_key(struct sm4_ctx *ctx, const uint8_t *key)
&gt;&gt; +{
&gt;&gt; +  uint32_t rk[4];
&gt;&gt; +  int i;
&gt;&gt; +
&gt;&gt; +  rk[0] = READ_UINT32(key +  0) ^ fk[0];
&gt;&gt; +  rk[1] = READ_UINT32(key +  4) ^ fk[1];
&gt;&gt; +  rk[2] = READ_UINT32(key +  8) ^ fk[2];
&gt;&gt; +  rk[3] = READ_UINT32(key + 12) ^ fk[3];
&gt;&gt; +
&gt;&gt; +  for (i = 0; i &lt; 32; i += 4)
&gt;&gt; +    {
&gt;&gt; +      rk[0] ^= sm4_key_sub(rk[1] ^ rk[2] ^ rk[3] ^ ck[i + 0]);
&gt;&gt; +      rk[1] ^= sm4_key_sub(rk[2] ^ rk[3] ^ rk[0] ^ ck[i + 1]);
&gt;&gt; +      rk[2] ^= sm4_key_sub(rk[3] ^ rk[0] ^ rk[1] ^ ck[i + 2]);
&gt;&gt; +      rk[3] ^= sm4_key_sub(rk[0] ^ rk[1] ^ rk[2] ^ ck[i + 3]);
&gt;&gt; +
&gt;&gt; +      ctx-&gt;rkey_enc[i + 0] = rk[0];
&gt;&gt; +      ctx-&gt;rkey_enc[i + 1] = rk[1];
&gt;&gt; +      ctx-&gt;rkey_enc[i + 2] = rk[2];
&gt;&gt; +      ctx-&gt;rkey_enc[i + 3] = rk[3];
&gt;&gt; +      ctx-&gt;rkey_dec[31 - 0 - i] = rk[0];
&gt;&gt; +      ctx-&gt;rkey_dec[31 - 1 - i] = rk[1];
&gt;&gt; +      ctx-&gt;rkey_dec[31 - 2 - i] = rk[2];
&gt;&gt; +      ctx-&gt;rkey_dec[31 - 3 - i] = rk[3];
&gt;&gt; +    }
&gt;&gt; +}
&gt; 
&gt; So subkeys are identical for encrypt and decrypt, just used in opposite
&gt; order? It seems unnecessary to use two copies.
&gt; 
&gt;&gt; +static void
&gt;&gt; +sm4_crypt_block(const uint32_t *rk, uint8_t *dst, const uint8_t *src)
&gt;&gt; +{
&gt;&gt; +  uint32_t x[4], i;
&gt; 
&gt; The loop counter i should have type plain int (or unsigned; in Nettle I
&gt; tend to use unsigned for non-negative values).
&gt; 
&gt;&gt; +  x[0] = READ_UINT32(src + 0 * 4);
&gt;&gt; +  x[1] = READ_UINT32(src + 1 * 4);
&gt;&gt; +  x[2] = READ_UINT32(src + 2 * 4);
&gt;&gt; +  x[3] = READ_UINT32(src + 3 * 4);
&gt;&gt; +
&gt;&gt; +  for (i = 0; i &lt; 32; i += 4)
&gt;&gt; +    {
&gt;&gt; +      x[0] = sm4_round(x[0], x[1], x[2], x[3], rk[i + 0]);
&gt;&gt; +      x[1] = sm4_round(x[1], x[2], x[3], x[0], rk[i + 1]);
&gt;&gt; +      x[2] = sm4_round(x[2], x[3], x[0], x[1], rk[i + 2]);
&gt;&gt; +      x[3] = sm4_round(x[3], x[0], x[1], x[2], rk[i + 3]);
&gt;&gt; +    }
&gt; 
&gt; Since the x[] array is indexed only by constants, you could consider
&gt; using scalar variables
&gt; 
&gt;    uint32_t x0, x1, x2 x3;
&gt; 
&gt; Probably makes no difference with modern compilers, but we'd like the
&gt; values to be allocated in registers, not as an array on the stack. And
&gt; similarly for the rk array above.
&gt; 
&gt;&gt; +  WRITE_UINT32(dst + 0 * 4, x[3 - 0]);
&gt;&gt; +  WRITE_UINT32(dst + 1 * 4, x[3 - 1]);
&gt;&gt; +  WRITE_UINT32(dst + 2 * 4, x[3 - 2]);
&gt;&gt; +  WRITE_UINT32(dst + 3 * 4, x[3 - 3]);
&gt;&gt; +}
&gt; 
&gt; If this is the same for encrypt and decrypt, you could move the block
&gt; loop into this function too, to avoid code duplication.
&gt; 
&gt;&gt; +void
&gt;&gt; +sm4_encrypt(const struct sm4_ctx *context,
&gt;&gt; +	    size_t length,
&gt;&gt; +	    uint8_t *dst,
&gt;&gt; +	    const uint8_t *src)
&gt;&gt; +{
&gt;&gt; +  const uint32_t *keys = context-&gt;rkey_enc;
&gt; [...]
&gt;&gt; +void
&gt;&gt; +sm4_decrypt(const struct sm4_ctx *context,
&gt;&gt; +	    size_t length,
&gt;&gt; +	    uint8_t *dst,
&gt;&gt; +	    const uint8_t *src)
&gt;&gt; +{
&gt;&gt; +  const uint32_t *keys = context-&gt;rkey_dec;
&gt; 
&gt; I see two alternatives to use only one copy of the subkeys:
&gt; 
&gt; 1. Keep using a single sm4_set_key function, but store only the first
&gt;     copy (currently rkey_enc). Change the sm4_decrypt function to access
&gt;     subkeys in the opposite order. To keep a shared sm4_crypt_block
&gt;     function, that function would need another +1/-1 argument used to
&gt;     determine subkey access order.
&gt; 
&gt; 2. Implement two separate functions sm4_set_encrypt_key and
&gt;     sm4_set_decrypt_key, where the latter stores the subkeys in opposite
&gt;     order. sm4_set_decrypt_key could be implemented as
&gt;     sm4_set_encrypt_key + sm4_invert_key, where sm4_invert_key is a function
&gt;     that just reverses the order (a bit similar to _nettle_aes_invert,
&gt;     but simpler). Then the same sm4_crypt function can be used for both
&gt;     encrypt and decrypt.
&gt; 
&gt; Not sure what's best, but I'd lean towards (2), since simplicity of the
&gt; more performance critical encrypt/decrypt operation seems more important
&gt; than a simplicity at key setup.
&gt; 
&gt; Regards,
&gt; /Niels
&gt; 

sorry for the late reply.

Thanks for your suggestion, it helped me a lot, I agree with your second
suggestion, implement two separate set_key functions, and use one
function to unify encryption and decryption, and also improve the code
with other suggestions you gave, these will be send in a later v2 patch.

Best regards,
Tianjia
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220516131915</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-05-16 13:19:15-0400</timestampReceived><subject>[PATCH 0/1] Export-sha256-sha512_compress-functions</subject><body>

Hello

I am working on implementing crypto offloader devices I use and maintain
in Linux in qemu.
The hardware does not do full hashes offload but only their compress
part. (The driver need to do padding etc...)

From all crypto library, only nettle provides helper for compress but
only for md5 and sha1.

The first one device I implement (sun4i-ss) only do sha1 and md5, so its
fine. But the second (sun8i-ce) need also sha224/sha256/sha384/sha512.

So it is why I propose to export sha256/sha512 compress functions.

Regards

Corentin Labbe (1):
  Export sha256/sha512_compress functions

 sha2.h     | 5 ++++-
 sha256.c   | 6 ++++++
 sha3-384.c | 1 +
 sha3.h     | 3 +++
 sha512.c   | 6 ++++++
 5 files changed, 20 insertions(+), 1 deletion(-)

-- 
2.35.1

_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se
</body></email><email><emailId>20220118182104</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-18 18:21:04-0400</timestampReceived><subject>[Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

 I created merge requests that have improvements of Poly1305 for arm64,
powerpc64, and s390x architectures by following using two-way interleaving.
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/38
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/39
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/41
The patches have 41.88% speedup for arm64, 142.95% speedup for powerpc64,
and 382.65% speedup for s390x.

OpenSSL is still ahead in terms of performance speed since it uses 4-way
interleaving or maybe more!!
Increasing the interleaving ways more than two has nothing to do with
parallelism since the execution units are already saturated by using 2-ways
for the three architectures. The reason behind the performance improvement
is the number of execution times of reduction procedure is cutted by half
for 4-way interleaving since the products of multiplying state parts by key
can be combined before the reduction phase. Let me know if you are
interested in doing that on nettle!

It would be nice if the arm64 patch will be tested on big-endian mode since
I don't have access to any big-endian variant for testing.

regards,
Mamone
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220119200641</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-19 20:06:41-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; The patches have 41.88% speedup for arm64, 142.95% speedup for powerpc64,
&gt; and 382.65% speedup for s390x.
&gt;
&gt; OpenSSL is still ahead in terms of performance speed since it uses 4-way
&gt; interleaving or maybe more!!
&gt; Increasing the interleaving ways more than two has nothing to do with
&gt; parallelism since the execution units are already saturated by using 2-ways
&gt; for the three architectures. The reason behind the performance improvement
&gt; is the number of execution times of reduction procedure is cutted by half
&gt; for 4-way interleaving since the products of multiplying state parts by key
&gt; can be combined before the reduction phase. Let me know if you are
&gt; interested in doing that on nettle!

Interesting. I haven't paid much attention to the poly1305
implementation since it was added back in 2013. The C implementation
doesn't try to use wider multiplication than 32x32 --&gt; 64, which is poor
for 64-bit platforms. Maybe we could use unsigned __int128 if we can
write a configure test to check if it is available and likely to be
efficient?

For most efficient interleaving, I take it one should precompute some
powers of the key, similar to how it's done in the recent gcm code?

&gt; It would be nice if the arm64 patch will be tested on big-endian mode since
&gt; I don't have access to any big-endian variant for testing.

Merged this one too on a branch for ci testing.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220119205445</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-19 20:54:45-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Wed, Jan 19, 2022 at 10:06 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; The patches have 41.88% speedup for arm64, 142.95% speedup for powerpc64,
&gt; &gt; and 382.65% speedup for s390x.
&gt; &gt;
&gt; &gt; OpenSSL is still ahead in terms of performance speed since it uses 4-way
&gt; &gt; interleaving or maybe more!!
&gt; &gt; Increasing the interleaving ways more than two has nothing to do with
&gt; &gt; parallelism since the execution units are already saturated by using
&gt; 2-ways
&gt; &gt; for the three architectures. The reason behind the performance
&gt; improvement
&gt; &gt; is the number of execution times of reduction procedure is cutted by half
&gt; &gt; for 4-way interleaving since the products of multiplying state parts by
&gt; key
&gt; &gt; can be combined before the reduction phase. Let me know if you are
&gt; &gt; interested in doing that on nettle!
&gt;
&gt; Interesting. I haven't paid much attention to the poly1305
&gt; implementation since it was added back in 2013. The C implementation
&gt; doesn't try to use wider multiplication than 32x32 --&gt; 64, which is poor
&gt; for 64-bit platforms. Maybe we could use unsigned __int128 if we can
&gt; write a configure test to check if it is available and likely to be
&gt; efficient?
&gt;

Wider multiplication would improve the performance for 64-bit general
registers but as the case for the current SIMD implementation, the radix
2^26 fits well there.


&gt; For most efficient interleaving, I take it one should precompute some
&gt; powers of the key, similar to how it's done in the recent gcm code?
&gt;

Since the loop of block iteration is moved to inside the assembly
implementation, computing one multiple of key at the function prologue
should be ok.

I forgot to mention that the reduction phase uses the tips instructed in
Reduction section in https://cryptojedi.org/papers/neoncrypto-20120320.pdf
for arm64 and s390x implementations while the chain path of h0 -&gt; h1  -&gt;
h2  -&gt; h3  -&gt; h4  -&gt; h0  -&gt; h1 still manages to achieve slightly higher
performance than the two independent carry path on powerpc64 arch.

regards,
Mamone


&gt; &gt; It would be nice if the arm64 patch will be tested on big-endian mode
&gt; since
&gt; &gt; I don't have access to any big-endian variant for testing.
&gt;
&gt; Merged this one too on a branch for ci testing.
&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220120082515</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-20 08:25:15-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; Wider multiplication would improve the performance for 64-bit general
&gt; registers but as the case for the current SIMD implementation, the radix
&gt; 2^26 fits well there.

If multiply throughput is the bottleneck, it makes sense to do as much
work as possible per multiply. So I don't think I understand the
benefits of interleaving, can you explain?

Let's consider the 64-bit case, since that's less writing. B = 2^64 as
usual. Then the state is

  H = h_2 B^2 + h_1 B + h_0 

(with h_2 rather small, depending on how far we normalize for each
block, lets assume at most 3 bits, or maybe even h_2 &lt;= 4).

  R = r_1 B + r_0

By the spec, high 4 bits of both r_0 and r_1, and low 2 bits of r_1 are
zero, which makes mutliplication R H (mod p) particularly nice.

We get 

  R H = r_0 h_0 + B (r_1 h_0 + r_0 h_1) 
      + B^2 (r_1 h_1 + r_0 h_2) + B^3 r_1 h_2

But then B^2 = 5/4 (mod p), and hence B^2 r_1 = 5 r_1 / 4 (mod p), where
the "/ 4" is just shifting out the two low zero bits. So let r_1' = 5
r_1 / 4,

  R H = r_0 h_0 + r_1' h_1 + B (r_1 h_0 + r_0 h_1 + r_1' h_2 + B r_0 h_2)

These are 4 long multiplications (64x64 --&gt; 128) and two short, 64x64
--&gt; for the products involving h_2. (The 32-bit version would be 16 long
multiplications and 4 short).

From the zero high bits, we also get bounds on these terms,

 f_0 = r_0 h_0 + r_1' h_1 &lt; 2^124 + 5*2^122 = 9*2^122

 f_1 = r_1 h_0 + r_0 h_1 + r_1' h_2 + B r_0 h_2
        &lt; 2^125 + 5*2^61 + 2^127

So these two chains can be added together as 128-bit quantities with no
overflow, in any order, there's plendy of parallelism. E.g., power
vmsumudm might be useful.

For final folding, we need to split f_1 into top 62 and low 66 bits,
multiply low part by 5 (fits in 64 bits), and add into f_0, which still
fits in 128 bits.

And then take the top 64 bits of f_0 and add into f_1 (result &lt;= 2^66
bits).

The current C implementation uses radix 26, and 25 multiplies (32x32
--&gt; 64) per block. And quite a lot of shifts. A radix 32 variant
analogous to the above would need 16 long multiplies and 4 short. I'd
expect that to be faster on most machines, but I'd have to try that out.


In contrast, trying to use a similar scheme for multiplying by (r^2 (mod
p)), as needed for an interleaved version, seems more expensive. There
are several contributions to the cost:

* First, the accumulation of products by power of B needs to take into
  account carry, as result can exceed 2^128, so one would need something
  closer to general schoolbok multiplication.

* Second, since r^2 (mod p) may exceed 2^128, we need three words rather
  than two, so three more short multiplications to add in.

* Third, we can't pre-divide key words by 4, since low bits are no longer
  guaranteed to be zero. This gives more expensive reduction, with more
  multiplies by 5.

The two first points makes smaller radix more attractive; if we need
three words for both factors, we can distribute the bits to ensure some
of the most significant bits are zero. 

&gt; Since the loop of block iteration is moved to inside the assembly
&gt; implementation, computing one multiple of key at the function prologue
&gt; should be ok.

For large messages, that's fine, but may add a significant cost for
messages of just two blocks.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220123191000</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-23 19:10:00-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

nisse@lysator.liu.se (Niels M=C3=B6ller) writes:

&gt; The current C implementation uses radix 26, and 25 multiplies (32x32
&gt; --&gt; 64) per block. And quite a lot of shifts. A radix 32 variant
&gt; analogous to the above would need 16 long multiplies and 4 short. I'd
&gt; expect that to be faster on most machines, but I'd have to try that out.

I've tried this out, see attached file. It has an #if 0/1 to choose
between radix 64 (depending on the non-standard __int128 type for
accumulated products) and radix 32 (portable C).

This is the speed I get for C implementations of poly1305_update on my
x86_64 laptop:

* Radix 26: 1.2 GByte/s (old code)

* Radix 32: 1.3 GByte/s

* Radix 64: 2.2 GByte/s

It would be interesting with benchmarks on actual 32-bit hardware,
32-bit ARM likely being the most relevant arch.

For comparison, the current x86_64 asm version: 2.5 GByte/s.

If I understood correctly, the suggestion to use radix 26 in djb's
original paper was motivated by a high-speed implementation using
floating point arithmetic (possibly in combination with SIMD), where the
product of two 26-bit integers can be represented exactly in an IEEE
double (but it gets a bit subtle if we want to accumulate several
products), I haven't really looked into implementing poly1305 with
either floating point or SIMD.

To improve test coverage, I've also extended poly1305 tests with tests
on random inputs, with results compared to a reference implementation
based on gmp/mini-gmp. I intend to merge those testing changes soon.
See
https://gitlab.com/gnutls/nettle/-/commit/b48217c8058676c8cd2fd12cdeba45775=
5ace309.

Unfortunately, the http interface of the main git repo at Lysator is
inaccessible at the moment due to an expired certificate; should be
fixed in a day or two.

Regards,
/Niels


["poly1305-internal.c" (text/x-csrc)]

/* poly1305-internal.c

   Copyright: 2013 Nikos Mavrogiannopoulos
   Copyright: 2013, 2022 Niels Möller

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
*/

#if HAVE_CONFIG_H
#include "config.h"
#endif

#include &lt;assert.h&gt;
#include &lt;string.h&gt;

#include "poly1305.h"
#include "poly1305-internal.h"

#include "macros.h"

#if 1
typedef unsigned __int128 nettle_uint128_t;

#define M64(a,b) ((nettle_uint128_t)(a) * (b))

#define r0 r.r64[0]
#define r1 r.r64[1]
#define s1 r.r64[2]
#define h0 h.h64[0]
#define h1 h.h64[1]
#define h2 hh

void
_nettle_poly1305_set_key(struct poly1305_ctx *ctx, const uint8_t key[16])
{
  uint64_t t0, t1;
  t0 = LE_READ_UINT64(key);
  t1 = LE_READ_UINT64(key + 8);

  ctx-&gt;r0 = t0 &amp; UINT64_C(0x0ffffffc0fffffff);
  ctx-&gt;r1 = t1 &amp; UINT64_C(0x0ffffffc0ffffffc);
  ctx-&gt;s1 = 5*(ctx-&gt;r1 &gt;&gt; 2);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
}

void
_nettle_poly1305_block (struct poly1305_ctx *ctx, const uint8_t *m, unsigned t2)
{
  uint64_t t0, t1;
  nettle_uint128_t s, f0, f1;

  /* Add in message block */
  t0 = ctx-&gt;h0 + LE_READ_UINT64(m);
  s = (nettle_uint128_t) (t0 &lt; ctx-&gt;h0) + ctx-&gt;h1 + LE_READ_UINT64(m+8);
  t1 = s;
  t2 += (s &gt;&gt; 64) + ctx-&gt;h2;

  /* Key constants are bounded by rk &lt; 2^60, sk &lt; 5*2^58, therefore
     all the fk sums fit in 128 bits without overflow, with at least
     one bit margin. */
  f0 = M64(t0, ctx-&gt;r0) + M64(t1, ctx-&gt;s1);
  f1 = M64(t0, ctx-&gt;r1) + M64(t1, ctx-&gt;r0) + t2 * ctx-&gt;s1
    + ((nettle_uint128_t)(t2 * ctx-&gt;r0) &lt;&lt; 64);

  /* Fold high part of f1. */
  f0 += 5*(f1 &gt;&gt; 66);
  f1 &amp;= ((nettle_uint128_t) 1 &lt;&lt; 66) - 1;
  ctx-&gt;h0 = f0;
  f1 += f0 &gt;&gt; 64;
  ctx-&gt;h1 = f1;
  ctx-&gt;h2 = f1 &gt;&gt; 64;
  assert (ctx-&gt;h2 &lt;= 4);
}

/* Adds digest to the nonce */
void
_nettle_poly1305_digest (struct poly1305_ctx *ctx, union nettle_block16 *s)
{
  uint64_t t0, t1, t2, c1, mask, s0;

  t0 = ctx-&gt;h0;
  t1 = ctx-&gt;h1;
  t2 = ctx-&gt;h2;

  /* Compute resulting carries when adding 5. */
  c1 = t0 &gt; -(UINT64_C(5));
  t2 += (t1 + c1 &lt; c1);

  /* Set if H &gt;= 2^130 - 5 */
  mask = - (t2 &gt;&gt; 2);

  t0 += mask &amp; 5;
  t1 += mask &amp; c1;

  /* FIXME: Take advantage of s being aligned as an unsigned long. */
  s0 = t0 + LE_READ_UINT64(s-&gt;b);
  t1 += (s0 &lt; t0) + LE_READ_UINT64(s-&gt;b+8);

  LE_WRITE_UINT64(s-&gt;b, s0);
  LE_WRITE_UINT64(s-&gt;b+8, t1);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
}
#else
#define M32(a,b) ((uint64_t)(a) * (b))

#define r0 r.r32[0]
#define r1 r.r32[1]
#define r2 r.r32[2]
#define r3 r.r32[3]
#define r4 r.r32[4]
#define s1 r.r32[5]
#define s2 s32[0]
#define s3 s32[1]
#define s4 s32[2]

#define h0 h.h32[0]
#define h1 h.h32[1]
#define h2 h.h32[2]
#define h3 h.h32[3]
#define h4 hh

void
_nettle_poly1305_set_key(struct poly1305_ctx *ctx, const uint8_t key[16])
{
  uint32_t t0,t1,t2,t3;
  t0 = LE_READ_UINT32(key);
  t1 = LE_READ_UINT32(key+4);
  t2 = LE_READ_UINT32(key+8);
  t3 = LE_READ_UINT32(key+12);

  ctx-&gt;r0 = t0 &amp; 0x0fffffff;
  ctx-&gt;r1 = t1 &amp; 0x0ffffffc;
  ctx-&gt;r2 = t2 &amp; 0x0ffffffc;
  ctx-&gt;r3 = t3 &amp; 0x0ffffffc;

  ctx-&gt;s1 = 5*(ctx-&gt;r1 &gt;&gt; 2);
  ctx-&gt;s2 = 5*(ctx-&gt;r2 &gt;&gt; 2);
  ctx-&gt;s3 = 5*(ctx-&gt;r3 &gt;&gt; 2);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
  ctx-&gt;h3 = 0;
  ctx-&gt;h4 = 0;
}

void
_nettle_poly1305_block (struct poly1305_ctx *ctx, const uint8_t *m, unsigned t4)
{
  uint32_t t0, t1, t2, t3;
  uint64_t s, f0, f1, f2, f3;

  /* Add in message block */
  t0 = ctx-&gt;h0 + LE_READ_UINT32(m);
  t0 = s;
  s =  (uint64_t) (t0 &lt; ctx-&gt;h0) + ctx-&gt;h1 + LE_READ_UINT32(m+4);
  t1 = s;
  s =  (s &gt;&gt; 32) + ctx-&gt;h2 + LE_READ_UINT32(m+8);
  t2 = s;
  s =  (s &gt;&gt; 32) + ctx-&gt;h3 + LE_READ_UINT32(m+12);
  t3 = s;
  t4 += (s &gt;&gt; 32) + ctx-&gt;h4;

  /* Key constants are bounded by rk &lt; 2^28, sk &lt; 5*2^26, therefore
     all the fk sums fit in 64 bits without overflow, with at least
     one bit margin. */
  f0 = M32(t0, ctx-&gt;r0) + M32(t1, ctx-&gt;s3) + M32(t2, ctx-&gt;s2) + M32(t3, ctx-&gt;s1);
  f1 = M32(t0, ctx-&gt;r1) + M32(t1, ctx-&gt;r0) + M32(t2, ctx-&gt;s3) + M32(t3, ctx-&gt;s2) + t4 * ctx-&gt;s1;
  f2 = M32(t0, ctx-&gt;r2) + M32(t1, ctx-&gt;r1) + M32(t2, ctx-&gt;r0) + M32(t3, ctx-&gt;s3) + t4 * ctx-&gt;s2;
  f3 = M32(t0, ctx-&gt;r3) + M32(t1, ctx-&gt;r2) + M32(t2, ctx-&gt;r1) + M32(t3, ctx-&gt;r0) + t4 * ctx-&gt;s3
    + ((uint64_t)(t4*ctx-&gt;r0) &lt;&lt; 32);

  /* Fold high part of f3. */
  f0 += 5*(f3 &gt;&gt; 34);
  f3 &amp;= UINT64_C(0x3ffffffff);
  ctx-&gt;h0 = f0;
  f1 += (f0 &gt;&gt; 32);
  ctx-&gt;h1 = f1;
  f2 += (f1 &gt;&gt; 32);
  ctx-&gt;h2 = f2;
  f3 += (f2 &gt;&gt; 32);
  ctx-&gt;h3 = f3;
  ctx-&gt;h4 = f3 &gt;&gt; 32;
  assert (ctx-&gt;h4 &lt;= 4);
}

/* Adds digest to the nonce */
void
_nettle_poly1305_digest (struct poly1305_ctx *ctx, union nettle_block16 *s)
{
  uint32_t t0, t1, t2, t3, t4, c1, c2, c3, mask;
  uint64_t f0, f1, f2;

  t0 = ctx-&gt;h0;
  t1 = ctx-&gt;h1;
  t2 = ctx-&gt;h2;
  t3 = ctx-&gt;h3;
  t4 = ctx-&gt;h4;

  /* Compute resulting carries when adding 5. */
  c1 = (t0 &gt;= 0xfffffffb);
  c2 = (t1 + c1 &lt; c1);
  c3 = (t2 + c2 &lt; t2);
  t4 += (t3 + c3 &lt; t3);

  /* Set if H &gt;= 2^130 - 5 */
  mask = - (t4 &gt;&gt; 2);

  t0 += mask &amp; 5;
  t1 += mask &amp; c1;
  t2 += mask &amp; c2;
  t3 += mask &amp; c3;

  /* FIXME: Take advantage of s being aligned as an unsigned long. */
  f0 = (uint64_t) t0 + LE_READ_UINT32(s-&gt;b);
  f1 = t1 + (f0 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+4);
  f2 = t2 + (f1 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+8);
  t3 += (f2 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+12);

  LE_WRITE_UINT32(s-&gt;b, f0);
  LE_WRITE_UINT32(s-&gt;b+4, f1);
  LE_WRITE_UINT32(s-&gt;b+8, f2);
  LE_WRITE_UINT32(s-&gt;b+12, t3);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
  ctx-&gt;h3 = 0;
  ctx-&gt;h4 = 0;
}
#endif


-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

[Attachment #5 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220123214057</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-23 21:40:57-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Sun, Jan 23, 2022 at 9:10 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt; &gt; The current C implementation uses radix 26, and 25 multiplies (32x32
&gt; &gt; --&gt; 64) per block. And quite a lot of shifts. A radix 32 variant
&gt; &gt; analogous to the above would need 16 long multiplies and 4 short. I'd
&gt; &gt; expect that to be faster on most machines, but I'd have to try that out.
&gt;
&gt; I've tried this out, see attached file. It has an #if 0/1 to choose
&gt; between radix 64 (depending on the non-standard __int128 type for
&gt; accumulated products) and radix 32 (portable C).
&gt;
&gt; This is the speed I get for C implementations of poly1305_update on my
&gt; x86_64 laptop:
&gt;
&gt; * Radix 26: 1.2 GByte/s (old code)
&gt;
&gt; * Radix 32: 1.3 GByte/s
&gt;
&gt; * Radix 64: 2.2 GByte/s
&gt;
&gt; It would be interesting with benchmarks on actual 32-bit hardware,
&gt; 32-bit ARM likely being the most relevant arch.
&gt;
&gt; For comparison, the current x86_64 asm version: 2.5 GByte/s.
&gt;

I made a performance test of this patch on the available architectures I
have access to.

Arm64 (gcc117 gfarm):
* Radix 26: 0.65 GByte/s
* Radix 26 (2-way interleaved): 0.92 GByte/s
* Radix 32: 0.55 GByte/s
* Radix 64: 0.58 GByte/s
POWER9:
* Radix 26: 0.47 GByte/s
* Radix 26 (2-way interleaved): 1.15 GByte/s
* Radix 32: 0.52 GByte/s
* Radix 64: 0.58 GByte/s
Z15:
* Radix 26: 0.65 GByte/s
* Radix 26 (2-way interleaved): 3.17 GByte/s
* Radix 32: 0.82 GByte/s
* Radix 64: 1.22 GByte/s

Apparently, the higher radix version has performance improvements on
x86_64, powerpc, and s390x but this is not the case for arm64 arch where
the performance has a slight hit there.

I tried to compile the new code with -m32 flag on x86_64 but I got
"poly1305-internal.c:46:18: error: ‘__int128' is not supported on this
target". Unfortunately, I don't have access to arm 32-bit too.

Also, I've disassembled the update function of Radix 64 and none of the
architectures has made use of SIMD support (including x86_64 that hasn't
used XMM registers which is standard for this arch, I don't know if gcc
supports such behavior for C compiling but I'm aware that MSVC takes
advantage of that standardization for further optimization on compiled C
code).

I'm trying to implement the radix 64 using SIMD to see if we can get any
performance boost, I'll post the result once I get done with it.

regards,
Mamone


&gt; If I understood correctly, the suggestion to use radix 26 in djb's
&gt; original paper was motivated by a high-speed implementation using
&gt; floating point arithmetic (possibly in combination with SIMD), where the
&gt; product of two 26-bit integers can be represented exactly in an IEEE
&gt; double (but it gets a bit subtle if we want to accumulate several
&gt; products), I haven't really looked into implementing poly1305 with
&gt; either floating point or SIMD.
&gt;
&gt; To improve test coverage, I've also extended poly1305 tests with tests
&gt; on random inputs, with results compared to a reference implementation
&gt; based on gmp/mini-gmp. I intend to merge those testing changes soon.
&gt; See
&gt;
&gt; https://gitlab.com/gnutls/nettle/-/commit/b48217c8058676c8cd2fd12cdeba457755ace309
&gt; .
&gt;
&gt; Unfortunately, the http interface of the main git repo at Lysator is
&gt; inaccessible at the moment due to an expired certificate; should be
&gt; fixed in a day or two.
&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220123225835</emailId><senderName>David Edelsohn</senderName><senderEmail>dje.gcc@gmail.com</senderEmail><timestampReceived>2022-01-23 22:58:35-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Sun, Jan 23, 2022 at 4:41 PM Maamoun TK &lt;maamoun.tk@googlemail.com&gt; wrote:
&gt;
&gt; On Sun, Jan 23, 2022 at 9:10 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:
&gt;
&gt; &gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt; &gt;
&gt; &gt; &gt; The current C implementation uses radix 26, and 25 multiplies (32x32
&gt; &gt; &gt; --&gt; 64) per block. And quite a lot of shifts. A radix 32 variant
&gt; &gt; &gt; analogous to the above would need 16 long multiplies and 4 short. I'd
&gt; &gt; &gt; expect that to be faster on most machines, but I'd have to try that out.
&gt; &gt;
&gt; &gt; I've tried this out, see attached file. It has an #if 0/1 to choose
&gt; &gt; between radix 64 (depending on the non-standard __int128 type for
&gt; &gt; accumulated products) and radix 32 (portable C).
&gt; &gt;
&gt; &gt; This is the speed I get for C implementations of poly1305_update on my
&gt; &gt; x86_64 laptop:
&gt; &gt;
&gt; &gt; * Radix 26: 1.2 GByte/s (old code)
&gt; &gt;
&gt; &gt; * Radix 32: 1.3 GByte/s
&gt; &gt;
&gt; &gt; * Radix 64: 2.2 GByte/s
&gt; &gt;
&gt; &gt; It would be interesting with benchmarks on actual 32-bit hardware,
&gt; &gt; 32-bit ARM likely being the most relevant arch.
&gt; &gt;
&gt; &gt; For comparison, the current x86_64 asm version: 2.5 GByte/s.
&gt; &gt;
&gt;
&gt; I made a performance test of this patch on the available architectures I
&gt; have access to.
&gt;
&gt; Arm64 (gcc117 gfarm):
&gt; * Radix 26: 0.65 GByte/s
&gt; * Radix 26 (2-way interleaved): 0.92 GByte/s
&gt; * Radix 32: 0.55 GByte/s
&gt; * Radix 64: 0.58 GByte/s
&gt; POWER9:
&gt; * Radix 26: 0.47 GByte/s
&gt; * Radix 26 (2-way interleaved): 1.15 GByte/s
&gt; * Radix 32: 0.52 GByte/s
&gt; * Radix 64: 0.58 GByte/s
&gt; Z15:
&gt; * Radix 26: 0.65 GByte/s
&gt; * Radix 26 (2-way interleaved): 3.17 GByte/s
&gt; * Radix 32: 0.82 GByte/s
&gt; * Radix 64: 1.22 GByte/s
&gt;
&gt; Apparently, the higher radix version has performance improvements on
&gt; x86_64, powerpc, and s390x but this is not the case for arm64 arch where
&gt; the performance has a slight hit there.

That might be an artifact of the specific ARM processor
microarchitecture or the memory subsystem of the ARM system, not
inherent to the Arm AArch64 architecture and ISA.

- David

&gt;
&gt; I tried to compile the new code with -m32 flag on x86_64 but I got
&gt; "poly1305-internal.c:46:18: error: ‘__int128' is not supported on this
&gt; target". Unfortunately, I don't have access to arm 32-bit too.
&gt;
&gt; Also, I've disassembled the update function of Radix 64 and none of the
&gt; architectures has made use of SIMD support (including x86_64 that hasn't
&gt; used XMM registers which is standard for this arch, I don't know if gcc
&gt; supports such behavior for C compiling but I'm aware that MSVC takes
&gt; advantage of that standardization for further optimization on compiled C
&gt; code).
&gt;
&gt; I'm trying to implement the radix 64 using SIMD to see if we can get any
&gt; performance boost, I'll post the result once I get done with it.
&gt;
&gt; regards,
&gt; Mamone
&gt;
&gt;
&gt; &gt; If I understood correctly, the suggestion to use radix 26 in djb's
&gt; &gt; original paper was motivated by a high-speed implementation using
&gt; &gt; floating point arithmetic (possibly in combination with SIMD), where the
&gt; &gt; product of two 26-bit integers can be represented exactly in an IEEE
&gt; &gt; double (but it gets a bit subtle if we want to accumulate several
&gt; &gt; products), I haven't really looked into implementing poly1305 with
&gt; &gt; either floating point or SIMD.
&gt; &gt;
&gt; &gt; To improve test coverage, I've also extended poly1305 tests with tests
&gt; &gt; on random inputs, with results compared to a reference implementation
&gt; &gt; based on gmp/mini-gmp. I intend to merge those testing changes soon.
&gt; &gt; See
&gt; &gt;
&gt; &gt; https://gitlab.com/gnutls/nettle/-/commit/b48217c8058676c8cd2fd12cdeba457755ace309
&gt; &gt; .
&gt; &gt;
&gt; &gt; Unfortunately, the http interface of the main git repo at Lysator is
&gt; &gt; inaccessible at the moment due to an expired certificate; should be
&gt; &gt; fixed in a day or two.
&gt; &gt;
&gt; &gt; Regards,
&gt; &gt; /Niels
&gt; &gt;
&gt; &gt;
&gt; &gt; --
&gt; &gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; &gt; Internet email is subject to wholesale government surveillance.
&gt; &gt;
&gt; _______________________________________________
&gt; nettle-bugs mailing list
&gt; nettle-bugs@lists.lysator.liu.se
&gt; http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220124075920</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-24 07:59:20-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; I made a performance test of this patch on the available architectures I
&gt; have access to.
&gt;
&gt; Arm64 (gcc117 gfarm):
&gt; * Radix 26: 0.65 GByte/s
&gt; * Radix 26 (2-way interleaved): 0.92 GByte/s
&gt; * Radix 32: 0.55 GByte/s
&gt; * Radix 64: 0.58 GByte/s
&gt; POWER9:
&gt; * Radix 26: 0.47 GByte/s
&gt; * Radix 26 (2-way interleaved): 1.15 GByte/s
&gt; * Radix 32: 0.52 GByte/s
&gt; * Radix 64: 0.58 GByte/s
&gt; Z15:
&gt; * Radix 26: 0.65 GByte/s
&gt; * Radix 26 (2-way interleaved): 3.17 GByte/s
&gt; * Radix 32: 0.82 GByte/s
&gt; * Radix 64: 1.22 GByte/s

Interesting. I'm a bit surprised the radix-64 doesn't perform better, in
particular on arm64. (But I'm not yet familiar with arm64 multiply
instructions).

Numbers for 2-way interleaving are impressive, I'd like to understand
how that works. Might be useful derive corresponding multiply
throughput, i.e., number of multiply operations (and with which multiply
instruction) completed per cycle, as well as total cycles per block

It looks like the folding done per-block in the radix-64 code costs at
least 5 or so cycles per block (since these operations are all
dependent, and we also have the multiply by 5 in there, probably adding
a few cycles more). Maybe at least the multiply can be postponed.

&gt; I tried to compile the new code with -m32 flag on x86_64 but I got
&gt; "poly1305-internal.c:46:18: error: ‘__int128' is not supported on this
&gt; target".

That's expected, in two ways: I don't expect radix-64 to give any
performance gain over radix-32 on any 32-bit archs. And I think __int128
is supported only on archs where it fits in two registers. If we start
using __int128 we need a configure test for it, and then it actually
makes things simpler, at least for this in this usecase, if it stays
unsupported on 32-bit archs where it shouldn't be used.

So to compile with -m32, the radix-64 code must be #if:ed out.

&gt; Also, I've disassembled the update function of Radix 64 and none of the
&gt; architectures has made use of SIMD support (including x86_64 that hasn't
&gt; used XMM registers which is standard for this arch, I don't know if gcc
&gt; supports such behavior for C compiling but I'm aware that MSVC takes
&gt; advantage of that standardization for further optimization on compiled C
&gt; code).

The radix-64 code really wants multiply instruction(s) for 64x64 --&gt;
128, and I think that's not so common SIMD instruction sets (but
powerpc64 vmsumudm looks potentially useful?) Either as a
single instruction, or as a pair of mulhigh/mullow instructions. And
some not too complicated way to do a 128-bit add with proper carry
propagation in the middle.

Arm32 neon does have 32x32 --&gt; 64, which looks like a good fit for the
radix-32 variant.

Regards,
/Niels
-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220124205654</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-24 20:56:54-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; This is the speed I get for C implementations of poly1305_update on my
&gt; x86_64 laptop:
&gt;
&gt; * Radix 26: 1.2 GByte/s (old code)
&gt;
&gt; * Radix 32: 1.3 GByte/s
&gt;
&gt; * Radix 64: 2.2 GByte/s
&gt;
&gt; It would be interesting with benchmarks on actual 32-bit hardware,
&gt; 32-bit ARM likely being the most relevant arch.
&gt;
&gt; For comparison, the current x86_64 asm version: 2.5 GByte/s.

I've tried reworking folding, to reduce latency. Idea is to let the most
significant state word be close to a word, rather than limited to &lt;= 4
as in the previous version. When multiplying by r, split one of the
multiplies to take out the low 2 bits. For the radix 64 version, that
term is

  B^2 t_2 * r0

Split t_2 as 4*hi + lo, then this can be reduced to

  B^2 lo * r0 + hi * 5*r0

(Using the same old B^2 = 5/4 (mod p) in a slightly different way).

The 5*r0 fits one word and can be precomputed, and then this
multiplication goes in parallell with the other multiplies, and no
multiply left in the final per-block folding. With this trick I get on
the same machine

Radix 32: 1.65 GByte/s

Radix 64: 2.75 GByte/s, i.e., faster than current x86_64 asm version.

I haven't yet done a strict analysis of bounds on the state and
temporaries, but I would expect that it works out with no possibility of
overflow.

See attached file. To fit the precomputed 5*r0 in a nice way I had to
rearrange the unions in struct poly1305_ctx a bit, I also attach the
patch to do this. Size of the struct should be the same, so I think it
can be done without any abi bump.

Regards,
/Niels


[Attachment #3 (text/x-diff)]

diff --git a/poly1305.h b/poly1305.h
index 99c63c8a..6c13a590 100644
--- a/poly1305.h
+++ b/poly1305.h
@@ -55,18 +55,15 @@ struct poly1305_ctx {
   /* Key, 128-bit value and some cached multiples. */
   union
   {
-    uint32_t r32[6];
-    uint64_t r64[3];
+    uint32_t r32[8];
+    uint64_t r64[4];
   } r;
-  uint32_t s32[3];
   /* State, represented as words of 26, 32 or 64 bits, depending on
      implementation. */
-  /* High bits first, to maintain alignment. */
-  uint32_t hh;
   union
   {
-    uint32_t h32[4];
-    uint64_t h64[2];
+    uint32_t h32[6];
+    uint64_t h64[3];
   } h;
 };
 

["poly1305-internal.c" (text/x-csrc)]

/* poly1305-internal.c

   Copyright: 2013 Nikos Mavrogiannopoulos
   Copyright: 2013, 2022 Niels Möller

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
*/

#if HAVE_CONFIG_H
#include "config.h"
#endif

#include &lt;assert.h&gt;
#include &lt;string.h&gt;

#include "poly1305.h"
#include "poly1305-internal.h"

#include "macros.h"

#if 1
typedef unsigned __int128 nettle_uint128_t;

#define M64(a,b) ((nettle_uint128_t)(a) * (b))

#define r0 r.r64[0]
#define r1 r.r64[1]
#define s0 r.r64[2]
#define s1 r.r64[3]
#define h0 h.h64[0]
#define h1 h.h64[1]
#define h2 h.h64[2]

void
_nettle_poly1305_set_key(struct poly1305_ctx *ctx, const uint8_t key[16])
{
  uint64_t t0, t1;
  t0 = LE_READ_UINT64 (key);
  t1 = LE_READ_UINT64 (key + 8);

  ctx-&gt;r0 = t0 &amp; UINT64_C (0x0ffffffc0fffffff);
  ctx-&gt;r1 = t1 &amp; UINT64_C (0x0ffffffc0ffffffc);
  ctx-&gt;s0 = 5*ctx-&gt;r0;
  ctx-&gt;s1 = 5*(ctx-&gt;r1 &gt;&gt; 2);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
}

void
_nettle_poly1305_block (struct poly1305_ctx *ctx, const uint8_t *m, unsigned m128)
{
  uint64_t t0, t1, t2;
  nettle_uint128_t s, f0, f1;

  /* Add in message block */
  t0 = ctx-&gt;h0 + LE_READ_UINT64(m);
  s = (nettle_uint128_t) ctx-&gt;h1 + (t0 &lt; ctx-&gt;h0) + LE_READ_UINT64(m+8);
  t1 = s;
  t2 = ctx-&gt;h2 + (s &gt;&gt; 64) + m128;

  /* Key constants are bounded by rk &lt; 2^60, sk &lt; 5*2^58, therefore
     all the fk sums fit in 128 bits without overflow, with at least
     one bit margin. */
  f0 = M64(t0, ctx-&gt;r0) + M64(t1, ctx-&gt;s1) + M64(t2 &gt;&gt; 2, ctx-&gt;s0);
  f1 = M64(t0, ctx-&gt;r1) + M64(t1, ctx-&gt;r0) + M64(t2, ctx-&gt;s1)
    + ((nettle_uint128_t)((t2 &amp; 3) * ctx-&gt;r0) &lt;&lt; 64);

  ctx-&gt;h0 = f0;
  f1 += f0 &gt;&gt; 64;
  ctx-&gt;h1 = f1;
  ctx-&gt;h2 = f1 &gt;&gt; 64;
}

/* Adds digest to the nonce */
void
_nettle_poly1305_digest (struct poly1305_ctx *ctx, union nettle_block16 *s)
{
  uint64_t t0, t1, t2, c0, c1, mask, m0;
  t0 = ctx-&gt;h0;
  t1 = ctx-&gt;h1;
  t2 = ctx-&gt;h2;

  /* Fold high part of t2 */
  c0 = 5 * (t2 &gt;&gt; 2);
  t2 &amp;= 3;

  t0 += c0; c1 = (t0 &lt; c0);
  t1 += c1;
  t2 += (t1 &lt; c1);

  /* Compute resulting carries when adding 5. */
  c1 = t0 &gt; -(UINT64_C(5));
  t2 += (t1 + c1 &lt; c1);

  /* Set if H &gt;= 2^130 - 5 */
  mask = - (t2 &gt;&gt; 2);

  t0 += mask &amp; 5;
  t1 += mask &amp; c1;

  /* FIXME: Take advantage of s being aligned as an unsigned long. */
  m0 = LE_READ_UINT64(s-&gt;b);
  t0 += m0;
  t1 += (t0 &lt; m0) + LE_READ_UINT64(s-&gt;b+8);

  LE_WRITE_UINT64(s-&gt;b, t0);
  LE_WRITE_UINT64(s-&gt;b+8, t1);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
}
#else
#define M32(a,b) ((uint64_t)(a) * (b))

#define r0 r.r32[0]
#define r1 r.r32[1]
#define r2 r.r32[2]
#define r3 r.r32[3]
#define s0 r.r32[4]
#define s1 r.r32[5]
#define s2 r.r32[6]
#define s3 r.r32[7]

#define h0 h.h32[0]
#define h1 h.h32[1]
#define h2 h.h32[2]
#define h3 h.h32[3]
#define h4 h.h32[4]

void
_nettle_poly1305_set_key(struct poly1305_ctx *ctx, const uint8_t key[16])
{
  uint32_t t0, t1, t2, t3;
  t0 = LE_READ_UINT32 (key);
  t1 = LE_READ_UINT32 (key+4);
  t2 = LE_READ_UINT32 (key+8);
  t3 = LE_READ_UINT32 (key+12);

  ctx-&gt;r0 = t0 &amp; 0x0fffffff;
  ctx-&gt;r1 = t1 &amp; 0x0ffffffc;
  ctx-&gt;r2 = t2 &amp; 0x0ffffffc;
  ctx-&gt;r3 = t3 &amp; 0x0ffffffc;

  ctx-&gt;s0 = 5*ctx-&gt;r0;
  ctx-&gt;s1 = 5*(ctx-&gt;r1 &gt;&gt; 2);
  ctx-&gt;s2 = 5*(ctx-&gt;r2 &gt;&gt; 2);
  ctx-&gt;s3 = 5*(ctx-&gt;r3 &gt;&gt; 2);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
  ctx-&gt;h3 = 0;
  ctx-&gt;h4 = 0;
}

void
_nettle_poly1305_block (struct poly1305_ctx *ctx, const uint8_t *m, unsigned m128)
{
  uint32_t t0, t1, t2, t3, t4;
  uint64_t s, f0, f1, f2, f3;

  /* Add in message block */
  t0 = ctx-&gt;h0 + LE_READ_UINT32(m);
  s = (uint64_t) ctx-&gt;h1 + (t0 &lt; ctx-&gt;h0) + LE_READ_UINT32(m+4);
  t1 = s;
  s = ctx-&gt;h2 + (s &gt;&gt; 32) + LE_READ_UINT32(m+8);
  t2 = s;
  s = ctx-&gt;h3 + (s &gt;&gt; 32) + LE_READ_UINT32(m+12);
  t3 = s;
  t4 = ctx-&gt;h4 + (s &gt;&gt; 32) + m128;

  /* Key constants are bounded by rk &lt; 2^28, sk &lt; 5*2^26, therefore
     all the fk sums fit in 64 bits without overflow, with at least
     one bit margin. */
  f0 = M32(t0, ctx-&gt;r0) + M32(t1, ctx-&gt;s3) + M32(t2, ctx-&gt;s2) + M32(t3, ctx-&gt;s1)
    + M32(t4 &gt;&gt; 2, ctx-&gt;s0);
  f1 = M32(t0, ctx-&gt;r1) + M32(t1, ctx-&gt;r0) + M32(t2, ctx-&gt;s3) + M32(t3, ctx-&gt;s2)
    + M32(t4, ctx-&gt;s1);
  f2 = M32(t0, ctx-&gt;r2) + M32(t1, ctx-&gt;r1) + M32(t2, ctx-&gt;r0) + M32(t3, ctx-&gt;s3)
    + M32(t4, ctx-&gt;s2);
  f3 = M32(t0, ctx-&gt;r3) + M32(t1, ctx-&gt;r2) + M32(t2, ctx-&gt;r1) + M32(t3, ctx-&gt;r0)
    + M32(t4, ctx-&gt;s3) + ((uint64_t)((t4 &amp; 3)*ctx-&gt;r0) &lt;&lt; 32);

  ctx-&gt;h0 = f0;
  f1 += f0 &gt;&gt; 32;
  ctx-&gt;h1 = f1;
  f2 += f1 &gt;&gt; 32;
  ctx-&gt;h2 = f2;
  f3 += f2 &gt;&gt; 32;
  ctx-&gt;h3 = f3;
  ctx-&gt;h4 = f3 &gt;&gt; 32;
}

/* Adds digest to the nonce */
void
_nettle_poly1305_digest (struct poly1305_ctx *ctx, union nettle_block16 *s)
{
  uint32_t t0, t1, t2, t3, t4, c0, c1, c2, c3, mask;
  uint64_t f0, f1, f2;

  t0 = ctx-&gt;h0;
  t1 = ctx-&gt;h1;
  t2 = ctx-&gt;h2;
  t3 = ctx-&gt;h3;
  t4 = ctx-&gt;h4;

  /* Fold high part of t4 */
  c0 = 5 * (t4 &gt;&gt; 2);
  t4 &amp;= 3;
  t0 += c0; c1 = (t0 &lt; c0);
  t1 += c1; c2 = (t1 &lt; c1);
  t2 += c2; c3 = (t2 &lt; c2);
  t3 += c3;
  t4 += (t3 &lt; c3);

  /* Compute resulting carries when adding 5. */
  c1 = (t0 &gt;= 0xfffffffb);
  c2 = (t1 + c1 &lt; c1);
  c3 = (t2 + c2 &lt; t2);
  t4 += (t3 + c3 &lt; t3);

  /* Set if H &gt;= 2^130 - 5 */
  mask = - (t4 &gt;&gt; 2);

  t0 += mask &amp; 5;
  t1 += mask &amp; c1;
  t2 += mask &amp; c2;
  t3 += mask &amp; c3;

  /* FIXME: Take advantage of s being aligned as an unsigned long. */
  f0 = (uint64_t) t0 + LE_READ_UINT32(s-&gt;b);
  f1 = t1 + (f0 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+4);
  f2 = t2 + (f1 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+8);
  t3 += (f2 &gt;&gt; 32) + LE_READ_UINT32(s-&gt;b+12);

  LE_WRITE_UINT32(s-&gt;b, f0);
  LE_WRITE_UINT32(s-&gt;b+4, f1);
  LE_WRITE_UINT32(s-&gt;b+8, f2);
  LE_WRITE_UINT32(s-&gt;b+12, t3);

  ctx-&gt;h0 = 0;
  ctx-&gt;h1 = 0;
  ctx-&gt;h2 = 0;
  ctx-&gt;h3 = 0;
  ctx-&gt;h4 = 0;
}
#endif


-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

[Attachment #6 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220125184414</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-25 18:44:14-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Mon, Jan 24, 2022 at 12:58 AM David Edelsohn &lt;dje.gcc@gmail.com&gt; wrote:

&gt; On Sun, Jan 23, 2022 at 4:41 PM Maamoun TK &lt;maamoun.tk@googlemail.com&gt;
&gt; wrote:
&gt; &gt;
&gt; &gt; On Sun, Jan 23, 2022 at 9:10 PM Niels Möller &lt;nisse@lysator.liu.se&gt;
&gt; wrote:
&gt; &gt;
&gt; &gt; &gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt; &gt; &gt;
&gt; &gt; &gt; &gt; The current C implementation uses radix 26, and 25 multiplies (32x32
&gt; &gt; &gt; &gt; --&gt; 64) per block. And quite a lot of shifts. A radix 32 variant
&gt; &gt; &gt; &gt; analogous to the above would need 16 long multiplies and 4 short. I'd
&gt; &gt; &gt; &gt; expect that to be faster on most machines, but I'd have to try that
&gt; out.
&gt; &gt; &gt;
&gt; &gt; &gt; I've tried this out, see attached file. It has an #if 0/1 to choose
&gt; &gt; &gt; between radix 64 (depending on the non-standard __int128 type for
&gt; &gt; &gt; accumulated products) and radix 32 (portable C).
&gt; &gt; &gt;
&gt; &gt; &gt; This is the speed I get for C implementations of poly1305_update on my
&gt; &gt; &gt; x86_64 laptop:
&gt; &gt; &gt;
&gt; &gt; &gt; * Radix 26: 1.2 GByte/s (old code)
&gt; &gt; &gt;
&gt; &gt; &gt; * Radix 32: 1.3 GByte/s
&gt; &gt; &gt;
&gt; &gt; &gt; * Radix 64: 2.2 GByte/s
&gt; &gt; &gt;
&gt; &gt; &gt; It would be interesting with benchmarks on actual 32-bit hardware,
&gt; &gt; &gt; 32-bit ARM likely being the most relevant arch.
&gt; &gt; &gt;
&gt; &gt; &gt; For comparison, the current x86_64 asm version: 2.5 GByte/s.
&gt; &gt; &gt;
&gt; &gt;
&gt; &gt; I made a performance test of this patch on the available architectures I
&gt; &gt; have access to.
&gt; &gt;
&gt; &gt; Arm64 (gcc117 gfarm):
&gt; &gt; * Radix 26: 0.65 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 0.92 GByte/s
&gt; &gt; * Radix 32: 0.55 GByte/s
&gt; &gt; * Radix 64: 0.58 GByte/s
&gt; &gt; POWER9:
&gt; &gt; * Radix 26: 0.47 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 1.15 GByte/s
&gt; &gt; * Radix 32: 0.52 GByte/s
&gt; &gt; * Radix 64: 0.58 GByte/s
&gt; &gt; Z15:
&gt; &gt; * Radix 26: 0.65 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 3.17 GByte/s
&gt; &gt; * Radix 32: 0.82 GByte/s
&gt; &gt; * Radix 64: 1.22 GByte/s
&gt; &gt;
&gt; &gt; Apparently, the higher radix version has performance improvements on
&gt; &gt; x86_64, powerpc, and s390x but this is not the case for arm64 arch where
&gt; &gt; the performance has a slight hit there.
&gt;
&gt; That might be an artifact of the specific ARM processor
&gt; microarchitecture or the memory subsystem of the ARM system, not
&gt; inherent to the Arm AArch64 architecture and ISA.
&gt;

Seems right, I've tested the enhanced versions of wider multiplication on
other gfarm aarch64 instances and I got different results.

On Mon, Jan 24, 2022 at 10:01 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; I made a performance test of this patch on the available architectures I
&gt; &gt; have access to.
&gt; &gt;
&gt; &gt; Arm64 (gcc117 gfarm):
&gt; &gt; * Radix 26: 0.65 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 0.92 GByte/s
&gt; &gt; * Radix 32: 0.55 GByte/s
&gt; &gt; * Radix 64: 0.58 GByte/s
&gt; &gt; POWER9:
&gt; &gt; * Radix 26: 0.47 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 1.15 GByte/s
&gt; &gt; * Radix 32: 0.52 GByte/s
&gt; &gt; * Radix 64: 0.58 GByte/s
&gt; &gt; Z15:
&gt; &gt; * Radix 26: 0.65 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 3.17 GByte/s
&gt; &gt; * Radix 32: 0.82 GByte/s
&gt; &gt; * Radix 64: 1.22 GByte/s
&gt;
&gt; Interesting. I'm a bit surprised the radix-64 doesn't perform better, in
&gt; particular on arm64. (But I'm not yet familiar with arm64 multiply
&gt; instructions).
&gt;

It looks like wider multiplication would achieve higher speed on different
aarch64 instance on gfarm. Here are the numbers on gcc185 instance:

* Radix 26: 0.83 GByte/s
* Radix 26 (2-way interleaved): 0.70 GByte/s
* Radix 64 (Latest version): 1.25 GByte/s

These numbers are a bit of a surprise too since the 2-way interleaving is
supposed to perform better than the old C version similarly to other
architectures!
Anyway, the benchmark numbers of powerpc and s390x were not taken from
gfarm instances and it's ok to be based on.


&gt; Numbers for 2-way interleaving are impressive, I'd like to understand
&gt; how that works.
&gt;

Vector 32-bit multiplication applies two multiply operations on inputs and
places the concatenated results on the destination vector. So the current
simd/altivec implementations interleave two blocks horizontally over vector
registers and execute both multiplication and reduction phases on both
blocks simultaneously. However, after each block iteration we should
combine the two states together by splitting the concatenated value and
adding it to the origin but to avoid that overhead Ione can multiply both
state parts with r^2 except for the last two blocks that imply multiplying
the first part with r^2 and the second one with r.
Let's consider a message of 4-blocks b0,b1,b2,b3 multiplying state by hash
has the sequence h = (h+b0) r^4 + b1 r^3 + b2 r^2 + b3 r
With interleaved implementation this sequence is executed in two iteration.
First iteration:
(h+b0) r^2 || b1 r^2
Second iteration:
((h+b0) r^2 + b2) r^2 || (b1 r^2 + b3) r

When getting out of the loop we combine the two state parts together so we
get the same correct sequence of r powers for each block.

Also, the two-independent carry technique that mentioned previously
overlaps two carry procedures with each other including the long carry from
h4 to h0 which offers the opportunity for further boost.

Might be useful derive corresponding multiply
&gt; throughput, i.e., number of multiply operations (and with which multiply
&gt; instruction) completed per cycle, as well as total cycles per block
&gt;

I'm not sure if I can depend on gfarm machines for such a purpose as it
seems to make incoherent performance results to me. I think I should look
for alternatives for more reliable results.

On Mon, Jan 24, 2022 at 10:56 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; I've tried reworking folding, to reduce latency. Idea is to let the most
&gt; significant state word be close to a word, rather than limited to &lt;= 4
&gt; as in the previous version. When multiplying by r, split one of the
&gt; multiplies to take out the low 2 bits. For the radix 64 version, that
&gt; term is
&gt;
&gt;   B^2 t_2 * r0
&gt;
&gt; Split t_2 as 4*hi + lo, then this can be reduced to
&gt;
&gt;   B^2 lo * r0 + hi * 5*r0
&gt;
&gt; (Using the same old B^2 = 5/4 (mod p) in a slightly different way).
&gt;
&gt; The 5*r0 fits one word and can be precomputed, and then this
&gt; multiplication goes in parallell with the other multiplies, and no
&gt; multiply left in the final per-block folding. With this trick I get on
&gt; the same machine
&gt;
&gt; Radix 32: 1.65 GByte/s
&gt;
&gt; Radix 64: 2.75 GByte/s, i.e., faster than current x86_64 asm version.
&gt;
&gt; I haven't yet done a strict analysis of bounds on the state and
&gt; temporaries, but I would expect that it works out with no possibility of
&gt; overflow.
&gt;
&gt; See attached file. To fit the precomputed 5*r0 in a nice way I had to
&gt; rearrange the unions in struct poly1305_ctx a bit, I also attach the
&gt; patch to do this. Size of the struct should be the same, so I think it
&gt; can be done without any abi bump.


Great! It performs better on all tested architectures. Apparently, AArch64
SIMD doesn't support 64*64-&gt;128 vector multiplication so I've implemented
this version on powerpc by utilizing vmsumudm power9-specific instruction.
I got 0.62 GByte/s for the C version and 0.65 GByte/s for the assembly
version, I'll attach the hardware implementation in this email.

regards,
Mamone
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220125202407</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-25 20:24:07-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; It looks like wider multiplication would achieve higher speed on different
&gt; aarch64 instance on gfarm. Here are the numbers on gcc185 instance:
&gt;
&gt; * Radix 26: 0.83 GByte/s
&gt; * Radix 26 (2-way interleaved): 0.70 GByte/s
&gt; * Radix 64 (Latest version): 1.25 GByte/s
&gt;
&gt; These numbers are a bit of a surprise too since the 2-way interleaving is
&gt; supposed to perform better than the old C version similarly to other
&gt; architectures!
&gt; Anyway, the benchmark numbers of powerpc and s390x were not taken from
&gt; gfarm instances and it's ok to be based on.

In the meantime, I pushed the latest C radix-32 version to a branch
poly1305-radix32, and benchmarked on one of the 32-bit ARM boards that
are part of the GMP test systems (the odxu4 system on
https://gmplib.org/devel/testsystems, labeled Cortex-A15/A7). I got:

  Radix 26 (old code): 326 MByte/s
  Radix 32 (new code): 260 MByte/s

So radix-32 doesn't seem to be a good option there. I had a quick look
at the generated assembly, besides the expected umull and umlal
instructions to do the main multiply work, there's an awful lot of loads
and stores to the stack, roughly half of the instructions. The compiler
installed is gcc-5.4, rather old.

&gt; Vector 32-bit multiplication applies two multiply operations on inputs and
&gt; places the concatenated results on the destination vector. So the current
&gt; simd/altivec implementations interleave two blocks horizontally over vector
&gt; registers and execute both multiplication and reduction phases on both
&gt; blocks simultaneously. However, after each block iteration we should
&gt; combine the two states together by splitting the concatenated value and
&gt; adding it to the origin but to avoid that overhead Ione can multiply both
&gt; state parts with r^2 except for the last two blocks that imply multiplying
&gt; the first part with r^2 and the second one with r.
&gt; Let's consider a message of 4-blocks b0,b1,b2,b3 multiplying state by hash
&gt; has the sequence h = (h+b0) r^4 + b1 r^3 + b2 r^2 + b3 r
&gt; With interleaved implementation this sequence is executed in two iteration.
&gt; First iteration:
&gt; (h+b0) r^2 || b1 r^2
&gt; Second iteration:
&gt; ((h+b0) r^2 + b2) r^2 || (b1 r^2 + b3) r
&gt;
&gt; When getting out of the loop we combine the two state parts together so we
&gt; get the same correct sequence of r powers for each block.
&gt;
&gt; Also, the two-independent carry technique that mentioned previously
&gt; overlaps two carry procedures with each other including the long carry from
&gt; h4 to h0 which offers the opportunity for further boost.

Hmm, it seems that avoiding long carry chains is the main reason why
radix 26 can be faster.

&gt; Great! It performs better on all tested architectures. Apparently, AArch64
&gt; SIMD doesn't support 64*64-&gt;128 vector multiplication so I've implemented
&gt; this version on powerpc by utilizing vmsumudm power9-specific instruction.
&gt; I got 0.62 GByte/s for the C version and 0.65 GByte/s for the assembly
&gt; version, I'll attach the hardware implementation in this email.

But you had 1.15 GByte/s for the 2-way interleaved version on this machine?

&gt; define(`FUNC_ALIGN', `5')
&gt; PROLOGUE(_nettle_poly1305_block)
&gt; 	ld			H0, 32(CTX)
&gt; 	ld			H1, 40(CTX)
&gt; 	ld			H2, 48(CTX)
&gt; 	ld			T0, 0(M)
&gt; 	ld			T1, 8(M)
&gt;
&gt; 	addc		T0, T0, H0
&gt; 	adde		T1, T1, H1
&gt; 	adde		T2, M128, H2
&gt;
&gt; 	li			IDX, 16
&gt; 	lxvd2x		VSR(R), 0, CTX
&gt; 	lxvd2x		VSR(S), IDX, CTX
&gt;
&gt; 	li			RZ, 0
&gt; 	vxor		ZERO, ZERO, ZERO
&gt; 	vxor		F0, F0, F0
&gt; 	vxor		F1, F1, F1
&gt; 	vxor		TMP, TMP, TMP
&gt;
&gt; 	xxpermdi	VSR(MU0), VSR(R), VSR(S), 0b01
&gt; 	xxswapd		VSR(MU1), VSR(R)
&gt; 	
&gt; 	mtvsrdd		VSR(T), T0, T1
&gt; 	mtvsrdd		VSR(T10), 0, T2
&gt; 	andi.		T2A, T2, 3
&gt; 	mtvsrdd		VSR(T11), 0, T2A
&gt; 	srdi		T2A, T2, 2
&gt; 	mtvsrdd		VSR(T00), T2A, RZ

I don't get all of the setup, but perhaps it would be better to load
input (T0, T1) and state (H0, H1, H2) directly into vector registers,
and avoid move between regular registers and vectors.

For the R and S values, the key setup could store them in the right
order so they don't have to be permuted after load.
 
&gt; 	vmsumudm	F0, T, MU0, F0
&gt; 	vmsumudm	F1, T, MU1, F1
&gt; 	vmsumudm	TMP, T11, MU1, TMP
&gt;
&gt; 	vmsumudm	F0, T00, S, F0
&gt; 	vmsumudm	F1, T10, MU0, F1

This part is as neat as I had hoped! Is there some variant of the
instructions that writes the result register without adding, to avoid
the explicit clearing of F0 and F1? It may also be doable with one
instruction less; the 5 instructions does 10 multiplies, but I think we
use only 7, the rest must somehow be zeroed or ignored.

&gt; 	xxmrgld		VSR(TMP), VSR(TMP), VSR(ZERO)
&gt; 	li			IDX, 32
&gt; 	xxswapd		VSR(F0), VSR(F0)
&gt; 	vadduqm		F1, F1, TMP
&gt; 	stxsdx		VSR(F0), IDX, CTX
&gt;
&gt; 	li			IDX, 40
&gt; 	xxmrgld		VSR(F0), VSR(ZERO), VSR(F0)
&gt; 	vadduqm		F1, F1, F0
&gt; 	xxswapd		VSR(F1), VSR(F1)
&gt; 	stxvd2x		VSR(F1), IDX, CTX

This is looks a bit verbose, if what we need to do is just to add high
part of F0 to low part of F1 (with carry to the high part of F1), and
store the result?

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220125222145</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-25 22:21:45-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Tue, Jan 25, 2022 at 10:24 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; It looks like wider multiplication would achieve higher speed on
&gt; different
&gt; &gt; aarch64 instance on gfarm. Here are the numbers on gcc185 instance:
&gt; &gt;
&gt; &gt; * Radix 26: 0.83 GByte/s
&gt; &gt; * Radix 26 (2-way interleaved): 0.70 GByte/s
&gt; &gt; * Radix 64 (Latest version): 1.25 GByte/s
&gt; &gt;
&gt; &gt; These numbers are a bit of a surprise too since the 2-way interleaving is
&gt; &gt; supposed to perform better than the old C version similarly to other
&gt; &gt; architectures!
&gt; &gt; Anyway, the benchmark numbers of powerpc and s390x were not taken from
&gt; &gt; gfarm instances and it's ok to be based on.
&gt;
&gt; In the meantime, I pushed the latest C radix-32 version to a branch
&gt; poly1305-radix32, and benchmarked on one of the 32-bit ARM boards that
&gt; are part of the GMP test systems (the odxu4 system on
&gt; https://gmplib.org/devel/testsystems, labeled Cortex-A15/A7). I got:
&gt;
&gt;   Radix 26 (old code): 326 MByte/s
&gt;   Radix 32 (new code): 260 MByte/s
&gt;
&gt; So radix-32 doesn't seem to be a good option there. I had a quick look
&gt; at the generated assembly, besides the expected umull and umlal
&gt; instructions to do the main multiply work, there's an awful lot of loads
&gt; and stores to the stack, roughly half of the instructions. The compiler
&gt; installed is gcc-5.4, rather old.
&gt;
&gt; &gt; Vector 32-bit multiplication applies two multiply operations on inputs
&gt; and
&gt; &gt; places the concatenated results on the destination vector. So the current
&gt; &gt; simd/altivec implementations interleave two blocks horizontally over
&gt; vector
&gt; &gt; registers and execute both multiplication and reduction phases on both
&gt; &gt; blocks simultaneously. However, after each block iteration we should
&gt; &gt; combine the two states together by splitting the concatenated value and
&gt; &gt; adding it to the origin but to avoid that overhead Ione can multiply both
&gt; &gt; state parts with r^2 except for the last two blocks that imply
&gt; multiplying
&gt; &gt; the first part with r^2 and the second one with r.
&gt; &gt; Let's consider a message of 4-blocks b0,b1,b2,b3 multiplying state by
&gt; hash
&gt; &gt; has the sequence h = (h+b0) r^4 + b1 r^3 + b2 r^2 + b3 r
&gt; &gt; With interleaved implementation this sequence is executed in two
&gt; iteration.
&gt; &gt; First iteration:
&gt; &gt; (h+b0) r^2 || b1 r^2
&gt; &gt; Second iteration:
&gt; &gt; ((h+b0) r^2 + b2) r^2 || (b1 r^2 + b3) r
&gt; &gt;
&gt; &gt; When getting out of the loop we combine the two state parts together so
&gt; we
&gt; &gt; get the same correct sequence of r powers for each block.
&gt; &gt;
&gt; &gt; Also, the two-independent carry technique that mentioned previously
&gt; &gt; overlaps two carry procedures with each other including the long carry
&gt; from
&gt; &gt; h4 to h0 which offers the opportunity for further boost.
&gt;
&gt; Hmm, it seems that avoiding long carry chains is the main reason why
&gt; radix 26 can be faster.
&gt;

Yes, with the sequential carry path the SIMD function would slightly beat
the C function of radix 26.


&gt; &gt; Great! It performs better on all tested architectures. Apparently,
&gt; AArch64
&gt; &gt; SIMD doesn't support 64*64-&gt;128 vector multiplication so I've implemented
&gt; &gt; this version on powerpc by utilizing vmsumudm power9-specific
&gt; instruction.
&gt; &gt; I got 0.62 GByte/s for the C version and 0.65 GByte/s for the assembly
&gt; &gt; version, I'll attach the hardware implementation in this email.
&gt;
&gt; But you had 1.15 GByte/s for the 2-way interleaved version on this machine?
&gt;

Right, the 2-way interleaved version is more efficient than this one and
supports POWER7+ processors.


&gt; &gt; define(`FUNC_ALIGN', `5')
&gt; &gt; PROLOGUE(_nettle_poly1305_block)
&gt; &gt;       ld                      H0, 32(CTX)
&gt; &gt;       ld                      H1, 40(CTX)
&gt; &gt;       ld                      H2, 48(CTX)
&gt; &gt;       ld                      T0, 0(M)
&gt; &gt;       ld                      T1, 8(M)
&gt; &gt;
&gt; &gt;       addc            T0, T0, H0
&gt; &gt;       adde            T1, T1, H1
&gt; &gt;       adde            T2, M128, H2
&gt; &gt;
&gt; &gt;       li                      IDX, 16
&gt; &gt;       lxvd2x          VSR(R), 0, CTX
&gt; &gt;       lxvd2x          VSR(S), IDX, CTX
&gt; &gt;
&gt; &gt;       li                      RZ, 0
&gt; &gt;       vxor            ZERO, ZERO, ZERO
&gt; &gt;       vxor            F0, F0, F0
&gt; &gt;       vxor            F1, F1, F1
&gt; &gt;       vxor            TMP, TMP, TMP
&gt; &gt;
&gt; &gt;       xxpermdi        VSR(MU0), VSR(R), VSR(S), 0b01
&gt; &gt;       xxswapd         VSR(MU1), VSR(R)
&gt; &gt;
&gt; &gt;       mtvsrdd         VSR(T), T0, T1
&gt; &gt;       mtvsrdd         VSR(T10), 0, T2
&gt; &gt;       andi.           T2A, T2, 3
&gt; &gt;       mtvsrdd         VSR(T11), 0, T2A
&gt; &gt;       srdi            T2A, T2, 2
&gt; &gt;       mtvsrdd         VSR(T00), T2A, RZ
&gt;
&gt; I don't get all of the setup, but perhaps it would be better to load
&gt; input (T0, T1) and state (H0, H1, H2) directly into vector registers,
&gt; and avoid move between regular registers and vectors.
&gt;

I was having difficulty using vector addition with carry so I got to deal
with the general register for that purpose. Also, general AND and Shift
operations are more easier to use than the vector ones since the latter
requires setting up a vector register for the immediate value.


&gt; For the R and S values, the key setup could store them in the right
&gt; order so they don't have to be permuted after load.
&gt;
&gt; &gt;       vmsumudm        F0, T, MU0, F0
&gt; &gt;       vmsumudm        F1, T, MU1, F1
&gt; &gt;       vmsumudm        TMP, T11, MU1, TMP
&gt; &gt;
&gt; &gt;       vmsumudm        F0, T00, S, F0
&gt; &gt;       vmsumudm        F1, T10, MU0, F1
&gt;
&gt; This part is as neat as I had hoped! Is there some variant of the
&gt; instructions that writes the result register without adding, to avoid
&gt; the explicit clearing of F0 and F1? It may also be doable with one
&gt; instruction less; the 5 instructions does 10 multiplies, but I think we
&gt; use only 7, the rest must somehow be zeroed or ignored.
&gt;

POWER10 adds 'vmuloud' and 'vmuleud' for one doubleword multiply which fits
well here. Now I realized that there is no need to clear F0, F1, TMP
registers since we can use ZERO register in place of the fourth operand.
However, the purpose of this implementation is to get an approximate
measurement of speed up in comparison to C version to vouch for adapting
2-way interleaving as a high-performance implementation.


&gt; &gt;       xxmrgld         VSR(TMP), VSR(TMP), VSR(ZERO)
&gt; &gt;       li                      IDX, 32
&gt; &gt;       xxswapd         VSR(F0), VSR(F0)
&gt; &gt;       vadduqm         F1, F1, TMP
&gt; &gt;       stxsdx          VSR(F0), IDX, CTX
&gt; &gt;
&gt; &gt;       li                      IDX, 40
&gt; &gt;       xxmrgld         VSR(F0), VSR(ZERO), VSR(F0)
&gt; &gt;       vadduqm         F1, F1, F0
&gt; &gt;       xxswapd         VSR(F1), VSR(F1)
&gt; &gt;       stxvd2x         VSR(F1), IDX, CTX
&gt;
&gt; This is looks a bit verbose, if what we need to do is just to add high
&gt; part of F0 to low part of F1 (with carry to the high part of F1), and
&gt; store the result?
&gt;

I couldn't find a neat way to do that so I sticked with the C theme besides
some vector adjusting operations.


&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220126211814</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-26 21:18:14-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt;&gt; This is the speed I get for C implementations of poly1305_update on my
&gt;&gt; x86_64 laptop:
&gt;&gt;
&gt;&gt; * Radix 26: 1.2 GByte/s (old code)
&gt;&gt;
&gt;&gt; * Radix 32: 1.3 GByte/s
&gt;&gt;
&gt;&gt; * Radix 64: 2.2 GByte/s
[...]
&gt;&gt; For comparison, the current x86_64 asm version: 2.5 GByte/s.
[...]
&gt; I've tried reworking folding, to reduce latency [...] With this trick I get on
&gt; the same machine
&gt;
&gt; Radix 32: 1.65 GByte/s
&gt;
&gt; Radix 64: 2.75 GByte/s, i.e., faster than current x86_64 asm version.

And I've now tried the same method for the x86_64 implementation. See
attached file + needed patch to asm.m4. This gives 2.9 GByte/s. 

I'm not entirely sure cycle numbers are accurate, with clock frequence
not being fixed. I think the machine runs bechmarks at 2.1GHz, and then
this corresponds to 11.5 cycles per block, 0.7 cycles per byte, 4
instructions per cycle, 0.5 multiply instructions per cycle.

This laptop has an AMD zen2 processor, which should be capable of
issuing four instructions per cycle and complete one multiply
instruction per cycle (according to
https://gmplib.org/~tege/x86-timing.pdf). 

This seems to indicate that on this hardware, speed is not limited by
multiplier throughput, instead, the bottleneck is instruction
decoding/issuing, with max four instructions per cycle.

Regards,
/Niels


[Attachment #3 (text/x-diff)]

diff --git a/asm.m4 b/asm.m4
index 4ac21c20..60c66c25 100644
--- a/asm.m4
+++ b/asm.m4
@@ -94,10 +94,10 @@ C For 64-bit implementation
 STRUCTURE(P1305)
   STRUCT(R0, 8)
   STRUCT(R1, 8)
+  STRUCT(S0, 8)
   STRUCT(S1, 8)
-  STRUCT(PAD, 12)
-  STRUCT(H2, 4)
   STRUCT(H0, 8)
   STRUCT(H1, 8)
+  STRUCT(H2, 8)
 
 divert

["poly1305-internal.asm" (text/plain)]

C x86_64/poly1305-internal.asm

ifelse(`
   Copyright (C) 2013 Niels Möller

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

	.file "poly1305-internal.asm"

C Registers mainly used by poly1305_block
define(`CTX', `%rdi') C First argument to all functions

define(`KEY', `%rsi')
define(`MASK',` %r8')
	C _poly1305_set_key(struct poly1305_ctx *ctx, const uint8_t key[16])
	.text
	ALIGN(16)
PROLOGUE(_nettle_poly1305_set_key)
	W64_ENTRY(2,0)
	mov	$0x0ffffffc0fffffff, MASK
	mov	(KEY), %rax
	and	MASK, %rax
	and	$-4, MASK
	mov	%rax, P1305_R0 (CTX)
	imul	$5, %rax
	mov	%rax, P1305_S0 (CTX)
	mov	8(KEY), %rax
	and	MASK, %rax
	mov	%rax, P1305_R1 (CTX)
	shr	$2, %rax
	imul	$5, %rax
	mov	%rax, P1305_S1 (CTX)
	xor	XREG(%rax), XREG(%rax)
	mov	%rax, P1305_H0 (CTX)
	mov	%rax, P1305_H1 (CTX)
	mov	%rax, P1305_H2 (CTX)
	
	W64_EXIT(2,0)
	ret

undefine(`KEY')
undefine(`MASK')

EPILOGUE(_nettle_poly1305_set_key)

define(`T0', `%rcx')
define(`T1', `%rsi')	C Overlaps message input pointer.
define(`T2', `%r8')
define(`H0', `%r9')
define(`H1', `%r10')
define(`F0', `%r11')
define(`F1', `%r12')

C Compute in parallel
C
C {H1,H0} = R0 T0 + S1 T1 + S0 (T2 &gt;&gt; 2)
C {F1,F0} = R1 T0 + R0 T1 + S1 T2
C T = R0 * (T2 &amp; 3)
C
C Then accumulate as
C
C     +--+--+--+
C     |T |H1|H0|
C     +--+--+--+
C   + |F1|F0|
C   --+--+--+--+
C     |H2|H1|H0|
C     +--+--+--+

	C _poly1305_block (struct poly1305_ctx *ctx, const uint8_t m[16], unsigned hi)
	
PROLOGUE(_nettle_poly1305_block)
	W64_ENTRY(3, 0)
	push	%r12
	mov	(%rsi), T0
	mov	8(%rsi), T1
	mov	XREG(%rdx), XREG(T2)	C Also zero extends

	add	P1305_H0 (CTX), T0
	adc	P1305_H1 (CTX), T1
	adc	P1305_H2 (CTX), T2

	mov	P1305_R1 (CTX), %rax
	mul	T0			C R1 T0
	mov	%rax, F0
	mov	%rdx, F1

	mov	T0, %rax		C Last use of T0 input
	mov	P1305_R0 (CTX), T0
	mul	T0			C R0*T0
	mov	%rax, H0
	mov	%rdx, H1

	mov	T1, %rax
	mul	T0			C R0*T1
	add	%rax, F0
	adc	%rdx, F1

	mov	P1305_S1 (CTX), T0
	mov	T1, %rax		C Last use of T1 input
	mul	T0			C S1*T1
	add	%rax, H0
	adc	%rdx, H1

	mov	T2, %rax
	mul	T0			C S1*T2
	add	%rax, F0
	adc	%rdx, F1

	mov	$3, XREG(T1)
	and	T2, T1

	shr	$2, T2
	mov	P1305_S0 (CTX), %rax
	mul	T2			C S0*(T2 &gt;&gt; 2)
	add	%rax, H0
	adc	%rdx, H1

	imul	P1305_R0 (CTX), T1	C R0*(T2 &amp; 3)
	add	F0, H1
	adc	T1, F1

	mov	H0, P1305_H0 (CTX)
	mov	H1, P1305_H1 (CTX)
	mov	F1, P1305_H2 (CTX)
	pop	%r12
	W64_EXIT(3, 0)
	ret
EPILOGUE(_nettle_poly1305_block)
undefine(`T0')
undefine(`T1')
undefine(`T2')
undefine(`H0')
undefine(`H1')
undefine(`F0')
undefine(`F1')

	C _poly1305_digest (struct poly1305_ctx *ctx, uint8_t *s)
define(`S', `%rsi')

define(`T0', `%rcx')
define(`T1', `%r8')
define(`H0', `%r9')
define(`H1', `%r10')
define(`F0', `%r11')
define(`F1', `%rrd')	C Overlaps CTX

PROLOGUE(_nettle_poly1305_digest)
	W64_ENTRY(2, 0)

	mov	P1305_H0 (CTX), H0
	mov	P1305_H1 (CTX), H1
	mov	P1305_H2 (CTX), F0

	xor	XREG(%rax), XREG(%rax)
	mov	%rax, P1305_H0 (CTX)
	mov	%rax, P1305_H1 (CTX)
	mov	%rax, P1305_H2 (CTX)

	mov	$3, XREG(%rax)
	and 	XREG(F0), XREG(%rax)
	shr	$2, F0
	imul	$5, F0
	add	F0, H0
	adc	$0, H1
	adc	$0, XREG(%rax)

	C Add 5, use result if &gt;= 2^130
	mov	$5, T0
	xor	T1, T1
	add	H0, T0
	adc	H1, T1
	adc	$0, XREG(%rax)		C Use adc $-4 ?
	cmp	$4, XREG(%rax)
	cmovnc	T0, H0
	cmovnc	T1, H1

	add	H0, (S)
	adc	H1, 8(S)

	W64_EXIT(2, 0)
	ret
EPILOGUE(_nettle_poly1305_digest)



-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

[Attachment #6 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220127212759</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-27 21:27:59-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt;&gt; Radix 64: 2.75 GByte/s, i.e., faster than current x86_64 asm version.
&gt;
&gt; And I've now tried the same method for the x86_64 implementation. See
&gt; attached file + needed patch to asm.m4. This gives 2.9 GByte/s. 
&gt;
&gt; I'm not entirely sure cycle numbers are accurate, with clock frequence
&gt; not being fixed. I think the machine runs bechmarks at 2.1GHz, and then
&gt; this corresponds to 11.5 cycles per block, 0.7 cycles per byte, 4
&gt; instructions per cycle, 0.5 multiply instructions per cycle.
&gt;
&gt; This laptop has an AMD zen2 processor, which should be capable of
&gt; issuing four instructions per cycle and complete one multiply
&gt; instruction per cycle (according to
&gt; https://gmplib.org/~tege/x86-timing.pdf). 
&gt;
&gt; This seems to indicate that on this hardware, speed is not limited by
&gt; multiplier throughput, instead, the bottleneck is instruction
&gt; decoding/issuing, with max four instructions per cycle.

Benchmarked also on my other nearby x86_64 machine (intel broadwell
processor). It's faster there too (from 1.4 GByte/s to 1.75). I'd expect
it to be generally faster, and have pushed it to the master-updates
branch.

I haven't looked that carefully at what the old code was doing, but I
think the final folding for each block used a multiply instruction that
then depends on the previous ones for that block, increasing the per
block latency. With the new code, all multiplies done for a block are
independent of each other.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220127214358</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-27 21:43:58-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Thu, Jan 27, 2022 at 11:28 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt; &gt;&gt; Radix 64: 2.75 GByte/s, i.e., faster than current x86_64 asm version.
&gt; &gt;
&gt; &gt; And I've now tried the same method for the x86_64 implementation. See
&gt; &gt; attached file + needed patch to asm.m4. This gives 2.9 GByte/s.
&gt; &gt;
&gt; &gt; I'm not entirely sure cycle numbers are accurate, with clock frequence
&gt; &gt; not being fixed. I think the machine runs bechmarks at 2.1GHz, and then
&gt; &gt; this corresponds to 11.5 cycles per block, 0.7 cycles per byte, 4
&gt; &gt; instructions per cycle, 0.5 multiply instructions per cycle.
&gt; &gt;
&gt; &gt; This laptop has an AMD zen2 processor, which should be capable of
&gt; &gt; issuing four instructions per cycle and complete one multiply
&gt; &gt; instruction per cycle (according to
&gt; &gt; https://gmplib.org/~tege/x86-timing.pdf).
&gt; &gt;
&gt; &gt; This seems to indicate that on this hardware, speed is not limited by
&gt; &gt; multiplier throughput, instead, the bottleneck is instruction
&gt; &gt; decoding/issuing, with max four instructions per cycle.
&gt;
&gt; Benchmarked also on my other nearby x86_64 machine (intel broadwell
&gt; processor). It's faster there too (from 1.4 GByte/s to 1.75). I'd expect
&gt; it to be generally faster, and have pushed it to the master-updates
&gt; branch.
&gt;
&gt; I haven't looked that carefully at what the old code was doing, but I
&gt; think the final folding for each block used a multiply instruction that
&gt; then depends on the previous ones for that block, increasing the per
&gt; block latency. With the new code, all multiplies done for a block are
&gt; independent of each other.
&gt;

Great! I believe this is the best we can get for processing one block. I'm
trying to implement two-way interleaving using AVX extension and the main
instruction of interest here is 'vpmuludq' that does double multiply
operation, the main concern here is there's a shortage of XMM registers as
there are 16 of them, I'm working on addressing this issue by using memory
operands of key values for 'vpmuludq' and hope the processor cache do his
thing here. I'm expecting to complete the assembly implementation tomorrow.

regards,
Mamone


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220223002834</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-02-23 00:28:34-0400</timestampReceived><subject>Poly1305 based on radix 2^44 for S390x and PowerPC</subject><body>

POWER9 and Z14 have introduced ' vmsumudm' and 'vmslg' instructions
respectively for full 64-bit vector multiplication. This note demonstrates
implementing Poly1305 based on radix 2^44 for optimal architecture
utilization.

In radix B = 2^44 we have the state and key as follows
H = B^2 H_2 + B H_1 + H_0
R = B^2 R_2 + B R_1 + R_0
Where degrees of H_2 and R_2 are 42 and 36 respectively whereas other
coefficients are of degree 44
We have B^3 = 20 so we can pre-compute R_1' = 20 * R_1 and R_2' = 20 * R_2
Now to multiply H by R we can write
R H = B^2(R_2 H_0 + R_1 H_1 + R_0 H_2) + B(R_1 H_0 + R_0 H_1 + R_2' H_2)
                  \____________T2__________/
\____________T1__________/
+ R_0 H_0 + R_2' H_1 + R_1' H_2
   \____________T0___________/

T_0 &lt; 2^88 + 2^85 + 2^91 = 2^93
T_1 &lt; 2^88 + 2^88 + 2^83 = 2^90
T_2 &lt; 2^80 + 2^88 + 2^86 = 2^90

This is a good place to combine the interleaved multiplication state
products of next consecutive blocks (In this case we interleave four-block
multiplications by per-computing powers of key R) which rises up the degree
of product parts.

T_0 &lt;  2^93 + 3*2^93 = 2^96
T_1 &lt;  2^90 + 3*2^91 = 2^94
T_2 &lt;  2^90 + 3*2^90 = 2^93

Now let's reduce this product to 2^130 divided to 2^44, 2^44, 2^42 for each
part.
T_0 ------------&gt; T_1 ---------------&gt; T_2 -------------------&gt; T_0
---------------&gt; T_1
        (96-44)             (94-44+1)             (93-42+1+3)
(55-44+1)
This chain keeps the carry addition for the last part (2^44+2^12) less than
2^44+2^22.
Note the carry from T_2 would be multiplied by 5 before adding it to T_0

While the sequential carry addition achieves decent performance speed on
PowerPC arch, an interleaved carry handling would get more speed up for
s390x so let's figure an interleaved variant of carry addition.
*----------------------------------------------------------------------------------------------*
|            Phase 1           |             Phase 2             |
  Phase 3           |
|------------------------------|---------------------------------|-------------------------------|

|   T_1 ------------&gt; T_2  |  T_2 -----------------&gt; T_0 |  T_0
--------------&gt; T_1 |
|            (94-44)            |          (93-42+1+3)          |
(55-44+1)           |
|------------------------------|---------------------------------|-------------------------------|
|   T_0 -------------&gt; T_1 |  T_1 --------------&gt; T_2    |
                    |
|            (96-44)            |          (52-44+1)
|                                     |
*-----------------------------------------------------------------------------------------------*

This variant implies 3 sequential phases rather than four. Moreover, the
long carry path from T_2 -&gt; T_0 would've executed in parallel with another
carry path.

Patches of implementation based on radix 2^44 for both architectures are
submitted to nettle with fat build support.
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/39
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/41

Benchmark the implementations on POWER9 and Z15
*---------------------------------------------------------------------------------------------*
|    Arch           |   Nettle patch (radix 2^44)    |   OpenSSL (radix
2^26)   |
|-------------------|--------------------------------------|-----------------------------------|

|   PowerPC    |    0.972 cycles/byte              |    1.453 cycles/byte
       |
|-------------------|--------------------------------------|-----------------------------------|
|   IBMz           |    0.840 cycles/byte              |    0.936
cycles/byte          |
*---------------------------------------------------------------------------------------------*

This note can also be applied on x86_64 arch with 'AVX512VL' and
'AVX512_IFMA' extension support by utilizing 'VPMADD52HUQ' and
'VPMADD52LUQ' instructions of full 52-bit multiplication.

regards,
Mamone
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220216175622</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-16 17:56:22-0400</timestampReceived><subject>gcm/ghash organization (was Re: x86_64 gcm)</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; I've written a first version of a gcm_hash for x86_64, using the
&gt; pclmulqdq (carryless mul) instructions. With only a single block at a time,
&gt; no interleaving, this gives to 4.3 GByte/s,

I've added proper config and fat setup and merged this. It could surely
be improved further, but it's already much faster than the C version on
processors that support these instructions.

I'm considering reorganizing the internal gcm functions. I think I'd
like to have

  void
  _nettle_ghash_set_key (struct gcm_key *gcm, const union nettle_block16 *key);

which sets the key (typically, the key block is zero encrypte using aes).

  void
  _nettle_ghash_update (const struct gcm_key *key, union nettle_block16 *x,
		        size_t length, const uint8_t *data);

where the input is complete blocks (padding done in the calling C code).
Not sure if length should be block count or byte count.

  void
  _nettle_ghash_digest (union nettle_block16 *digest, const union nettle_block16 *x);

xors the final state into the digest block. Main point of this function
is that the implementation can chose internal byteorder, eliminating
byteswaps at start and end of the update function.

Would perhaps be good to also delete the code for GCM_TABLE_BITS != 8,
which isn't enabled and haven't been tested in years.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220222195524</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-02-22 19:55:24-0400</timestampReceived><subject>Re: gcm/ghash organization (was Re: x86_64 gcm)</subject><body>

nisse@lysator.liu.se (Niels Möller) writes:

&gt; I'm considering reorganizing the internal gcm functions. I think I'd
&gt; like to have
&gt;
&gt;   void
&gt;   _nettle_ghash_set_key (struct gcm_key *gcm, const union nettle_block16 *key);
&gt;
&gt; which sets the key (typically, the key block is zero encrypte using aes).
&gt;
&gt;   void
&gt;   _nettle_ghash_update (const struct gcm_key *key, union nettle_block16 *x,
&gt; 		        size_t length, const uint8_t *data);
&gt;
&gt; where the input is complete blocks (padding done in the calling C code).
&gt; Not sure if length should be block count or byte count.

I'm trying this out, on the branch ghash-refactor, new internal
interface in
https://git.lysator.liu.se/nettle/nettle/-/blob/ghash-refactor/ghash-internal.h

I settled for block count rather than byte count.

&gt;   void
&gt;   _nettle_ghash_digest (union nettle_block16 *digest, const union nettle_block16 *x);

And I've dropped this function. Using different byte order complicates
unit testing, testing, and I think cost of byteswapping the 16-byte
state at start and end of ghash_update is pretty small.

I've done the needed changes for the C, the x86_64, arm64 and powerpc64
implementations. s390x code also needs update, I hope to get to that in
a few days (unless someone else wants to do that).

Update has been fairly simple, split gcm_hash.asm into one file each for
gcm_init_key and gcm_hash, update functions to new names and
conventions, and delete the code to handle a partial block at the end of
gsm_hash. Some small further simplifications are likely possible.

&gt; Would perhaps be good to also delete the code for GCM_TABLE_BITS != 8,
&gt; which isn't enabled and haven't been tested in years.

Done that too.

The main gain is less complexity in the asm code, which no longer needs
to deal with partial blocks, and less #ifdef complexity in the fat build
setup.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220223090812</emailId><senderName>Amos Jeffries</senderName><senderEmail>squid3@treenet.co.nz</senderEmail><timestampReceived>2022-02-23 09:08:12-0400</timestampReceived><subject>Re: Poly1305 based on radix 2^44 for S390x and PowerPC</subject><body>

On 23/02/22 13:28, Maamoun TK wrote:
&gt; 
&gt; Benchmark the implementations on POWER9 and Z15
&gt; *---------------------------------------------------------------------------------------------*
&gt; |    Arch           |   Nettle patch (radix 2^44)    |   OpenSSL (radix
&gt; 2^26)   |
&gt; |-------------------|--------------------------------------|-----------------------------------|
&gt; 
&gt; |   PowerPC    |    0.972 cycles/byte              |    1.453 cycles/byte
&gt;         |
&gt; |-------------------|--------------------------------------|-----------------------------------|
&gt; |   IBMz           |    0.840 cycles/byte              |    0.936
&gt; cycles/byte          |
&gt; *---------------------------------------------------------------------------------------------*
&gt; 


What would be more useful here is a column against the non-patched 
Nettle speeds. It is sure nice to see nettle being faster than OpenSSL, 
but what if this proposed change is actually a degrade in current nettle 
speeds on those machines?


Amos
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220503064320</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-03 06:43:20-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; I've added Poly1305 optimization based on radix 26 using AVX2 extension for
&gt; x86_64 architecture with fat build support, the patch yields significant
&gt; speedup compared to upstream.
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/46

Cool. Do I get it right, that when AVX2 is enabled, also single block
poly1305 will use radix 2^26?

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220503070947</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-03 07:09:47-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Tue, May 3, 2022 at 8:43 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; I've added Poly1305 optimization based on radix 26 using AVX2 extension
&gt; for
&gt; &gt; x86_64 architecture with fat build support, the patch yields significant
&gt; &gt; speedup compared to upstream.
&gt; &gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/46
&gt;
&gt; Cool. Do I get it right, that when AVX2 is enabled, also single block
&gt; poly1305 will use radix 2^26?
&gt;

hmm right, didn't cross my mind. I'll add 2^64 -&gt; 2^26 procedure at
prologue of _nettle_poly1305_4core() and  2^26 -&gt; 2^64 at epilogue to
workaround this.


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220503072621</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-03 07:26:21-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; hmm right, didn't cross my mind. I'll add 2^64 -&gt; 2^26 procedure at
&gt; prologue of _nettle_poly1305_4core() and  2^26 -&gt; 2^64 at epilogue to
&gt; workaround this.

If possible, I think it would be nice to let subkeys stored in struct
poly1305_ctx stay in radix-2^64, and compute the radix-2^26 versions
into registers when needed in _nettle_poly1305_4core. And thuse treat
the subkeys stored in the struct as const even if it technically isn't
const declared.

I take it you need more radix-2^26 subkey constants than can fit in the
current struct layout anyway?

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220503075543</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-03 07:55:43-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Tue, May 3, 2022 at 9:26 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; hmm right, didn't cross my mind. I'll add 2^64 -&gt; 2^26 procedure at
&gt; &gt; prologue of _nettle_poly1305_4core() and  2^26 -&gt; 2^64 at epilogue to
&gt; &gt; workaround this.
&gt;
&gt; If possible, I think it would be nice to let subkeys stored in struct
&gt; poly1305_ctx stay in radix-2^64, and compute the radix-2^26 versions
&gt; into registers when needed in _nettle_poly1305_4core. And thuse treat
&gt; the subkeys stored in the struct as const even if it technically isn't
&gt; const declared.
&gt;
&gt; I take it you need more radix-2^26 subkey constants than can fit in the
&gt; current struct layout anyway?
&gt;

It needs 3 powers of key, also the struct layout has to have even more
slots if we need to consider the pre-multiplied values of subkeys. We can
either extend the layout or use _nettle_poly1305_4core starting with 64
blocks and more as this function begins to consume less cycles than average
consumptions of the 2^64 version.


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220504163450</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-04 16:34:50-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Tue, May 3, 2022 at 9:55 AM Maamoun TK &lt;maamoun.tk@googlemail.com&gt; wrote:

&gt; On Tue, May 3, 2022 at 9:26 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:
&gt;
&gt;&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;&gt;
&gt;&gt; &gt; hmm right, didn't cross my mind. I'll add 2^64 -&gt; 2^26 procedure at
&gt;&gt; &gt; prologue of _nettle_poly1305_4core() and  2^26 -&gt; 2^64 at epilogue to
&gt;&gt; &gt; workaround this.
&gt;&gt;
&gt;&gt; If possible, I think it would be nice to let subkeys stored in struct
&gt;&gt; poly1305_ctx stay in radix-2^64, and compute the radix-2^26 versions
&gt;&gt; into registers when needed in _nettle_poly1305_4core. And thuse treat
&gt;&gt; the subkeys stored in the struct as const even if it technically isn't
&gt;&gt; const declared.
&gt;&gt;
&gt;&gt; I take it you need more radix-2^26 subkey constants than can fit in the
&gt;&gt; current struct layout anyway?
&gt;&gt;
&gt;
&gt; It needs 3 powers of key, also the struct layout has to have even more
&gt; slots if we need to consider the pre-multiplied values of subkeys.
&gt;

I'm thinking of extending the structure layout of 'poly1305_ctx' to take in
pre-computed powers of key. How backward compatible would it be to append
additional key arrays to that structure?


&gt; We can either extend the layout or use _nettle_poly1305_4core starting
&gt; with 64 blocks and more as this function begins to consume less cycles than
&gt; average consumptions of the 2^64 version.
&gt;
&gt;
&gt;&gt; Regards,
&gt;&gt; /Niels
&gt;&gt;
&gt;&gt; --
&gt;&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt;&gt; Internet email is subject to wholesale government surveillance.
&gt;&gt;
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220504174737</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-04 17:47:37-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; I'm thinking of extending the structure layout of 'poly1305_ctx' to take in
&gt; pre-computed powers of key. How backward compatible would it be to append
&gt; additional key arrays to that structure?

It would break ABI compatibility, and require a new library soname.
Other than that, it would be API compatible, code using nettle could be
recompiled with no source code changes.

So definitely possible, but not something to do lightly. I think it's
best to postpone until have reason to break the ABI for other reasons.

&gt;&gt; We can either extend the layout or use _nettle_poly1305_4core starting
&gt;&gt; with 64 blocks and more as this function begins to consume less cycles than
&gt;&gt; average consumptions of the 2^64 version.

Do you need to do 64 blocks (16 iterations of the
_nettle_poly1305_4core loop) before it beats the single block radix-64
version? Then precomputations must be rather costly? But due to the abi
issues, I think we have to live with redoing them for each call.

I haven't looked closely at your code yet. It seems important that
_nettle_poly1305_4core can loop to process larger messages without
redoing precomputations. Can we use parallelism in the precomputation? I
imagine what needs to be done has the following steps:

1. Input is the key in radix 2^64, stored in r64[0] and r64[1] (the
   premultiplied values aren't helpful, I'm afraid).

2. Split into radix 2^26, five pieces, that goes into 32-bit (sub-)registers.
   Also premultiplied by 5 when useful. Call this K1.

3. Compute squared key, K2 = K1^2, and premultiplied by 5 when useful. In
   principle, squaring can be easier than multiply, 15 32x32 --&gt; 64
   multiplies rather than 25, but may not apply here since we want to
   part of the reduction at the same time.

4. Compute K3 = K1 K2 and K4 = K2^2. If we don't try to do any special
   for the squaring, at least these two multiply operations could be
   done in a SIMD fashion.

Also, I think the result after this precomputation probably doesn't have
to be in canonical representation, with each digit in the range 0 &lt;= x &lt;
2^26, it can most likely be made to work with a somewhat larger range.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220506161039</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-06 16:10:39-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Wed, May 4, 2022 at 7:47 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; I'm thinking of extending the structure layout of 'poly1305_ctx' to take
&gt; in
&gt; &gt; pre-computed powers of key. How backward compatible would it be to append
&gt; &gt; additional key arrays to that structure?
&gt;
&gt; It would break ABI compatibility, and require a new library soname.
&gt; Other than that, it would be API compatible, code using nettle could be
&gt; recompiled with no source code changes.
&gt;
&gt; So definitely possible, but not something to do lightly. I think it's
&gt; best to postpone until have reason to break the ABI for other reasons.
&gt;
&gt; &gt;&gt; We can either extend the layout or use _nettle_poly1305_4core starting
&gt; &gt;&gt; with 64 blocks and more as this function begins to consume less cycles
&gt; than
&gt; &gt;&gt; average consumptions of the 2^64 version.
&gt;
&gt; Do you need to do 64 blocks (16 iterations of the
&gt; _nettle_poly1305_4core loop) before it beats the single block radix-64
&gt; version? Then precomputations must be rather costly? But due to the abi
&gt; issues, I think we have to live with redoing them for each call.
&gt;
&gt; I haven't looked closely at your code yet. It seems important that
&gt; _nettle_poly1305_4core can loop to process larger messages without
&gt; redoing precomputations. Can we use parallelism in the precomputation? I
&gt; imagine what needs to be done has the following steps:
&gt;
&gt; 1. Input is the key in radix 2^64, stored in r64[0] and r64[1] (the
&gt;    premultiplied values aren't helpful, I'm afraid).
&gt;
&gt; 2. Split into radix 2^26, five pieces, that goes into 32-bit
&gt; (sub-)registers.
&gt;    Also premultiplied by 5 when useful. Call this K1.
&gt;
&gt; 3. Compute squared key, K2 = K1^2, and premultiplied by 5 when useful. In
&gt;    principle, squaring can be easier than multiply, 15 32x32 --&gt; 64
&gt;    multiplies rather than 25, but may not apply here since we want to
&gt;    part of the reduction at the same time.
&gt;
&gt; 4. Compute K3 = K1 K2 and K4 = K2^2. If we don't try to do any special
&gt;    for the squaring, at least these two multiply operations could be
&gt;    done in a SIMD fashion.
&gt;

I tried these steps to compute key powers. Squaring K2 = K1^2 does have
multiplications of 15 32x32 rather than 25 divided into 3 groups that
summed together. It also needs to pre-multiply key by 2 to apply perfect
square. Full reduction round has been applied on products as I couldn't
find easy way to partly reduce them then it computes K3 = K1 K2 and K4 = K2
K2 simultaneously in SIMD fashion.
I used XMM registers to reduce the transition bandwidth between stack and
move instructions. The result of mentioned improvements yield a sligh
speedup of pre-computations that has still been drawing more cycles than
2^64 version for less than 64 blocks as a whole process.

In case extending the layout of 'poly1305_ctx' structure is not an option,
I would suggest applying that threshold of message length in an
arch-specific manner. How do you think we can do that?

regards,
Mamone


&gt; Also, I think the result after this precomputation probably doesn't have
&gt; to be in canonical representation, with each digit in the range 0 &lt;= x &lt;
&gt; 2^26, it can most likely be made to work with a somewhat larger range.
&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220507081017</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-07 08:10:17-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt; In case extending the layout of 'poly1305_ctx' structure is not an option,
&gt; I would suggest applying that threshold of message length in an
&gt; arch-specific manner. How do you think we can do that?

Based on how thresholds are handled in gmp, I'd suggest a

#define POLY1305_CORE4_THRESHOLD

which can be defined as an arch-dependent constant for non-fat builds,
and as an alias for a global variable in fat builds, the variable should
how some sane initial value, and be updated as part of the fat setup of
the function using that threshold. For thread safety, it would make some
sense with memory barriers to ensure that the threshold is updated
before the function pointer, but perhaps not strictly necessary. One
could consider setting different values depending on processor model,
but I think that's beyond an initial version.

It would be good to add some size argument to nettle-benchmark to
make it easier to choose right threshold. If we end up with more
thresholds like this, we could consider tuning them more automatically,
analogous to the gmp/tune/tuneup program. But for start, manual tuning
is good enough.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220509071503</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-05-09 07:15:03-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

On Sat, May 7, 2022 at 10:10 AM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;
&gt; &gt; In case extending the layout of 'poly1305_ctx' structure is not an
&gt; option,
&gt; &gt; I would suggest applying that threshold of message length in an
&gt; &gt; arch-specific manner. How do you think we can do that?
&gt;
&gt; Based on how thresholds are handled in gmp, I'd suggest a
&gt;
&gt; #define POLY1305_CORE4_THRESHOLD
&gt;
&gt; which can be defined as an arch-dependent constant for non-fat builds,
&gt; and as an alias for a global variable in fat builds, the variable should
&gt; how some sane initial value, and be updated as part of the fat setup of
&gt; the function using that threshold. For thread safety, it would make some
&gt; sense with memory barriers to ensure that the threshold is updated
&gt; before the function pointer, but perhaps not strictly necessary. One
&gt; could consider setting different values depending on processor model,
&gt; but I think that's beyond an initial version.
&gt;
&gt; It would be good to add some size argument to nettle-benchmark to
&gt; make it easier to choose right threshold. If we end up with more
&gt; thresholds like this, we could consider tuning them more automatically,
&gt; analogous to the gmp/tune/tuneup program. But for start, manual tuning
&gt; is good enough.
&gt;

I made Poly1305 core4 compatible with 2^64 version on x86_64 in addition to
settling a threshold of 64-block. On arm64 it seems a proper threshold for
Poly1305 core2 is 32-block so I settled that too. Both optimization cores
of PPC and s390x achieve relative performance speed for the minimum
supported message length (4-block) so there is no need for threshold in
this case.


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220514180719</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-14 18:07:19-0400</timestampReceived><subject>Re: [Arm64, PowerPC64, S390x] Optimize Poly1305</subject><body>

Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:

&gt;  I created merge requests that have improvements of Poly1305 for arm64,
&gt; powerpc64, and s390x architectures by following using two-way interleaving.
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/38
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/39
&gt; https://git.lysator.liu.se/nettle/nettle/-/merge_requests/41
&gt; The patches have 41.88% speedup for arm64, 142.95% speedup for powerpc64,
&gt; and 382.65% speedup for s390x.

I've had a closer look at the ppc merge request #39.

I think it would be good to do the single block radix 2^44 version first
(I'm assuming that's in itself is an improvement over the C code, and
over using radix 2^64?). Is 44 bit pieces ideal (130 = 44+44+42), or
would anything get simpler with, e.g., 130 = 48 + 48 + 34, or 130 = 56 +
56 + 18)?

For the 4-way code, the name and organization seems inspired by
chacha_4core, which is a bit different since it also has a four-block
output, and then the caller has to be aware. I think it would be better
to look at the recent ghash. Maybe one can have an internal
_poly1306_update, following similar conventions as _ghash_update? Then
the C code doesn't need to know how many blocks are done at a time,
which should make things a bit simpler (although the assembly code would
need logic to do left-over blocks, just like for ghash).

&gt; OpenSSL is still ahead in terms of performance speed since it uses 4-way
&gt; interleaving or maybe more!!
&gt; Increasing the interleaving ways more than two has nothing to do with
&gt; parallelism since the execution units are already saturated by using 2-ways
&gt; for the three architectures. The reason behind the performance improvement
&gt; is the number of execution times of reduction procedure is cutted by half
&gt; for 4-way interleaving since the products of multiplying state parts by key
&gt; can be combined before the reduction phase. Let me know if you are
&gt; interested in doing that on nettle!

Good to know that 2-way is sufficient to saturate execution units. Going
to 4-way does have a startup cost for each call, since we don't have
space for extra pre-computed powers. But for large messages, we'll get
the best speed if we can make reduction as cheap as possible.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220517184642</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-05-17 18:46:42-0400</timestampReceived><subject>Re: [PATCH 0/1] Export-sha256-sha512_compress-functions</subject><body>

Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt; writes:

&gt; I am working on implementing crypto offloader devices I use and maintain
&gt; in Linux in qemu.
&gt; The hardware does not do full hashes offload but only their compress
&gt; part. (The driver need to do padding etc...)

Thanks for explaining the use case.

&gt; From all crypto library, only nettle provides helper for compress but
&gt; only for md5 and sha1.

That's somewhat historic, I have been considering removign them from the
public api (but kept them because I think I found some usage via debian
code search last time I looked).

&gt; The first one device I implement (sun4i-ss) only do sha1 and md5, so its
&gt; fine. But the second (sun8i-ce) need also sha224/sha256/sha384/sha512.

This is some ARM hardware, unrelated to Sun microsystems? You had me
confused for a while.

&gt; So it is why I propose to export sha256/sha512 compress functions.

Makes some sense. What should the api be like? The old md5 and sha1
compress functions look like

  void
  nettle_md5_compress(uint32_t *state, const uint8_t *data);

  void
  nettle_sha1_compress(uint32_t *state, const uint8_t *data);

(and if we want them to be fully supported, the nettle_ prefix should be
made optional). They used to be called _nettle_*_compress, to indicate
their somewhat internal status.

For newer internal interfaces (for assembly implementation), we've been
discussing replacing the single-block compression function with
functions that can process multiple blocks, e.g., something like

  void 
  _sha1_compress_n (uint32_t *state, size_t nblocks, const uint8_t *data);

That can be a significant improvement on architectures where the
compression function is so fast (with special instructions and the like)
that per-block overhead of the function call, loading constants, and
loading and storing the state, becomes a significant. But should perhaps
be optional, so assembly architectures where it brings no significant
benefit can stay simpler.

What's useful for your usecase? I would lean towards sticking to simple
single-block functions in the public api, and not expose directly what's
used in the assembly code.

I guess it's also conceivable with archs that provide good acceleration
for sha_update without directly exposing the compression function in a
convenient way. Sounds a bit unlikely, but in the worst case, I guess
one could have the public compress functions fall back to the plain C
implementation.

What about sha3? It seems Nettle exposes the sha3_permute function, but
it's completely undocumented.

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se

</body></email><email><emailId>20220517192130</emailId><senderName>Corentin Labbe</senderName><senderEmail>clabbe.montjoie@gmail.com</senderEmail><timestampReceived>2022-05-17 19:21:30-0400</timestampReceived><subject>Re: [PATCH 0/1] Export-sha256-sha512_compress-functions</subject><body>

Le Tue, May 17, 2022 at 08:46:42PM +0200, Niels Möller a écrit :
&gt; Corentin Labbe &lt;clabbe.montjoie@gmail.com&gt; writes:
&gt; 
&gt; &gt; I am working on implementing crypto offloader devices I use and maintain
&gt; &gt; in Linux in qemu.
&gt; &gt; The hardware does not do full hashes offload but only their compress
&gt; &gt; part. (The driver need to do padding etc...)
&gt; 
&gt; Thanks for explaining the use case.
&gt; 
&gt; &gt; From all crypto library, only nettle provides helper for compress but
&gt; &gt; only for md5 and sha1.
&gt; 
&gt; That's somewhat historic, I have been considering removign them from the
&gt; public api (but kept them because I think I found some usage via debian
&gt; code search last time I looked).
&gt; 
&gt; &gt; The first one device I implement (sun4i-ss) only do sha1 and md5, so its
&gt; &gt; fine. But the second (sun8i-ce) need also sha224/sha256/sha384/sha512.
&gt; 
&gt; This is some ARM hardware, unrelated to Sun microsystems? You had me
&gt; confused for a while.

My crypto devices are from ARM Allwinner SoCs.
Allwinner SoCs codename are sun[0-9]*i, so unrelated to Sun microsystems.
So in Linux Allwinner "arch" is often called sunxi.
So sun4i-ss is the Security System on sun4i SoCs and sun8i-ce is the crypto engine on \
sun8i SoCs.

&gt; 
&gt; &gt; So it is why I propose to export sha256/sha512 compress functions.
&gt; 
&gt; Makes some sense. What should the api be like? The old md5 and sha1
&gt; compress functions look like
&gt; 
&gt; void
&gt; nettle_md5_compress(uint32_t *state, const uint8_t *data);
&gt; 
&gt; void
&gt; nettle_sha1_compress(uint32_t *state, const uint8_t *data);
&gt; 
&gt; (and if we want them to be fully supported, the nettle_ prefix should be
&gt; made optional). They used to be called _nettle_*_compress, to indicate
&gt; their somewhat internal status.
&gt; 
&gt; For newer internal interfaces (for assembly implementation), we've been
&gt; discussing replacing the single-block compression function with
&gt; functions that can process multiple blocks, e.g., something like
&gt; 
&gt; void 
&gt; _sha1_compress_n (uint32_t *state, size_t nblocks, const uint8_t *data);
&gt; 
&gt; That can be a significant improvement on architectures where the
&gt; compression function is so fast (with special instructions and the like)
&gt; that per-block overhead of the function call, loading constants, and
&gt; loading and storing the state, becomes a significant. But should perhaps
&gt; be optional, so assembly architectures where it brings no significant
&gt; benefit can stay simpler.
&gt; 
&gt; What's useful for your usecase? I would lean towards sticking to simple
&gt; single-block functions in the public api, and not expose directly what's
&gt; used in the assembly code.
&gt; 
&gt; I guess it's also conceivable with archs that provide good acceleration
&gt; for sha_update without directly exposing the compression function in a
&gt; convenient way. Sounds a bit unlikely, but in the worst case, I guess
&gt; one could have the public compress functions fall back to the plain C
&gt; implementation.
&gt; 
&gt; What about sha3? It seems Nettle exposes the sha3_permute function, but
&gt; it's completely undocumented.
&gt; 

Speed is not really a topic for me, I just need them for emulation.
So I am totally fine with
ALGONAME_compress(uint32_t *state, const uint8_t *data)

But I think the compress_n version could be provided in parallel and still usefull \
for my case.

So basicly, do you agree if I resent my patch by simply removing all nettle_ prefix ?
Then a second patch adding sha_compress/md5_compress (But I dont know if you want and \
how to deprecate old nettle_md5_compress and such) And a third patch adding all \
compress_n functions.

Regards
_______________________________________________
nettle-bugs mailing list -- nettle-bugs@lists.lysator.liu.se
To unsubscribe send an email to nettle-bugs-leave@lists.lysator.liu.se


</body></email><email><emailId>20220221083709</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:09-0400</timestampReceived><subject>[PATCH v2 0/7] Introduce SM4 symmetric cipher algorithm</subject><body>

SM4 is a block cipher standard published by the government of the People's
Republic of China, and it was issued by the State Cryptography Administration
on March 21, 2012. The standard is GM/T 0002-2012 "SM4 block cipher algorithm".

SM4 algorithm is a symmetric cipher algorithm in ShangMi cryptosystems. The
block length and key length are both 128 bits. Both the encryption algorithm
and the key derivation algorithm use 32 rounds of non-linear iterative
structure, and the S box is a fixed 8 bits. The RFC 8998 specification
defines the usage of ShangMi algorithm suite in TLS 1.3, etc. According to
the State Cryptography Administration of China, its security and efficiency
are equivalent to AES-128.

Reference specification:
1. http://www.gmbz.org.cn/upload/2018-04-04/1522788048733065051.pdf
2. http://gmbz.org.cn/main/viewfile/20180108015408199368.html
3. https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html
4. https://datatracker.ietf.org/doc/html/rfc8998

---
v2 changes:
  - use separate set_key functions to avoid two copies of the subkeys.
  - unify encryption and decryption operations with one function.
  - use unsigned type instead of uint32_t for loop counter i.
  - use four variables instead of four5-element array.

Tianjia Zhang (7):
  doc: Add Copyright of SM3 hash algorithm
  Introduce SM4 symmetric cipher algorithm
  testsuite: add test for SM4 symmetric algorithm
  nettle-benchmark: bench SM4 symmetric algorithm
  doc: documentation for SM4 cipher algorithm
  gcm: Add SM4 as the GCM underlying cipher
  doc: documentation for GCM using SM4 cipher

 Makefile.in                  |   2 +
 examples/nettle-benchmark.c  |   2 +
 gcm-sm4-meta.c               |  60 ++++++++++
 gcm-sm4.c                    |  81 +++++++++++++
 gcm.h                        |  25 +++-
 nettle-meta-aeads.c          |   1 +
 nettle-meta-ciphers.c        |   1 +
 nettle-meta.h                |   3 +
 nettle.texinfo               |  81 +++++++++++++
 sm4-meta.c                   |  49 ++++++++
 sm4.c                        | 223 +++++++++++++++++++++++++++++++++++
 sm4.h                        |  69 +++++++++++
 testsuite/.gitignore         |   1 +
 testsuite/Makefile.in        |   2 +-
 testsuite/gcm-test.c         |  18 +++
 testsuite/meta-aead-test.c   |   1 +
 testsuite/meta-cipher-test.c |   3 +-
 testsuite/sm4-test.c         |  19 +++
 18 files changed, 638 insertions(+), 3 deletions(-)
 create mode 100644 gcm-sm4-meta.c
 create mode 100644 gcm-sm4.c
 create mode 100644 sm4-meta.c
 create mode 100644 sm4.c
 create mode 100644 sm4.h
 create mode 100644 testsuite/sm4-test.c

-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083710</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:10-0400</timestampReceived><subject>[PATCH v2 1/7] doc: Add Copyright of SM3 hash algorithm</subject><body>

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 nettle.texinfo | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/nettle.texinfo b/nettle.texinfo
index 76934637..45b06720 100644
--- a/nettle.texinfo
+++ b/nettle.texinfo
@@ -293,6 +293,10 @@ Written by @value{AUTHOR}, using Peter Gutmann's SHA1 code as a model.
 @item SHA3
 Written by @value{AUTHOR}.
 
+@item SM3
+The C implementation of the SM3 message digest is written by Tianjia
+Zhang, and the code is based on the implementation by Jia Zhang.
+
 @item TWOFISH
 The implementation of the TWOFISH cipher is written by Ruud de Rooij.
 
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083711</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:11-0400</timestampReceived><subject>[PATCH v2 2/7] Introduce SM4 symmetric cipher algorithm</subject><body>

Introduce the SM4 cipher algorithms (OSCCA GB/T 32907-2016).

SM4 (GBT.32907-2016) is a cryptographic standard issued by the
Organization of State Commercial Administration of China (OSCCA)
as an authorized cryptographic algorithms for the use within China.

SMS4 was originally created for use in protecting wireless
networks, and is mandated in the Chinese National Standard for
Wireless LAN WAPI (Wired Authentication and Privacy Infrastructure)
(GB.15629.11-2003).

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 Makefile.in                  |   1 +
 nettle-meta-ciphers.c        |   1 +
 nettle-meta.h                |   2 +
 sm4-meta.c                   |  49 ++++++++
 sm4.c                        | 223 +++++++++++++++++++++++++++++++++++
 sm4.h                        |  69 +++++++++++
 testsuite/meta-cipher-test.c |   3 +-
 7 files changed, 347 insertions(+), 1 deletion(-)
 create mode 100644 sm4-meta.c
 create mode 100644 sm4.c
 create mode 100644 sm4.h

diff --git a/Makefile.in b/Makefile.in
index f6bc2155..ba1a2db2 100644
--- a/Makefile.in
+++ b/Makefile.in
@@ -150,6 +150,7 @@ nettle_SOURCES = aes-decrypt-internal.c aes-decrypt.c aes-decrypt-table.c \
 		 serpent-meta.c \
 		 streebog.c streebog-meta.c \
 		 twofish.c twofish-meta.c \
+		 sm4.c sm4-meta.c \
 		 umac-nh.c umac-nh-n.c umac-l2.c umac-l3.c \
 		 umac-poly64.c umac-poly128.c umac-set-key.c \
 		 umac32.c umac64.c umac96.c umac128.c \
diff --git a/nettle-meta-ciphers.c b/nettle-meta-ciphers.c
index 49cb47a7..f8d691cf 100644
--- a/nettle-meta-ciphers.c
+++ b/nettle-meta-ciphers.c
@@ -54,6 +54,7 @@ const struct nettle_cipher * const _nettle_ciphers[] = {
   &amp;nettle_arctwo64,
   &amp;nettle_arctwo128,
   &amp;nettle_arctwo_gutmann128,
+  &amp;nettle_sm4,
   NULL
 };
 
diff --git a/nettle-meta.h b/nettle-meta.h
index d684947e..3d0440e8 100644
--- a/nettle-meta.h
+++ b/nettle-meta.h
@@ -89,6 +89,8 @@ extern const struct nettle_cipher nettle_arctwo64;
 extern const struct nettle_cipher nettle_arctwo128;
 extern const struct nettle_cipher nettle_arctwo_gutmann128;
 
+extern const struct nettle_cipher nettle_sm4;
+
 struct nettle_hash
 {
   const char *name;
diff --git a/sm4-meta.c b/sm4-meta.c
new file mode 100644
index 00000000..d7234984
--- /dev/null
+++ b/sm4-meta.c
@@ -0,0 +1,49 @@
+/* sm4-meta.c
+
+   Copyright (C) 2022 Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+*/
+
+#if HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include "nettle-meta.h"
+
+#include "sm4.h"
+
+const struct nettle_cipher nettle_sm4 = {
+  "sm4",
+  sizeof(struct sm4_ctx),
+  SM4_BLOCK_SIZE,
+  SM4_KEY_SIZE,
+  (nettle_set_key_func *) sm4_set_encrypt_key,
+  (nettle_set_key_func *) sm4_set_decrypt_key,
+  (nettle_cipher_func *) sm4_crypt,
+  (nettle_cipher_func *) sm4_crypt
+};
diff --git a/sm4.c b/sm4.c
new file mode 100644
index 00000000..7b3c049a
--- /dev/null
+++ b/sm4.c
@@ -0,0 +1,223 @@
+/* sm4.c
+
+   Copyright (C) 2022 Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+*/
+
+#if HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include &lt;assert.h&gt;
+#include &lt;string.h&gt;
+
+#include "sm4.h"
+
+#include "macros.h"
+
+
+static const uint32_t fk[4] =
+{
+  0xa3b1bac6, 0x56aa3350, 0x677d9197, 0xb27022dc
+};
+
+static const uint32_t ck[32] =
+{
+  0x00070e15, 0x1c232a31, 0x383f464d, 0x545b6269,
+  0x70777e85, 0x8c939aa1, 0xa8afb6bd, 0xc4cbd2d9,
+  0xe0e7eef5, 0xfc030a11, 0x181f262d, 0x343b4249,
+  0x50575e65, 0x6c737a81, 0x888f969d, 0xa4abb2b9,
+  0xc0c7ced5, 0xdce3eaf1, 0xf8ff060d, 0x141b2229,
+  0x30373e45, 0x4c535a61, 0x686f767d, 0x848b9299,
+  0xa0a7aeb5, 0xbcc3cad1, 0xd8dfe6ed, 0xf4fb0209,
+  0x10171e25, 0x2c333a41, 0x484f565d, 0x646b7279
+};
+
+static const uint8_t sbox[256] =
+{
+  0xd6, 0x90, 0xe9, 0xfe, 0xcc, 0xe1, 0x3d, 0xb7,
+  0x16, 0xb6, 0x14, 0xc2, 0x28, 0xfb, 0x2c, 0x05,
+  0x2b, 0x67, 0x9a, 0x76, 0x2a, 0xbe, 0x04, 0xc3,
+  0xaa, 0x44, 0x13, 0x26, 0x49, 0x86, 0x06, 0x99,
+  0x9c, 0x42, 0x50, 0xf4, 0x91, 0xef, 0x98, 0x7a,
+  0x33, 0x54, 0x0b, 0x43, 0xed, 0xcf, 0xac, 0x62,
+  0xe4, 0xb3, 0x1c, 0xa9, 0xc9, 0x08, 0xe8, 0x95,
+  0x80, 0xdf, 0x94, 0xfa, 0x75, 0x8f, 0x3f, 0xa6,
+  0x47, 0x07, 0xa7, 0xfc, 0xf3, 0x73, 0x17, 0xba,
+  0x83, 0x59, 0x3c, 0x19, 0xe6, 0x85, 0x4f, 0xa8,
+  0x68, 0x6b, 0x81, 0xb2, 0x71, 0x64, 0xda, 0x8b,
+  0xf8, 0xeb, 0x0f, 0x4b, 0x70, 0x56, 0x9d, 0x35,
+  0x1e, 0x24, 0x0e, 0x5e, 0x63, 0x58, 0xd1, 0xa2,
+  0x25, 0x22, 0x7c, 0x3b, 0x01, 0x21, 0x78, 0x87,
+  0xd4, 0x00, 0x46, 0x57, 0x9f, 0xd3, 0x27, 0x52,
+  0x4c, 0x36, 0x02, 0xe7, 0xa0, 0xc4, 0xc8, 0x9e,
+  0xea, 0xbf, 0x8a, 0xd2, 0x40, 0xc7, 0x38, 0xb5,
+  0xa3, 0xf7, 0xf2, 0xce, 0xf9, 0x61, 0x15, 0xa1,
+  0xe0, 0xae, 0x5d, 0xa4, 0x9b, 0x34, 0x1a, 0x55,
+  0xad, 0x93, 0x32, 0x30, 0xf5, 0x8c, 0xb1, 0xe3,
+  0x1d, 0xf6, 0xe2, 0x2e, 0x82, 0x66, 0xca, 0x60,
+  0xc0, 0x29, 0x23, 0xab, 0x0d, 0x53, 0x4e, 0x6f,
+  0xd5, 0xdb, 0x37, 0x45, 0xde, 0xfd, 0x8e, 0x2f,
+  0x03, 0xff, 0x6a, 0x72, 0x6d, 0x6c, 0x5b, 0x51,
+  0x8d, 0x1b, 0xaf, 0x92, 0xbb, 0xdd, 0xbc, 0x7f,
+  0x11, 0xd9, 0x5c, 0x41, 0x1f, 0x10, 0x5a, 0xd8,
+  0x0a, 0xc1, 0x31, 0x88, 0xa5, 0xcd, 0x7b, 0xbd,
+  0x2d, 0x74, 0xd0, 0x12, 0xb8, 0xe5, 0xb4, 0xb0,
+  0x89, 0x69, 0x97, 0x4a, 0x0c, 0x96, 0x77, 0x7e,
+  0x65, 0xb9, 0xf1, 0x09, 0xc5, 0x6e, 0xc6, 0x84,
+  0x18, 0xf0, 0x7d, 0xec, 0x3a, 0xdc, 0x4d, 0x20,
+  0x79, 0xee, 0x5f, 0x3e, 0xd7, 0xcb, 0x39, 0x48
+};
+
+static uint32_t
+sm4_t_non_lin_sub(uint32_t x)
+{
+  uint32_t out;
+
+  out  = (uint32_t)sbox[x &amp; 0xff];
+  out |= (uint32_t)sbox[(x &gt;&gt; 8) &amp; 0xff] &lt;&lt; 8;
+  out |= (uint32_t)sbox[(x &gt;&gt; 16) &amp; 0xff] &lt;&lt; 16;
+  out |= (uint32_t)sbox[(x &gt;&gt; 24) &amp; 0xff] &lt;&lt; 24;
+
+  return out;
+}
+
+static uint32_t
+sm4_key_lin_sub(uint32_t x)
+{
+  return x ^ ROTL32(13, x) ^ ROTL32(23, x);
+}
+
+static uint32_t
+sm4_enc_lin_sub(uint32_t x)
+{
+  return x ^ ROTL32(2, x) ^ ROTL32(10, x) ^ ROTL32(18, x) ^ ROTL32(24, x);
+}
+
+static uint32_t
+sm4_key_sub(uint32_t x)
+{
+  return sm4_key_lin_sub(sm4_t_non_lin_sub(x));
+}
+
+static uint32_t
+sm4_enc_sub(uint32_t x)
+{
+  return sm4_enc_lin_sub(sm4_t_non_lin_sub(x));
+}
+
+static uint32_t
+sm4_round(uint32_t x0, uint32_t x1, uint32_t x2, uint32_t x3, uint32_t rk)
+{
+  return x0 ^ sm4_enc_sub(x1 ^ x2 ^ x3 ^ rk);
+}
+
+static void
+sm4_set_key(struct sm4_ctx *ctx, const uint8_t *key, int encrypt)
+{
+  uint32_t rk0, rk1, rk2, rk3;
+  unsigned i;
+
+  rk0 = READ_UINT32(key +  0) ^ fk[0];
+  rk1 = READ_UINT32(key +  4) ^ fk[1];
+  rk2 = READ_UINT32(key +  8) ^ fk[2];
+  rk3 = READ_UINT32(key + 12) ^ fk[3];
+
+  for (i = 0; i &lt; 32; i += 4)
+    {
+      rk0 ^= sm4_key_sub(rk1 ^ rk2 ^ rk3 ^ ck[i + 0]);
+      rk1 ^= sm4_key_sub(rk2 ^ rk3 ^ rk0 ^ ck[i + 1]);
+      rk2 ^= sm4_key_sub(rk3 ^ rk0 ^ rk1 ^ ck[i + 2]);
+      rk3 ^= sm4_key_sub(rk0 ^ rk1 ^ rk2 ^ ck[i + 3]);
+
+      if (encrypt)
+        {
+          ctx-&gt;rkey[i + 0] = rk0;
+          ctx-&gt;rkey[i + 1] = rk1;
+          ctx-&gt;rkey[i + 2] = rk2;
+          ctx-&gt;rkey[i + 3] = rk3;
+        }
+      else
+        {
+          ctx-&gt;rkey[31 - 0 - i] = rk0;
+          ctx-&gt;rkey[31 - 1 - i] = rk1;
+          ctx-&gt;rkey[31 - 2 - i] = rk2;
+          ctx-&gt;rkey[31 - 3 - i] = rk3;
+        }
+    }
+}
+
+void
+sm4_set_encrypt_key(struct sm4_ctx *ctx, const uint8_t *key)
+{
+  sm4_set_key(ctx, key, 1);
+}
+
+void
+sm4_set_decrypt_key(struct sm4_ctx *ctx, const uint8_t *key)
+{
+  sm4_set_key(ctx, key, 0);
+}
+
+void
+sm4_crypt(const struct sm4_ctx *context,
+	  size_t length,
+	  uint8_t *dst,
+	  const uint8_t *src)
+{
+  const uint32_t *rk = context-&gt;rkey;
+
+  assert( !(length % SM4_BLOCK_SIZE) );
+
+  for ( ; length; length -= SM4_BLOCK_SIZE)
+    {
+      uint32_t x0, x1, x2, x3;
+      unsigned i;
+
+      x0 = READ_UINT32(src + 0 * 4);
+      x1 = READ_UINT32(src + 1 * 4);
+      x2 = READ_UINT32(src + 2 * 4);
+      x3 = READ_UINT32(src + 3 * 4);
+
+      for (i = 0; i &lt; 32; i += 4)
+        {
+          x0 = sm4_round(x0, x1, x2, x3, rk[i + 0]);
+          x1 = sm4_round(x1, x2, x3, x0, rk[i + 1]);
+          x2 = sm4_round(x2, x3, x0, x1, rk[i + 2]);
+          x3 = sm4_round(x3, x0, x1, x2, rk[i + 3]);
+        }
+
+      WRITE_UINT32(dst + 0 * 4, x3);
+      WRITE_UINT32(dst + 1 * 4, x2);
+      WRITE_UINT32(dst + 2 * 4, x1);
+      WRITE_UINT32(dst + 3 * 4, x0);
+
+      src += SM4_BLOCK_SIZE;
+      dst += SM4_BLOCK_SIZE;
+    }
+}
diff --git a/sm4.h b/sm4.h
new file mode 100644
index 00000000..608eb3f3
--- /dev/null
+++ b/sm4.h
@@ -0,0 +1,69 @@
+/* sm4.h
+
+   Copyright (C) 2022 Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+*/
+
+#ifndef NETTLE_SM4_H_INCLUDED
+#define NETTLE_SM4_H_INCLUDED
+
+#include "nettle-types.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/* Name mangling */
+#define sm4_set_encrypt_key nettle_sm4_set_encrypt_key
+#define sm4_set_decrypt_key nettle_sm4_set_decrypt_key
+#define sm4_crypt nettle_sm4_crypt
+
+#define SM4_BLOCK_SIZE 16
+#define SM4_KEY_SIZE 16
+
+struct sm4_ctx
+{
+  uint32_t rkey[32];
+};
+
+void
+sm4_set_encrypt_key(struct sm4_ctx *ctx, const uint8_t *key);
+
+void
+sm4_set_decrypt_key(struct sm4_ctx *ctx, const uint8_t *key);
+
+void
+sm4_crypt(const struct sm4_ctx *context,
+	  size_t length, uint8_t *dst,
+	  const uint8_t *src);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* NETTLE_SM4_H_INCLUDED */
diff --git a/testsuite/meta-cipher-test.c b/testsuite/meta-cipher-test.c
index f949fd76..62488b7f 100644
--- a/testsuite/meta-cipher-test.c
+++ b/testsuite/meta-cipher-test.c
@@ -18,7 +18,8 @@ const char* ciphers[] = {
   "serpent256",
   "twofish128",
   "twofish192",
-  "twofish256"
+  "twofish256",
+  "sm4"
 };
 
 void
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083712</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:12-0400</timestampReceived><subject>[PATCH v2 3/7] testsuite: add test for SM4 symmetric algorithm</subject><body>

Add a testuite for SM4 symmetric algorithm. Test vectors are based
on: https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 testsuite/.gitignore  |  1 +
 testsuite/Makefile.in |  2 +-
 testsuite/sm4-test.c  | 19 +++++++++++++++++++
 3 files changed, 21 insertions(+), 1 deletion(-)
 create mode 100644 testsuite/sm4-test.c

diff --git a/testsuite/.gitignore b/testsuite/.gitignore
index ca41472e..07127d2b 100644
--- a/testsuite/.gitignore
+++ b/testsuite/.gitignore
@@ -98,6 +98,7 @@
 /sha512-256-test
 /sha512-test
 /sm3-test
+/sm4-test
 /streebog-test
 /twofish-test
 /umac-test
diff --git a/testsuite/Makefile.in b/testsuite/Makefile.in
index 6734d3e6..c2662826 100644
--- a/testsuite/Makefile.in
+++ b/testsuite/Makefile.in
@@ -24,7 +24,7 @@ TS_NETTLE_SOURCES = aes-test.c aes-keywrap-test.c arcfour-test.c arctwo-test.c \
 		    sha384-test.c sha512-test.c sha512-224-test.c sha512-256-test.c \
 		    sha3-permute-test.c sha3-224-test.c sha3-256-test.c \
 		    sha3-384-test.c sha3-512-test.c \
-		    shake256-test.c streebog-test.c sm3-test.c \
+		    shake256-test.c streebog-test.c sm3-test.c sm4-test.c \
 		    serpent-test.c twofish-test.c version-test.c \
 		    knuth-lfib-test.c \
 		    cbc-test.c cfb-test.c ctr-test.c gcm-test.c eax-test.c ccm-test.c \
diff --git a/testsuite/sm4-test.c b/testsuite/sm4-test.c
new file mode 100644
index 00000000..97d9d58a
--- /dev/null
+++ b/testsuite/sm4-test.c
@@ -0,0 +1,19 @@
+#include "testutils.h"
+#include "sm4.h"
+
+void
+test_main(void)
+{
+  /* test vectors from:
+   * https://tools.ietf.org/id/draft-ribose-cfrg-sm4-10.html
+   */
+  test_cipher(&amp;nettle_sm4,
+	      SHEX("0123456789ABCDEF FEDCBA9876543210"),
+	      SHEX("0123456789ABCDEF FEDCBA9876543210"),
+	      SHEX("681EDF34D206965E 86B3E94F536E4246"));
+
+  test_cipher(&amp;nettle_sm4,
+	      SHEX("FEDCBA9876543210 0123456789ABCDEF"),
+	      SHEX("0001020304050607 08090A0B0C0D0E0F"),
+	      SHEX("F766678F13F01ADE AC1B3EA955ADB594"));
+}
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083713</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:13-0400</timestampReceived><subject>[PATCH v2 4/7] nettle-benchmark: bench SM4 symmetric algorithm</subject><body>

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 examples/nettle-benchmark.c | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/examples/nettle-benchmark.c b/examples/nettle-benchmark.c
index ba5dd284..802a7234 100644
--- a/examples/nettle-benchmark.c
+++ b/examples/nettle-benchmark.c
@@ -63,6 +63,7 @@
 #include "sha1.h"
 #include "sha2.h"
 #include "sha3.h"
+#include "sm4.h"
 #include "twofish.h"
 #include "umac.h"
 #include "cmac.h"
@@ -926,6 +927,7 @@ main(int argc, char **argv)
       &amp;nettle_des3,
       &amp;nettle_serpent256,
       &amp;nettle_twofish128, &amp;nettle_twofish192, &amp;nettle_twofish256,
+      &amp;nettle_sm4,
       NULL
     };
 
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083714</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:14-0400</timestampReceived><subject>[PATCH v2 5/7] doc: documentation for SM4 cipher algorithm</subject><body>

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 nettle.texinfo | 39 +++++++++++++++++++++++++++++++++++++++
 1 file changed, 39 insertions(+)

diff --git a/nettle.texinfo b/nettle.texinfo
index 45b06720..a6cc9379 100644
--- a/nettle.texinfo
+++ b/nettle.texinfo
@@ -297,6 +297,9 @@ Written by @value{AUTHOR}.
 The C implementation of the SM3 message digest is written by Tianjia
 Zhang, and the code is based on the implementation by Jia Zhang.
 
+@item SM4
+The implementation of the SM4 cipher is written by Tianjia Zhang.
+
 @item TWOFISH
 The implementation of the TWOFISH cipher is written by Ruud de Rooij.
 
@@ -2277,6 +2280,42 @@ in any other way.
 Analogous to @code{twofish_encrypt}
 @end deftypefun
 
+@node SM4
+@subsection SM4
+@cindex SM4
+
+SM4 is a block cipher standard adopted by the government of the People's
+Republic of China, and it was issued by the State Cryptography Administration
+on March 21, 2012. The standard is GM/T 0002-2012 "SM4 block cipher algorithm".
+Nettle defines it in @file{&lt;nettle/sm4.h&gt;}.
+
+@deftp {Context struct} {struct sm4_ctx}
+@end deftp
+
+@defvr Constant SM4_BLOCK_SIZE
+The SM4 block-size, 16.
+@end defvr
+
+@defvr Constant SM4_KEY_SIZE
+Default SM4 key size, 16.
+@end defvr
+
+@deftypefun void sm4_set_encrypt_key (struct sm4_ctx *@var{ctx}, const uint8_t \
*@var{key}) +Initialize the cipher. The function is used for encryption.
+@end deftypefun
+
+@deftypefun void sm4_set_decrypt_key (struct sm4_ctx *@var{ctx}, const uint8_t \
*@var{key}) +Initialize the cipher. The function is used for decryption.
+@end deftypefun
+
+@deftypefun void sm4_crypt (const struct sm4_ctx *@var{ctx}, size_t @var{length}, \
uint8_t *@var{dst}, const uint8_t *@var{src}) +Cryption function. @var{length} must \
be an integral multiple of the +block size. If it is more than one block, the data is \
processed in ECB +mode. @code{src} and @code{dst} may be equal, but they must not \
overlap +in any other way. The same function is used for both encryption and
+decryption.
+@end deftypefun
+
 @node nettle_cipher abstraction
 @subsection The @code{struct nettle_cipher} abstraction
 @cindex nettle_cipher
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220221083715</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:15-0400</timestampReceived><subject>[PATCH v2 6/7] gcm: Add SM4 as the GCM underlying cipher</subject><body>

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 Makefile.in                |  1 +
 gcm-sm4-meta.c             | 60 ++++++++++++++++++++++++++++
 gcm-sm4.c                  | 81 ++++++++++++++++++++++++++++++++++++++
 gcm.h                      | 25 +++++++++++-
 nettle-meta-aeads.c        |  1 +
 nettle-meta.h              |  1 +
 testsuite/gcm-test.c       | 18 +++++++++
 testsuite/meta-aead-test.c |  1 +
 8 files changed, 187 insertions(+), 1 deletion(-)
 create mode 100644 gcm-sm4-meta.c
 create mode 100644 gcm-sm4.c

diff --git a/Makefile.in b/Makefile.in
index ba1a2db2..e96bac31 100644
--- a/Makefile.in
+++ b/Makefile.in
@@ -112,6 +112,7 @@ nettle_SOURCES = aes-decrypt-internal.c aes-decrypt.c aes-decrypt-table.c \
 		 gcm-aes256.c gcm-aes256-meta.c \
 		 gcm-camellia128.c gcm-camellia128-meta.c \
 		 gcm-camellia256.c gcm-camellia256-meta.c \
+		 gcm-sm4.c gcm-sm4-meta.c \
 		 cmac.c cmac64.c cmac-aes128.c cmac-aes256.c cmac-des3.c \
 		 cmac-aes128-meta.c cmac-aes256-meta.c cmac-des3-meta.c \
 		 gost28147.c gosthash94.c gosthash94-meta.c \
diff --git a/gcm-sm4-meta.c b/gcm-sm4-meta.c
new file mode 100644
index 00000000..090460d3
--- /dev/null
+++ b/gcm-sm4-meta.c
@@ -0,0 +1,60 @@
+/* gcm-sm4-meta.c
+
+   Copyright (C) 2022 Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+*/
+
+#if HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include &lt;assert.h&gt;
+
+#include "nettle-meta.h"
+
+#include "gcm.h"
+
+static nettle_set_key_func gcm_sm4_set_nonce_wrapper;
+static void
+gcm_sm4_set_nonce_wrapper (void *ctx, const uint8_t *nonce)
+{
+  gcm_sm4_set_iv (ctx, GCM_IV_SIZE, nonce);
+}
+
+const struct nettle_aead nettle_gcm_sm4 =
+  { "gcm_sm4", sizeof(struct gcm_sm4_ctx),
+    GCM_BLOCK_SIZE, SM4_KEY_SIZE,
+    GCM_IV_SIZE, GCM_DIGEST_SIZE,
+    (nettle_set_key_func *) gcm_sm4_set_key,
+    (nettle_set_key_func *) gcm_sm4_set_key,
+    gcm_sm4_set_nonce_wrapper,
+    (nettle_hash_update_func *) gcm_sm4_update,
+    (nettle_crypt_func *) gcm_sm4_encrypt,
+    (nettle_crypt_func *) gcm_sm4_decrypt,
+    (nettle_hash_digest_func *) gcm_sm4_digest,
+  };
diff --git a/gcm-sm4.c b/gcm-sm4.c
new file mode 100644
index 00000000..19d91ae9
--- /dev/null
+++ b/gcm-sm4.c
@@ -0,0 +1,81 @@
+/* gcm-sm4.c
+
+   Galois counter mode using SM4 as the underlying cipher.
+
+   Copyright (C) 2022 Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
+
+   This file is part of GNU Nettle.
+
+   GNU Nettle is free software: you can redistribute it and/or
+   modify it under the terms of either:
+
+     * the GNU Lesser General Public License as published by the Free
+       Software Foundation; either version 3 of the License, or (at your
+       option) any later version.
+
+   or
+
+     * the GNU General Public License as published by the Free
+       Software Foundation; either version 2 of the License, or (at your
+       option) any later version.
+
+   or both in parallel, as here.
+
+   GNU Nettle is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received copies of the GNU General Public License and
+   the GNU Lesser General Public License along with this program.  If
+   not, see http://www.gnu.org/licenses/.
+*/
+
+#if HAVE_CONFIG_H
+# include "config.h"
+#endif
+
+#include &lt;assert.h&gt;
+
+#include "gcm.h"
+
+void
+gcm_sm4_set_key(struct gcm_sm4_ctx *ctx, const uint8_t *key)
+{
+  GCM_SET_KEY(ctx, sm4_set_encrypt_key, sm4_crypt, key);
+}
+
+void
+gcm_sm4_set_iv(struct gcm_sm4_ctx *ctx,
+	       size_t length, const uint8_t *iv)
+{
+  GCM_SET_IV (ctx, length, iv);
+}
+
+void
+gcm_sm4_update(struct gcm_sm4_ctx *ctx,
+	       size_t length, const uint8_t *data)
+{
+  GCM_UPDATE (ctx, length, data);
+}
+
+void
+gcm_sm4_encrypt(struct gcm_sm4_ctx *ctx,
+		size_t length, uint8_t *dst, const uint8_t *src)
+{
+  GCM_ENCRYPT(ctx, sm4_crypt, length, dst, src);
+}
+
+void
+gcm_sm4_decrypt(struct gcm_sm4_ctx *ctx,
+		size_t length, uint8_t *dst, const uint8_t *src)
+{
+  GCM_DECRYPT(ctx, sm4_crypt, length, dst, src);
+}
+
+void
+gcm_sm4_digest(struct gcm_sm4_ctx *ctx,
+	       size_t length, uint8_t *digest)
+{
+  GCM_DIGEST(ctx, sm4_crypt, length, digest);
+}
diff --git a/gcm.h b/gcm.h
index 96578530..39af5ab0 100644
--- a/gcm.h
+++ b/gcm.h
@@ -40,6 +40,7 @@
 
 #include "aes.h"
 #include "camellia.h"
+#include "sm4.h"
 
 #ifdef __cplusplus
 extern "C" {
@@ -95,6 +96,13 @@ extern "C" {
 #define gcm_camellia256_decrypt nettle_gcm_camellia256_decrypt
 #define gcm_camellia256_digest nettle_gcm_camellia256_digest
 
+#define gcm_sm4_set_key nettle_gcm_sm4_set_key
+#define gcm_sm4_set_iv nettle_gcm_sm4_set_iv
+#define gcm_sm4_update nettle_gcm_sm4_update
+#define gcm_sm4_encrypt nettle_gcm_sm4_encrypt
+#define gcm_sm4_decrypt nettle_gcm_sm4_decrypt
+#define gcm_sm4_digest nettle_gcm_sm4_digest
+
 #define GCM_BLOCK_SIZE 16
 #define GCM_IV_SIZE (GCM_BLOCK_SIZE - 4)
 #define GCM_DIGEST_SIZE 16
@@ -322,7 +330,22 @@ void gcm_camellia256_decrypt(struct gcm_camellia256_ctx *ctx,
 void gcm_camellia256_digest(struct gcm_camellia256_ctx *ctx,
 			    size_t length, uint8_t *digest);
 
-  
+
+struct gcm_sm4_ctx GCM_CTX(struct sm4_ctx);
+
+void gcm_sm4_set_key(struct gcm_sm4_ctx *ctx, const uint8_t *key);
+void gcm_sm4_set_iv(struct gcm_sm4_ctx *ctx,
+		    size_t length, const uint8_t *iv);
+void gcm_sm4_update(struct gcm_sm4_ctx *ctx,
+		    size_t length, const uint8_t *data);
+void gcm_sm4_encrypt(struct gcm_sm4_ctx *ctx,
+		     size_t length, uint8_t *dst, const uint8_t *src);
+void gcm_sm4_decrypt(struct gcm_sm4_ctx *ctx,
+		     size_t length, uint8_t *dst, const uint8_t *src);
+void gcm_sm4_digest(struct gcm_sm4_ctx *ctx,
+		    size_t length, uint8_t *digest);
+
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/nettle-meta-aeads.c b/nettle-meta-aeads.c
index c99cc465..78f38a3c 100644
--- a/nettle-meta-aeads.c
+++ b/nettle-meta-aeads.c
@@ -43,6 +43,7 @@ const struct nettle_aead * const _nettle_aeads[] = {
   &amp;nettle_gcm_aes256,
   &amp;nettle_gcm_camellia128,
   &amp;nettle_gcm_camellia256,
+  &amp;nettle_gcm_sm4,
   &amp;nettle_eax_aes128,
   &amp;nettle_chacha_poly1305,
   NULL
diff --git a/nettle-meta.h b/nettle-meta.h
index 3d0440e8..19dc96c5 100644
--- a/nettle-meta.h
+++ b/nettle-meta.h
@@ -200,6 +200,7 @@ extern const struct nettle_aead nettle_gcm_aes192;
 extern const struct nettle_aead nettle_gcm_aes256;
 extern const struct nettle_aead nettle_gcm_camellia128;
 extern const struct nettle_aead nettle_gcm_camellia256;
+extern const struct nettle_aead nettle_gcm_sm4;
 extern const struct nettle_aead nettle_eax_aes128;
 extern const struct nettle_aead nettle_chacha_poly1305;
 
diff --git a/testsuite/gcm-test.c b/testsuite/gcm-test.c
index d68af4e0..1897c41a 100644
--- a/testsuite/gcm-test.c
+++ b/testsuite/gcm-test.c
@@ -574,6 +574,24 @@ test_main(void)
 		 "16aedbf5a0de6a57 a637b39b"),	/* iv */
 	    SHEX("5791883f822013f8bd136fc36fb9946b"));	/* tag */
 
+  /*
+   * GCM-SM4 Test Vectors from
+   * https://datatracker.ietf.org/doc/html/rfc8998
+   */
+  test_aead(&amp;nettle_gcm_sm4, NULL,
+	    SHEX("0123456789ABCDEFFEDCBA9876543210"),
+	    SHEX("FEEDFACEDEADBEEFFEEDFACEDEADBEEFABADDAD2"),
+	    SHEX("AAAAAAAAAAAAAAAABBBBBBBBBBBBBBBB"
+	         "CCCCCCCCCCCCCCCCDDDDDDDDDDDDDDDD"
+	         "EEEEEEEEEEEEEEEEFFFFFFFFFFFFFFFF"
+	         "EEEEEEEEEEEEEEEEAAAAAAAAAAAAAAAA"),
+	    SHEX("17F399F08C67D5EE19D0DC9969C4BB7D"
+	         "5FD46FD3756489069157B282BB200735"
+	         "D82710CA5C22F0CCFA7CBF93D496AC15"
+	         "A56834CBCF98C397B4024A2691233B8D"),
+	    SHEX("00001234567800000000ABCD"),
+	    SHEX("83DE3541E4C2B58177E065A9BF7B62EC"));
+
   /* Test gcm_hash, with varying message size, keys and iv all zero.
      Not compared to any other implementation. */
   test_gcm_hash (SDATA("a"),
diff --git a/testsuite/meta-aead-test.c b/testsuite/meta-aead-test.c
index 1fcede40..ceeca227 100644
--- a/testsuite/meta-aead-test.c
+++ b/testsuite/meta-aead-test.c
@@ -8,6 +8,7 @@ const char* aeads[] = {
   "gcm_aes256",
   "gcm_camellia128",
   "gcm_camellia256",
+  "gcm_sm4",
   "eax_aes128",
   "chacha_poly1305",
 };
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220221083716</emailId><senderName>Tianjia Zhang</senderName><senderEmail>tianjia.zhang@linux.alibaba.com</senderEmail><timestampReceived>2022-02-21 08:37:16-0400</timestampReceived><subject>[PATCH v2 7/7] doc: documentation for GCM using SM4 cipher</subject><body>

Signed-off-by: Tianjia Zhang &lt;tianjia.zhang@linux.alibaba.com&gt;
---
 nettle.texinfo | 38 ++++++++++++++++++++++++++++++++++++++
 1 file changed, 38 insertions(+)

diff --git a/nettle.texinfo b/nettle.texinfo
index a6cc9379..7a256eda 100644
--- a/nettle.texinfo
+++ b/nettle.texinfo
@@ -3360,6 +3360,44 @@ that @var{length} is @code{GCM_DIGEST_SIZE}, but if you \
provide a smaller  value, only the first @var{length} octets of the digest are \
written.  @end deftypefun
 
+@subsubsection @acronym{GCM}-SM4 interface
+
+The following functions implement the case of @acronym{GCM} using
+SM4 as the underlying cipher.
+
+@deftp {Context struct} {struct gcm_sm4_ctx}
+Context structs, defined using @code{GCM_CTX}.
+@end deftp
+
+@deftypefun void gcm_sm4_set_key (struct gcm_sm4_ctx *@var{ctx}, const uint8_t \
*@var{key}) +Initializes @var{ctx} using the given key.
+@end deftypefun
+
+@deftypefun void gcm_sm4_set_iv (struct gcm_sm4_ctx *@var{ctx}, size_t @var{length}, \
const uint8_t *@var{iv}) +Initializes the per-message state, using the given \
@acronym{IV}. +@end deftypefun
+
+@deftypefun void gcm_sm4_update (struct gcm_sm4_ctx *@var{ctx}, size_t @var{length}, \
const uint8_t *@var{data}) +Provides associated data to be authenticated. If used, \
must be called +before @code{gcm_sm4_encrypt} or @code{gcm_sm4_decrypt}. All but the
+last call for each message @emph{must} use a length that is a multiple
+of the block size.
+@end deftypefun
+
+@deftypefun void gcm_sm4_encrypt (struct gcm_sm4_ctx *@var{ctx}, size_t \
@var{length}, uint8_t *@var{dst}, const uint8_t *@var{src}) +@deftypefunx void \
gcm_sm4_decrypt (struct gcm_sm4_ctx *@var{ctx}, size_t @var{length}, uint8_t \
*@var{dst}, const uint8_t *@var{src}) +Encrypts or decrypts the data of a message. \
All but the last call for +each message @emph{must} use a length that is a multiple \
of the block +size.
+@end deftypefun
+
+@deftypefun void gcm_sm4_digest (struct gcm_sm4_ctx *@var{ctx}, size_t @var{length}, \
uint8_t *@var{digest}) +Extracts the message digest (also known ``authentication \
tag''). This is +the final operation when processing a message. It's strongly \
recommended +that @var{length} is @code{GCM_DIGEST_SIZE}, but if you provide a \
smaller +value, only the first @var{length} octets of the digest are written.
+@end deftypefun
+
 @node CCM
 @subsection Counter with CBC-MAC mode
 
-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220223093449</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-02-23 09:34:49-0400</timestampReceived><subject>Re: Poly1305 based on radix 2^44 for S390x and PowerPC</subject><body>

On Wed, Feb 23, 2022 at 11:11 AM Amos Jeffries &lt;squid3@treenet.co.nz&gt; wrote:

&gt; On 23/02/22 13:28, Maamoun TK wrote:
&gt; &gt;
&gt; &gt; Benchmark the implementations on POWER9 and Z15
&gt; &gt;
&gt; *---------------------------------------------------------------------------------------------*
&gt; &gt; |    Arch           |   Nettle patch (radix 2^44)    |   OpenSSL (radix
&gt; &gt; 2^26)   |
&gt; &gt;
&gt; |-------------------|--------------------------------------|-----------------------------------|
&gt; &gt;
&gt; &gt; |   PowerPC    |    0.972 cycles/byte              |    1.453 cycles/byte
&gt; &gt;         |
&gt; &gt;
&gt; |-------------------|--------------------------------------|-----------------------------------|
&gt; &gt; |   IBMz           |    0.840 cycles/byte              |    0.936
&gt; &gt; cycles/byte          |
&gt; &gt;
&gt; *---------------------------------------------------------------------------------------------*
&gt; &gt;
&gt;
&gt;
&gt; What would be more useful here is a column against the non-patched
&gt; Nettle speeds. It is sure nice to see nettle being faster than OpenSSL,
&gt; but what if this proposed change is actually a degrade in current nettle
&gt; speeds on those machines?
&gt;

I've attached a benchmark numbers of these patches against nettle
mainstream that reference C implementation in the merge request
descriptions. There's no drawback here whatsoever.

regards,
Mamone


&gt; Amos
&gt; _______________________________________________
&gt; nettle-bugs mailing list
&gt; nettle-bugs@lists.lysator.liu.se
&gt; http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220223094054</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-02-23 09:40:54-0400</timestampReceived><subject>Re: gcm/ghash organization (was Re: x86_64 gcm)</subject><body>

On Tue, Feb 22, 2022 at 9:55 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; nisse@lysator.liu.se (Niels Möller) writes:
&gt;
&gt; &gt; I'm considering reorganizing the internal gcm functions. I think I'd
&gt; &gt; like to have
&gt; &gt;
&gt; &gt;   void
&gt; &gt;   _nettle_ghash_set_key (struct gcm_key *gcm, const union nettle_block16
&gt; *key);
&gt; &gt;
&gt; &gt; which sets the key (typically, the key block is zero encrypte using aes).
&gt; &gt;
&gt; &gt;   void
&gt; &gt;   _nettle_ghash_update (const struct gcm_key *key, union nettle_block16
&gt; *x,
&gt; &gt;                       size_t length, const uint8_t *data);
&gt; &gt;
&gt; &gt; where the input is complete blocks (padding done in the calling C code).
&gt; &gt; Not sure if length should be block count or byte count.
&gt;
&gt; I'm trying this out, on the branch ghash-refactor, new internal
&gt; interface in
&gt;
&gt; https://git.lysator.liu.se/nettle/nettle/-/blob/ghash-refactor/ghash-internal.h
&gt;
&gt; I settled for block count rather than byte count.
&gt;
&gt; &gt;   void
&gt; &gt;   _nettle_ghash_digest (union nettle_block16 *digest, const union
&gt; nettle_block16 *x);
&gt;
&gt; And I've dropped this function. Using different byte order complicates
&gt; unit testing, testing, and I think cost of byteswapping the 16-byte
&gt; state at start and end of ghash_update is pretty small.
&gt;
&gt; I've done the needed changes for the C, the x86_64, arm64 and powerpc64
&gt; implementations. s390x code also needs update, I hope to get to that in
&gt; a few days (unless someone else wants to do that).
&gt;

I handled the s390x part and pushed a MR for changes.


&gt; Update has been fairly simple, split gcm_hash.asm into one file each for
&gt; gcm_init_key and gcm_hash, update functions to new names and
&gt; conventions, and delete the code to handle a partial block at the end of
&gt; gsm_hash. Some small further simplifications are likely possible.
&gt;
&gt; &gt; Would perhaps be good to also delete the code for GCM_TABLE_BITS != 8,
&gt; &gt; which isn't enabled and haven't been tested in years.
&gt;
&gt; Done that too.
&gt;
&gt; The main gain is less complexity in the asm code, which no longer needs
&gt; to deal with partial blocks, and less #ifdef complexity in the fat build
&gt; setup.
&gt;

Good point, the new structure makes more sense from a low-level perspective.

regards,
Mamone


&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt; _______________________________________________
&gt; nettle-bugs mailing list
&gt; nettle-bugs@lists.lysator.liu.se
&gt; http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220119164359</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-19 16:43:59-0400</timestampReceived><subject>[S390x] Optimize various elliptic curve functions</subject><body>

 I created merge requests that have improvements of prime modulo functions
of elliptic curves and scalar multiply of curve25519 and curve448 for s390x
architecture
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/42
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/43
The prime modulo functions are implemented using basic arithmetic
instructions (including addition with carry and subtraction with borrow)
while scalar multiply of curve25519 and curve448 takes advantage of
hardware-accelerated instruction pcc (Perform Cryptographic Computation).
Benchmark numbers of the patches are attached to the MR description.

regards,
Mamone
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220120211651</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-01-20 21:16:51-0400</timestampReceived><subject>Re: [Arm64, S390x] Optimize Chacha20</subject><body>

On Thu, Jan 20, 2022 at 11:08 PM Maamoun TK &lt;maamoun.tk@googlemail.com&gt;
wrote:

&gt; On Thu, Jan 20, 2022 at 10:32 PM Niels Möller &lt;nisse@lysator.liu.se&gt;
&gt; wrote:
&gt;
&gt;&gt; Maamoun TK &lt;maamoun.tk@googlemail.com&gt; writes:
&gt;&gt;
&gt;&gt; &gt; As far as I understand, SIMD is called Advanced SIMD on AArch64 and it's
&gt;&gt; &gt; standard for this architecture. simd is enabled by default in GCC but it
&gt;&gt; &gt; can be disabled with nosimd option as I can see in here
&gt;&gt; &gt; https://gcc.gnu.org/onlinedocs/gcc/AArch64-Options.html which is why I
&gt;&gt; made
&gt;&gt; &gt; a specific config option for it.
&gt;&gt;
&gt;&gt; If it's present on all known aarch64 systems (and HWCAP_ASIMD flag
&gt;&gt; always set), I think we can keep things simpler and use the code
&gt;&gt; unconditionally, with no extra subdir, no fat build function pointers or
&gt;&gt; configure flag.
&gt;&gt;
&gt;
&gt; Ok, I'll commit the changes with vanilla assembly files.
&gt;

Done! The MR is updated
https://git.lysator.liu.se/nettle/nettle/-/merge_requests/37

regards,
Mamone


&gt;
&gt;
&gt;&gt; I've pushed the merge button for the s390x merge request.
&gt;&gt;
&gt;
&gt; Nice! I've made various tests on each core function so merging the changes
&gt; is gonna be ok.
&gt;
&gt; In another topic, I'm making experiments on your poly1305 optimizing tips
&gt; and I'll get back to you once I'm up to something.
&gt;
&gt; regards,
&gt; Mamone
&gt;
&gt; Regards,
&gt;&gt; /Niels
&gt;&gt;
&gt;&gt; --
&gt;&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt;&gt; Internet email is subject to wholesale government surveillance.
&gt;&gt;
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220121035321</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 03:53:21-0400</timestampReceived><subject>Re: [PATCH 4/7] ecc: Add powerpc64 assembly for ecc_384_modp</subject><body>

On Tue, 2022-01-04 at 21:28 +0100, Niels Möller wrote:
&gt; 
&gt; &gt; +define(`FUNC_ALIGN', `5')
&gt; &gt; +PROLOGUE(_nettle_ecc_secp384r1_modp)
&gt; &gt; +
&gt; &gt; +       std     H0, -48(SP)
&gt; &gt; +       std     H1, -40(SP)
&gt; &gt; +       std     H2, -32(SP)
&gt; &gt; +       std     H3, -24(SP)
&gt; &gt; +       std     H4, -16(SP)
&gt; &gt; +       std     H5, -8(SP)
&gt; 
&gt; I find it clearer to use register names rather than the m4 defines
&gt; for
&gt; save and restore of callee-save registers.

Here's the modified code which uses the actual registers when saving
and restoring from stack.

Amitay.
-- 

Before marriage, a man yearns for the woman he loves. After marriage,
the
'Y' becomes silent.

["ecc-secp384r1-modp.asm" (ecc-secp384r1-modp.asm)]

C powerpc64/ecc-secp384r1-modp.asm

ifelse(`
   Copyright (C) 2021 Martin Schwenke, Amitay Isaacs &amp; Alastair D ´Silva, IBM Corporation

   Based on x86_64/ecc-secp256r1-redc.asm

   This file is part of GNU Nettle.

   GNU Nettle is free software: you can redistribute it and/or
   modify it under the terms of either:

     * the GNU Lesser General Public License as published by the Free
       Software Foundation; either version 3 of the License, or (at your
       option) any later version.

   or

     * the GNU General Public License as published by the Free
       Software Foundation; either version 2 of the License, or (at your
       option) any later version.

   or both in parallel, as here.

   GNU Nettle is distributed in the hope that it will be useful,
   but WITHOUT ANY WARRANTY; without even the implied warranty of
   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
   General Public License for more details.

   You should have received copies of the GNU General Public License and
   the GNU Lesser General Public License along with this program.  If
   not, see http://www.gnu.org/licenses/.
')

	.file "ecc-secp384r1-modp.asm"

C Register usage:

define(`SP', `r1')

define(`RP', `r4')
define(`XP', `r5')

define(`D5', `r6')
define(`T0', `r7')
define(`T1', `r8')
define(`T2', `r9')
define(`T3', `r10')
define(`T4', `r11')
define(`T5', `r12')
define(`H0', `r14')
define(`H1', `r15')
define(`H2', `r16')
define(`H3', `r17')
define(`H4', `r18')
define(`H5', `r19')
define(`C2', `r3')
define(`C0', H5)	C Overlap
define(`TMP', XP)	C Overlap


	C void ecc_secp384r1_modp (const struct ecc_modulo *m, mp_limb_t *rp, mp_limb_t *xp)
	.text
define(`FUNC_ALIGN', `5')
PROLOGUE(_nettle_ecc_secp384r1_modp)

	std	r14, -48(SP)
	std	r15, -40(SP)
	std	r16, -32(SP)
	std	r17, -24(SP)
	std	r18, -16(SP)
	std	r19, -8(SP)

	C First get top 2 limbs, which need folding twice.
	C B^10 = B^6 + B^4 + 2^32 (B-1)B^4.
	C We handle the terms as follow:
	C
	C B^6: Folded immediatly.
	C
	C B^4: Delayed, added in in the next folding.
	C
	C 2^32(B-1) B^4: Low half limb delayed until the next
	C folding. Top 1.5 limbs subtracted and shifter now, resulting
	C in 2.5 limbs. The low limb saved in D5, high 1.5 limbs added
	C in.

	ld	H4, 80(XP)
	ld	H5, 88(XP)
	C Shift right 32 bits, into H1, H0
	srdi	H1, H5, 32
	sldi	D5, H5, 32
	srdi	H0, H4, 32
	or	H0, H0, D5

	C	H1 H0
	C       -  H1 H0
	C       --------
	C       H1 H0 D5
	subfic	D5, H0, 0
	subfe	H0, H1, H0
	addme	H1, H1

	li	C2, 0
	addc	H0, H4, H0
	adde	H1, H5, H1
	addze	C2, C2

	C Add in to high part
	ld	T1, 48(XP)
	ld	T2, 56(XP)
	addc	H0, T1, H0
	adde	H1, T2, H1
	addze	C2, C2		C Do C2 later

	C +1 term
	ld	T0, 0(XP)
	ld	T1, 8(XP)
	ld	T2, 16(XP)
	ld	T3, 24(XP)
	ld	T4, 32(XP)
	ld	T5, 40(XP)
	ld	H2, 64(XP)
	ld	H3, 72(XP)
	addc	T0, H0, T0
	adde	T1, H1, T1
	adde	T2, H2, T2
	adde	T3, H3, T3
	adde	T4, H4, T4
	adde	T5, H5, T5
	li	C0, 0
	addze	C0, C0

	C +B^2 term
	addc	T2, H0, T2
	adde	T3, H1, T3
	adde	T4, H2, T4
	adde	T5, H3, T5
	addze	C0, C0

	C Shift left, including low half of H4
	sldi	H4, H4, 32
	srdi	TMP, H3, 32
	or	H4, TMP, H4

	sldi	H3, H3, 32
	srdi	TMP, H2, 32
	or	H3, TMP, H3

	sldi	H2, H2, 32
	srdi	TMP, H1, 32
	or	H2, TMP, H2

	sldi	H1, H1, 32
	srdi	TMP, H0, 32
	or	H1, TMP, H1

	sldi	H0, H0, 32

	C   H4 H3 H2 H1 H0  0
	C  -   H4 H3 H2 H1 H0
	C  ---------------
	C   H4 H3 H2 H1 H0 TMP

	subfic	TMP, H0, 0
	subfe	H0, H1, H0
	subfe	H1, H2, H1
	subfe	H2, H3, H2
	subfe	H3, H4, H3
	addme	H4, H4

	addc	T0, TMP, T0
	adde	T1, H0, T1
	adde	T2, H1, T2
	adde	T3, H2, T3
	adde	T4, H3, T4
	adde	T5, H4, T5
	addze	C0, C0

	C Remains to add in C2 and C0
	C Set H1, H0 = (2^96 - 2^32 + 1) C0
	sldi	H1, C0, 32
	subfc	H0, H1, C0
	addme	H1, H1

	C Set H3, H2 = (2^96 - 2^32 + 1) C2
	sldi	H3, C2, 32
	subfc	H2, H3, C2
	addme	H3, H3
	addc	H2, C0, H2

	li	C0, 0
	addc	T0, H0, T0
	adde	T1, H1, T1
	adde	T2, H2, T2
	adde	T3, H3, T3
	adde	T4, C2, T4
	adde	T5, D5, T5		C Value delayed from initial folding
	addze	C0, C0

	C Final unlikely carry
	sldi	H1, C0, 32
	subfc	H0, H1, C0
	addme	H1, H1

	addc	T0, H0, T0
	adde	T1, H1, T1
	adde	T2, C0, T2
	addze	T3, T3
	addze	T4, T4
	addze	T5, T5

	std	T0, 0(RP)
	std	T1, 8(RP)
	std	T2, 16(RP)
	std	T3, 24(RP)
	std	T4, 32(RP)
	std	T5, 40(RP)

	ld	r14, -48(SP)
	ld	r15, -40(SP)
	ld	r16, -32(SP)
	ld	r17, -24(SP)
	ld	r18, -16(SP)
	ld	r19, -8(SP)

	blr
EPILOGUE(_nettle_ecc_secp384r1_modp)

[Attachment #4 (text/plain)]

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs


</body></email><email><emailId>20220121040232</emailId><senderName>Amitay Isaacs</senderName><senderEmail>amitay@ozlabs.org</senderEmail><timestampReceived>2022-01-21 04:02:32-0400</timestampReceived><subject>[PATCH v2 0/6] Add powerpc64 assembly for elliptic curves</subject><body>

Hi,

This series of patches add the powerpc64 assembly for modp/redc functions
for elliptic curves P192, P224, P256, P384, P521, X25519 and X448. It results
in 15-30% performance improvements as measured on POWER9 system using
hogweed-benchmark.

I posted the modified codes in the earlier email thread, but I think
posting them as a seperate series will make them easier to cherry pick.


V2 changes:
  - Use actual register names when storing/restoring from stack
  - Drop m4 definitions which are not in use
  - Simplify C2 folding for P192 curve

Amitay Isaacs (2):
  ecc: Add powerpc64 assembly for ecc_192_modp
  ecc: Add powerpc64 assembly for ecc_224_modp

Martin Schwenke (4):
  ecc: Add powerpc64 assembly for ecc_384_modp
  ecc: Add powerpc64 assembly for ecc_521_modp
  ecc: Add powerpc64 assembly for ecc_25519_modp
  ecc: Add powerpc64 assembly for ecc_448_modp

 powerpc64/ecc-curve25519-modp.asm | 101 +++++++++++++
 powerpc64/ecc-curve448-modp.asm   | 174 +++++++++++++++++++++++
 powerpc64/ecc-secp192r1-modp.asm  |  87 ++++++++++++
 powerpc64/ecc-secp224r1-modp.asm  | 123 ++++++++++++++++
 powerpc64/ecc-secp384r1-modp.asm  | 227 ++++++++++++++++++++++++++++++
 powerpc64/ecc-secp521r1-modp.asm  | 166 ++++++++++++++++++++++
 6 files changed, 878 insertions(+)
 create mode 100644 powerpc64/ecc-curve25519-modp.asm
 create mode 100644 powerpc64/ecc-curve448-modp.asm
 create mode 100644 powerpc64/ecc-secp192r1-modp.asm
 create mode 100644 powerpc64/ecc-secp224r1-modp.asm
 create mode 100644 powerpc64/ecc-secp384r1-modp.asm
 create mode 100644 powerpc64/ecc-secp521r1-modp.asm

-- 
2.34.1

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220129142934</emailId><senderName>Niels =?utf-8?Q?M=C3=B6ller?=</senderName><senderEmail>nisse@lysator.liu.se</senderEmail><timestampReceived>2022-01-29 14:29:34-0400</timestampReceived><subject>Latency in polynomial evaluation</subject><body>

Hi, I've been thinking a bit more on the structure of polynomial
evaluation, which at a high level is rather similar for ghash and gcm.

** Intro **

The function to be computed is

  R_j = K (R_{j-1} + M_j)

where K is the secret key and M_j are the message blocks. Operations
take place in some finite field. With n message blocks, M_0, ...,
M_{n-1}, and initial R_{-1} = 0, we get

  R_{n-1} = m_{n-1}  K + m_{n-2} K^2 + ... m_0 K^n

I.e., a degree n polynomial with coefficients M_j, constant term = 0,
evaluated at the point K.

To be concrete, consider a 64-bit architecture, and a finite field where
elements are represented as two words (for poly1305, we need two words
plus a few extra bits, but ignore that now, for simplicity. And for
ghash, let's also ignore the complications from bit-reversal). Let B
represent the bignum base. I'm going to make this a bit handwavy, but
we'll have B = 2^64 or B = x^64 depending on type of field.

The finite field is represented by a mod P operation, where the
structure of P is nice, with a leading one bit followed by more than 64
zeros, and then a few more non-zero bits at the end. This implies that
we can define a multiply operation

  Y_2 B^2 + Y_1 B + Y_0 = (X_1 B + X_0) (K_1 B + K_0)  (mod P)

by four independent independent multiplication instructions (widening,
64x64 --&gt; 128) involving X and some precomputed values depending on K,
and accumulation only involving shifts/adds that have low latency. The
result is not fully reduced mod P; it consists of three words.

We can then reduce this to two words by multiplying Y_2 by a suitable
single-word constant, with final two-word result

  R = C Y_2 + Y_1 B + Y_0

Applying this to the original recurrency, R_j = K (R_{j-1} + M_j), the X
input corresponds to R_{j-1} + M_j, so the critical dependency path from
R_{j-1} to R_j includes *two* multiply latencies. E.g, if multiply latency
is 5 cycles, it's not possible to get this evaluation scheme to run
faster than 10 cycles per block. (In practice, the accumulation will
also contribute one or a few cycles to the critical path, but *much*
less than those two multiplies).

So question is, how can we do better?

** Postponing reduction **

The approach taken in the new x86_64 poly1305 code I just pushed is to
skip the final reduction, and let the state be one word larger (and this
is particularly cheap for poly1305, because we don't quite increase
state size by a word, but from two words + 3 bits to two words plus ~60
bits). The multiply operation becomes

  Y_2 B^2 + Y_1 B + Y_0 = (X_2 B^2 + X_1 B + X_0) (K_1 B + K_0)  (mod P)

This can be arranged with 6 independent multiply instructions + cheap
accumulation. (I haven't worked out the details for the ghash case, but
I do expect that it's rather practival there too).

Then the dependency chain from one block to the next is reduced to one
multiply latency, 5 cycles in our example. In case all other needed
instructions can be scheduled (manually, or by the processor's
out-of-order machinery) to run in 5 cycles in parallel with the
multiplies, we would get a running time of 5 cycles per block.

** Interleaving **

The other approach, used in the recent powerpc gcm code, is to
interleave multiple blocks. For simplicity, only consider 2-way
interleaving here. The key thing is that if we expand te recurrency
once, we get

  R_j = K (M_j + K (R_{j-2} + M_{j-1})) 
      = K M_j + K^2 (R_{j-2} + M_{j-1})

We get two field multiplications, but one of them, K M_j, is completely
independent of previous blocks (R_{j-2}), and can be computed in
parallel. It may add a cycle or so to accumulation latency, but we can
do essentially twice as much work without making the critical path
longer. We get 8 independent multiply instructions, and one dependent
for the final folding. Can be extended to more than two blocks if needed
(depending on number of available registers).

Another variant could be to separate even and odd parts of the
polynomial being evaluated, and evaluate both parts at K^2. We can then
compute the two recurrencies

  E_j = K^2 (E_{j-1} + M_{2j})
  O_j = K^2 (O_{j-1} + M_{2j+1})

in parallel. It's unclear to me what the pros and cons are compared to
previous variant. One may get some advantage from both multiplies using
the same factor K^2. On the other hand, each recurrency has to be
accumulated and folded separately, which costs instructions and
registers. Maybe more useful for hardware implementation? This variant
is currently not used in Nettle.

** Doing both **

It's possible to combine those two tricks. Processing of two blocks
would then be an operation of the form

  Z_2 B^2 + Z_1 B + Z_0 
    = (X_2 B^2 + X_1 B + X_0) K^2 + (Y_1 B + Y_0) K

Here, the Xs (three words) represent R_{j-2} + M_{j-1}, the Ys repreent
M_{j-2}, and the Zs represents R_j, as three words (without final folding).
We would need 10 independent multiples, one more than with plain
interleaving, but critical path includes only one multiply latency.

I think this is a promising alternative, if one would otherwise need to
interleaving a large number of blocks to get full utilization of the
multipliers.

** How to choose **

When implementing one of those schemes, different processor resources
may be the bottleneck. I'd expect it to be one of

 o  Multiply latency, i.e, latency of the dependency chain from one block
    to the next (including also a few additions, but multiply latency
    willb e the main part). If this is the bottleneck, it means all
    other instructions can be scheduled in parallel, and the processor
    will sit idle for some cycles, waiting for a multiply to complete.
    Typical latency for multiply is 5 times longer than for an addition
    (but ratio difers quite a bit between processors, of course)

 o  Multiply throughput, i.e., the maximum number of (independent) multiply
    instructions that can be run per cycle. Typical number is 0.5 -- 2.
    If this is the bottleneck, the processor will spend some cycles idle,
    waiting for a multiplier to be ready to accept a new input.

 o  A superscalar processor can issue several instructinos in the same
    cycle, but there's a fix small limit. Typical number is 2 -- 6. So,
    e.g., if the processor can issue maximum 4 instructions per cycle,
    the evaluation loop consists of 40 instructions, and the loop
    actually runs in close to 10 cycles per iteration, then instruction
    issue is the bottleneck.

The tricks discussed in this note are useful for finding an evaluation
scheme where multiply latency isn't a bottleneck. But once a loop hits
the limit on multiply throughput or instructions per cycle, other tricks
are needed to optimize further. In particular, the postponed reduction
has a cost in multiply throughput, since it needs some additional
multiply instruction.

I think one should aim to hit the limit on multiply throughput; that one
is hard to negotiate (it's possible to reduce the number of multiply
instructions somewhat, by the Karatsuba trick, but due to the additional
overhead, likely to be useful only on processors with particularly low
multiply throughput).

Regards,
/Niels

-- 
Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
Internet email is subject to wholesale government surveillance.

_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email><email><emailId>20220204080724</emailId><senderName>Maamoun TK</senderName><senderEmail>maamoun.tk@googlemail.com</senderEmail><timestampReceived>2022-02-04 08:07:24-0400</timestampReceived><subject>Re: Latency in polynomial evaluation</subject><body>

On Sat, Jan 29, 2022 at 4:29 PM Niels Möller &lt;nisse@lysator.liu.se&gt; wrote:

&gt; Hi, I've been thinking a bit more on the structure of polynomial
&gt; evaluation, which at a high level is rather similar for ghash and gcm.
&gt;
&gt; ** Intro **
&gt;
&gt; The function to be computed is
&gt;
&gt;   R_j = K (R_{j-1} + M_j)
&gt;
&gt; where K is the secret key and M_j are the message blocks. Operations
&gt; take place in some finite field. With n message blocks, M_0, ...,
&gt; M_{n-1}, and initial R_{-1} = 0, we get
&gt;
&gt;   R_{n-1} = m_{n-1}  K + m_{n-2} K^2 + ... m_0 K^n
&gt;
&gt; I.e., a degree n polynomial with coefficients M_j, constant term = 0,
&gt; evaluated at the point K.
&gt;
&gt; To be concrete, consider a 64-bit architecture, and a finite field where
&gt; elements are represented as two words (for poly1305, we need two words
&gt; plus a few extra bits, but ignore that now, for simplicity. And for
&gt; ghash, let's also ignore the complications from bit-reversal). Let B
&gt; represent the bignum base. I'm going to make this a bit handwavy, but
&gt; we'll have B = 2^64 or B = x^64 depending on type of field.
&gt;
&gt; The finite field is represented by a mod P operation, where the
&gt; structure of P is nice, with a leading one bit followed by more than 64
&gt; zeros, and then a few more non-zero bits at the end. This implies that
&gt; we can define a multiply operation
&gt;
&gt;   Y_2 B^2 + Y_1 B + Y_0 = (X_1 B + X_0) (K_1 B + K_0)  (mod P)
&gt;
&gt; by four independent independent multiplication instructions (widening,
&gt; 64x64 --&gt; 128) involving X and some precomputed values depending on K,
&gt; and accumulation only involving shifts/adds that have low latency. The
&gt; result is not fully reduced mod P; it consists of three words.
&gt;
&gt; We can then reduce this to two words by multiplying Y_2 by a suitable
&gt; single-word constant, with final two-word result
&gt;
&gt;   R = C Y_2 + Y_1 B + Y_0
&gt;
&gt; Applying this to the original recurrency, R_j = K (R_{j-1} + M_j), the X
&gt; input corresponds to R_{j-1} + M_j, so the critical dependency path from
&gt; R_{j-1} to R_j includes *two* multiply latencies. E.g, if multiply latency
&gt; is 5 cycles, it's not possible to get this evaluation scheme to run
&gt; faster than 10 cycles per block. (In practice, the accumulation will
&gt; also contribute one or a few cycles to the critical path, but *much*
&gt; less than those two multiplies).
&gt;
&gt; So question is, how can we do better?
&gt;
&gt; ** Postponing reduction **
&gt;
&gt; The approach taken in the new x86_64 poly1305 code I just pushed is to
&gt; skip the final reduction, and let the state be one word larger (and this
&gt; is particularly cheap for poly1305, because we don't quite increase
&gt; state size by a word, but from two words + 3 bits to two words plus ~60
&gt; bits). The multiply operation becomes
&gt;
&gt;   Y_2 B^2 + Y_1 B + Y_0 = (X_2 B^2 + X_1 B + X_0) (K_1 B + K_0)  (mod P)
&gt;
&gt; This can be arranged with 6 independent multiply instructions + cheap
&gt; accumulation. (I haven't worked out the details for the ghash case, but
&gt; I do expect that it's rather practival there too).
&gt;
&gt; Then the dependency chain from one block to the next is reduced to one
&gt; multiply latency, 5 cycles in our example. In case all other needed
&gt; instructions can be scheduled (manually, or by the processor's
&gt; out-of-order machinery) to run in 5 cycles in parallel with the
&gt; multiplies, we would get a running time of 5 cycles per block.
&gt;
&gt; ** Interleaving **
&gt;
&gt; The other approach, used in the recent powerpc gcm code, is to
&gt; interleave multiple blocks. For simplicity, only consider 2-way
&gt; interleaving here. The key thing is that if we expand te recurrency
&gt; once, we get
&gt;
&gt;   R_j = K (M_j + K (R_{j-2} + M_{j-1}))
&gt;       = K M_j + K^2 (R_{j-2} + M_{j-1})
&gt;
&gt; We get two field multiplications, but one of them, K M_j, is completely
&gt; independent of previous blocks (R_{j-2}), and can be computed in
&gt; parallel. It may add a cycle or so to accumulation latency, but we can
&gt; do essentially twice as much work without making the critical path
&gt; longer. We get 8 independent multiply instructions, and one dependent
&gt; for the final folding.


But in radix 2^64, doesn't K^2 need to be represented in 3 words as
described in a previous note so considering that we get 10 independent
multiply instructions rather than 8?


&gt; Can be extended to more than two blocks if needed
&gt; (depending on number of available registers).
&gt;
&gt; Another variant could be to separate even and odd parts of the
&gt; polynomial being evaluated, and evaluate both parts at K^2. We can then
&gt; compute the two recurrencies
&gt;
&gt;   E_j = K^2 (E_{j-1} + M_{2j})
&gt;   O_j = K^2 (O_{j-1} + M_{2j+1})
&gt;
&gt; in parallel. It's unclear to me what the pros and cons are compared to
&gt; previous variant. One may get some advantage from both multiplies using
&gt; the same factor K^2. On the other hand, each recurrency has to be
&gt; accumulated and folded separately, which costs instructions and
&gt; registers. Maybe more useful for hardware implementation? This variant
&gt; is currently not used in Nettle.
&gt;
&gt; ** Doing both **
&gt;
&gt; It's possible to combine those two tricks. Processing of two blocks
&gt; would then be an operation of the form
&gt;
&gt;   Z_2 B^2 + Z_1 B + Z_0
&gt;     = (X_2 B^2 + X_1 B + X_0) K^2 + (Y_1 B + Y_0) K
&gt;
&gt; Here, the Xs (three words) represent R_{j-2} + M_{j-1}, the Ys repreent
&gt; M_{j-2}, and the Zs represents R_j, as three words (without final folding).
&gt; We would need 10 independent multiples, one more than with plain
&gt; interleaving, but critical path includes only one multiply latency.
&gt;

Same matter here, if we consider 3 words for K^3 this sums up the number of
independent multiples to 13.


&gt; I think this is a promising alternative, if one would otherwise need to
&gt; interleaving a large number of blocks to get full utilization of the
&gt; multipliers.
&gt;
&gt; ** How to choose **
&gt;
&gt; When implementing one of those schemes, different processor resources
&gt; may be the bottleneck. I'd expect it to be one of
&gt;
&gt;  o  Multiply latency, i.e, latency of the dependency chain from one block
&gt;     to the next (including also a few additions, but multiply latency
&gt;     willb e the main part). If this is the bottleneck, it means all
&gt;     other instructions can be scheduled in parallel, and the processor
&gt;     will sit idle for some cycles, waiting for a multiply to complete.
&gt;     Typical latency for multiply is 5 times longer than for an addition
&gt;     (but ratio difers quite a bit between processors, of course)
&gt;
&gt;  o  Multiply throughput, i.e., the maximum number of (independent) multiply
&gt;     instructions that can be run per cycle. Typical number is 0.5 -- 2.
&gt;     If this is the bottleneck, the processor will spend some cycles idle,
&gt;     waiting for a multiplier to be ready to accept a new input.
&gt;
&gt;  o  A superscalar processor can issue several instructinos in the same
&gt;     cycle, but there's a fix small limit. Typical number is 2 -- 6. So,
&gt;     e.g., if the processor can issue maximum 4 instructions per cycle,
&gt;     the evaluation loop consists of 40 instructions, and the loop
&gt;     actually runs in close to 10 cycles per iteration, then instruction
&gt;     issue is the bottleneck.
&gt;
&gt; The tricks discussed in this note are useful for finding an evaluation
&gt; scheme where multiply latency isn't a bottleneck. But once a loop hits
&gt; the limit on multiply throughput or instructions per cycle, other tricks
&gt; are needed to optimize further. In particular, the postponed reduction
&gt; has a cost in multiply throughput, since it needs some additional
&gt; multiply instruction.
&gt;
&gt; I think one should aim to hit the limit on multiply throughput; that one
&gt; is hard to negotiate (it's possible to reduce the number of multiply
&gt; instructions somewhat, by the Karatsuba trick, but due to the additional
&gt; overhead, likely to be useful only on processors with particularly low
&gt; multiply throughput).
&gt;
&gt; Regards,
&gt; /Niels
&gt;
&gt; --
&gt; Niels Möller. PGP key CB4962D070D77D7FCB8BA36271D8F1FF368C6677.
&gt; Internet email is subject to wholesale government surveillance.
&gt;
&gt; _______________________________________________
&gt; nettle-bugs mailing list
&gt; nettle-bugs@lists.lysator.liu.se
&gt; http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs
&gt;
_______________________________________________
nettle-bugs mailing list
nettle-bugs@lists.lysator.liu.se
http://lists.lysator.liu.se/mailman/listinfo/nettle-bugs

</body></email></emails>